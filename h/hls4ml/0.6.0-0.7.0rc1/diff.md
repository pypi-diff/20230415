# Comparing `tmp/hls4ml-0.6.0.tar.gz` & `tmp/hls4ml-0.7.0rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "hls4ml-0.6.0.tar", last modified: Fri Nov 12 12:26:47 2021, max compression
+gzip compressed data, was "hls4ml-0.7.0rc1.tar", last modified: Sat Apr 15 01:45:10 2023, max compression
```

## Comparing `hls4ml-0.6.0.tar` & `hls4ml-0.7.0rc1.tar`

### file list

```diff
@@ -1,265 +1,536 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.390328 hls4ml-0.6.0/
--rw-r--r--   0 runner    (1001) docker     (121)    11357 2021-11-12 12:26:42.000000 hls4ml-0.6.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (121)      122 2021-11-12 12:26:42.000000 hls4ml-0.6.0/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (121)     3002 2021-11-12 12:26:47.390328 hls4ml-0.6.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     2192 2021-11-12 12:26:42.000000 hls4ml-0.6.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.346327 hls4ml-0.6.0/example-prjs/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.350327 hls4ml-0.6.0/example-prjs/conv-1layer/
--rw-r--r--   0 runner    (1001) docker     (121)      799 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/build_prj.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/
--rw-r--r--   0 runner    (1001) docker     (121)     2689 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/myproject.cpp
--rw-r--r--   0 runner    (1001) docker     (121)     1150 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/myproject.h
--rw-r--r--   0 runner    (1001) docker     (121)     2173 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/parameters.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/weights/
--rw-r--r--   0 runner    (1001) docker     (121)      148 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/weights/b1.h
--rw-r--r--   0 runner    (1001) docker     (121)      179 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/weights/b2.h
--rw-r--r--   0 runner    (1001) docker     (121)      668 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/weights/w1.h
--rw-r--r--   0 runner    (1001) docker     (121)     1257 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/firmware/weights/w2.h
--rw-r--r--   0 runner    (1001) docker     (121)       72 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/myproject.tcl
--rw-r--r--   0 runner    (1001) docker     (121)     2177 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv-1layer/myproject_test.cpp
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/conv2d-1layer/
--rw-r--r--   0 runner    (1001) docker     (121)      799 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/build_prj.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/
--rw-r--r--   0 runner    (1001) docker     (121)     2869 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/myproject.cpp
--rw-r--r--   0 runner    (1001) docker     (121)     1213 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/myproject.h
--rw-r--r--   0 runner    (1001) docker     (121)     2953 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/parameters.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/weights/
--rw-r--r--   0 runner    (1001) docker     (121)      155 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/weights/b1.h
--rw-r--r--   0 runner    (1001) docker     (121)      292 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/weights/b2.h
--rw-r--r--   0 runner    (1001) docker     (121)      418 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/weights/w1.h
--rw-r--r--   0 runner    (1001) docker     (121)    21834 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/weights/w2.h
--rw-r--r--   0 runner    (1001) docker     (121)       72 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/myproject.tcl
--rw-r--r--   0 runner    (1001) docker     (121)     5176 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/conv2d-1layer/myproject_test.cpp
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/demo-conversion/
--rw-r--r--   0 runner    (1001) docker     (121)      232 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/demo-conversion/README.md
--rw-r--r--   0 runner    (1001) docker     (121)      284 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/demo-conversion/keras-config.yml
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/higgs-1layer/
--rw-r--r--   0 runner    (1001) docker     (121)      798 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/build_prj.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.354328 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/
--rw-r--r--   0 runner    (1001) docker     (121)     2175 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/myproject.cpp
--rw-r--r--   0 runner    (1001) docker     (121)     1142 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/myproject.h
--rw-r--r--   0 runner    (1001) docker     (121)     1991 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/parameters.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.358327 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/weights/
--rw-r--r--   0 runner    (1001) docker     (121)      632 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/weights/b1.h
--rw-r--r--   0 runner    (1001) docker     (121)      111 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/weights/b2.h
--rw-r--r--   0 runner    (1001) docker     (121)     5393 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/weights/w1.h
--rw-r--r--   0 runner    (1001) docker     (121)      641 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/firmware/weights/w2.h
--rw-r--r--   0 runner    (1001) docker     (121)       72 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/myproject.tcl
--rw-r--r--   0 runner    (1001) docker     (121)     1368 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/higgs-1layer/myproject_test.cpp
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.358327 hls4ml-0.6.0/example-prjs/sublayer/
--rw-r--r--   0 runner    (1001) docker     (121)      550 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/build_prj.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.358327 hls4ml-0.6.0/example-prjs/sublayer/firmware/
--rw-r--r--   0 runner    (1001) docker     (121)     3621 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/myproject.cpp
--rw-r--r--   0 runner    (1001) docker     (121)     1218 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/myproject.h
--rw-r--r--   0 runner    (1001) docker     (121)     3994 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/parameters.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.358327 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/
--rw-r--r--   0 runner    (1001) docker     (121)     1460 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/b1.h
--rw-r--r--   0 runner    (1001) docker     (121)      804 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/b2.h
--rw-r--r--   0 runner    (1001) docker     (121)      460 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/b2_0.h
--rw-r--r--   0 runner    (1001) docker     (121)      474 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/b2_1.h
--rw-r--r--   0 runner    (1001) docker     (121)      804 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/b3.h
--rw-r--r--   0 runner    (1001) docker     (121)      230 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/b4.h
--rw-r--r--   0 runner    (1001) docker     (121)    12253 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/w1.h
--rw-r--r--   0 runner    (1001) docker     (121)    17994 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/w2.h
--rw-r--r--   0 runner    (1001) docker     (121)     9318 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/w2_0.h
--rw-r--r--   0 runner    (1001) docker     (121)     8811 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/w2_1.h
--rw-r--r--   0 runner    (1001) docker     (121)     8872 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/w3.h
--rw-r--r--   0 runner    (1001) docker     (121)     2146 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/firmware/weights/w4.h
--rw-r--r--   0 runner    (1001) docker     (121)     1380 2021-11-12 12:26:42.000000 hls4ml-0.6.0/example-prjs/sublayer/myproject_test.cpp
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.358327 hls4ml-0.6.0/hls4ml/
--rw-r--r--   0 runner    (1001) docker     (121)      505 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.362328 hls4ml-0.6.0/hls4ml/converters/
--rw-r--r--   0 runner    (1001) docker     (121)    14532 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.362328 hls4ml-0.6.0/hls4ml/converters/keras/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2981 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/convolution.py
--rw-r--r--   0 runner    (1001) docker     (121)     4719 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/core.py
--rw-r--r--   0 runner    (1001) docker     (121)     2175 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/graph.py
--rw-r--r--   0 runner    (1001) docker     (121)     1408 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/merge.py
--rw-r--r--   0 runner    (1001) docker     (121)     3755 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/pooling.py
--rw-r--r--   0 runner    (1001) docker     (121)     3568 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/qkeras.py
--rw-r--r--   0 runner    (1001) docker     (121)     5199 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/qkeras_layers.py
--rw-r--r--   0 runner    (1001) docker     (121)     2556 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/reshape.py
--rw-r--r--   0 runner    (1001) docker     (121)     3353 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras/reshaping.py
--rw-r--r--   0 runner    (1001) docker     (121)    13305 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/keras_to_hls.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.362328 hls4ml-0.6.0/hls4ml/converters/onnx/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3520 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx/convolution.py
--rw-r--r--   0 runner    (1001) docker     (121)     3868 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx/core.py
--rw-r--r--   0 runner    (1001) docker     (121)     1556 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx/merge.py
--rw-r--r--   0 runner    (1001) docker     (121)     4564 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx/pooling.py
--rw-r--r--   0 runner    (1001) docker     (121)     1427 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx/reshape.py
--rw-r--r--   0 runner    (1001) docker     (121)    11228 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/onnx_to_hls.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.362328 hls4ml-0.6.0/hls4ml/converters/pytorch/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3713 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/pytorch/convolution.py
--rw-r--r--   0 runner    (1001) docker     (121)     2409 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/pytorch/core.py
--rw-r--r--   0 runner    (1001) docker     (121)      216 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/pytorch/pooling.py
--rw-r--r--   0 runner    (1001) docker     (121)     6659 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/pytorch_to_hls.py
--rw-r--r--   0 runner    (1001) docker     (121)    14936 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/tf_to_hls.py
--rw-r--r--   0 runner    (1001) docker     (121)     2945 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/converters/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.366328 hls4ml-0.6.0/hls4ml/model/
--rw-r--r--   0 runner    (1001) docker     (121)      227 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    86993 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/hls_layers.py
--rw-r--r--   0 runner    (1001) docker     (121)    30134 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/hls_model.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.366328 hls4ml-0.6.0/hls4ml/model/optimizer/
--rw-r--r--   0 runner    (1001) docker     (121)     2723 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1219 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.366328 hls4ml-0.6.0/hls4ml/model/optimizer/passes/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1341 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/bn_fuse.py
--rw-r--r--   0 runner    (1001) docker     (121)     8721 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/bn_quant.py
--rw-r--r--   0 runner    (1001) docker     (121)     3035 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/clone.py
--rw-r--r--   0 runner    (1001) docker     (121)     3632 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/conv_same_pad.py
--rw-r--r--   0 runner    (1001) docker     (121)      775 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/fuse_biasadd.py
--rw-r--r--   0 runner    (1001) docker     (121)     2102 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/multi_dense.py
--rw-r--r--   0 runner    (1001) docker     (121)      510 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/nop.py
--rw-r--r--   0 runner    (1001) docker     (121)     2519 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/pointwise.py
--rw-r--r--   0 runner    (1001) docker     (121)    11376 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/qkeras.py
--rw-r--r--   0 runner    (1001) docker     (121)     5557 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/repack_stream.py
--rw-r--r--   0 runner    (1001) docker     (121)      825 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/optimizer/passes/transpose_opt.py
--rw-r--r--   0 runner    (1001) docker     (121)    24654 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/model/profiling.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.366328 hls4ml-0.6.0/hls4ml/report/
--rw-r--r--   0 runner    (1001) docker     (121)      159 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/report/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6797 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/report/vivado_report.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.370328 hls4ml-0.6.0/hls4ml/templates/
--rw-r--r--   0 runner    (1001) docker     (121)      416 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      431 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/supported_boards.json
--rw-r--r--   0 runner    (1001) docker     (121)     1285 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/templates.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.370328 hls4ml-0.6.0/hls4ml/templates/vivado/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.374328 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/
--rw-r--r--   0 runner    (1001) docker     (121)    10180 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_common.h
--rw-r--r--   0 runner    (1001) docker     (121)     6473 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_decl.h
--rw-r--r--   0 runner    (1001) docker     (121)    12185 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed.h
--rw-r--r--   0 runner    (1001) docker     (121)    85182 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed_base.h
--rw-r--r--   0 runner    (1001) docker     (121)    27302 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed_ref.h
--rw-r--r--   0 runner    (1001) docker     (121)     6985 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed_special.h
--rw-r--r--   0 runner    (1001) docker     (121)    10012 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int.h
--rw-r--r--   0 runner    (1001) docker     (121)    71735 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int_base.h
--rw-r--r--   0 runner    (1001) docker     (121)    55218 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int_ref.h
--rw-r--r--   0 runner    (1001) docker     (121)     6301 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int_special.h
--rw-r--r--   0 runner    (1001) docker     (121)     4894 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_shift_reg.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.374328 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/etc/
--rw-r--r--   0 runner    (1001) docker     (121)   261465 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/etc/ap_private.h
--rw-r--r--   0 runner    (1001) docker     (121)     7480 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/hls_stream.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.374328 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/utils/
--rw-r--r--   0 runner    (1001) docker     (121)     2227 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/utils/x_hls_utils.h
--rwxr-xr-x   0 runner    (1001) docker     (121)      529 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/build_lib.sh
--rw-r--r--   0 runner    (1001) docker     (121)     3730 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/build_prj.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.374328 hls4ml-0.6.0/hls4ml/templates/vivado/firmware/
--rw-r--r--   0 runner    (1001) docker     (121)      257 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/firmware/defines.h
--rw-r--r--   0 runner    (1001) docker     (121)     1338 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/firmware/myproject.cpp
--rw-r--r--   0 runner    (1001) docker     (121)     1055 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/firmware/myproject.h
--rw-r--r--   0 runner    (1001) docker     (121)      271 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/firmware/parameters.h
--rw-r--r--   0 runner    (1001) docker     (121)     1693 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/myproject_bridge.cpp
--rw-r--r--   0 runner    (1001) docker     (121)     3252 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/myproject_test.cpp
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/
--rw-r--r--   0 runner    (1001) docker     (121)       89 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/README.md
--rw-r--r--   0 runner    (1001) docker     (121)    29708 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_activation.h
--rw-r--r--   0 runner    (1001) docker     (121)    23337 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_activation_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     1664 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_array.h
--rw-r--r--   0 runner    (1001) docker     (121)     5586 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_batchnorm.h
--rw-r--r--   0 runner    (1001) docker     (121)     4639 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_batchnorm_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     2564 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_common.h
--rw-r--r--   0 runner    (1001) docker     (121)     2409 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv1d.h
--rw-r--r--   0 runner    (1001) docker     (121)     8073 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_latency.h
--rw-r--r--   0 runner    (1001) docker     (121)     8253 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_resource.h
--rw-r--r--   0 runner    (1001) docker     (121)     3557 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     3364 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv2d.h
--rw-r--r--   0 runner    (1001) docker     (121)    19428 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_latency.h
--rw-r--r--   0 runner    (1001) docker     (121)    10328 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_resource.h
--rw-r--r--   0 runner    (1001) docker     (121)     4788 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)    13944 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     1473 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense.h
--rw-r--r--   0 runner    (1001) docker     (121)     3690 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense_compressed.h
--rw-r--r--   0 runner    (1001) docker     (121)     5377 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense_latency.h
--rw-r--r--   0 runner    (1001) docker     (121)    10540 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense_resource.h
--rw-r--r--   0 runner    (1001) docker     (121)     2275 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)    32133 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_garnet.h
--rw-r--r--   0 runner    (1001) docker     (121)    13118 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_helpers.h
--rw-r--r--   0 runner    (1001) docker     (121)     1246 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_image.h
--rw-r--r--   0 runner    (1001) docker     (121)     1192 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_image_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)    11530 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_merge.h
--rw-r--r--   0 runner    (1001) docker     (121)     9459 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_merge_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     3450 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_mult.h
--rw-r--r--   0 runner    (1001) docker     (121)     4192 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_padding.h
--rw-r--r--   0 runner    (1001) docker     (121)     2164 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_padding_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)    11812 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_pooling.h
--rw-r--r--   0 runner    (1001) docker     (121)    23183 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_pooling_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     4897 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_sepconv1d_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     6243 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_sepconv2d_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)    10909 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_sepconv_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)     3580 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_stream.h
--rw-r--r--   0 runner    (1001) docker     (121)      742 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_types.h
--rw-r--r--   0 runner    (1001) docker     (121)      139 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado/vivado_synth.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/
--rw-r--r--   0 runner    (1001) docker     (121)      618 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/build_lib.sh
--rw-r--r--   0 runner    (1001) docker     (121)      435 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/myproject_axi.cpp
--rw-r--r--   0 runner    (1001) docker     (121)      251 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/myproject_axi.h
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.350327 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/python_drivers/
--rw-r--r--   0 runner    (1001) docker     (121)     3545 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/python_drivers/axi_stream_driver.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/
--rw-r--r--   0 runner    (1001) docker     (121)     1509 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_lite_design.tcl
--rw-r--r--   0 runner    (1001) docker     (121)     3454 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_stream_design.tcl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.350327 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/python_drivers/
--rw-r--r--   0 runner    (1001) docker     (121)     3545 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/python_drivers/axi_stream_driver.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/tcl_scripts/
--rw-r--r--   0 runner    (1001) docker     (121)     3898 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/tcl_scripts/axi_stream_design.tcl
--rw-r--r--   0 runner    (1001) docker     (121)     6364 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator_config.py
--rw-r--r--   0 runner    (1001) docker     (121)     3334 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_accelerator_template.py
--rw-r--r--   0 runner    (1001) docker     (121)    35803 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/templates/vivado_template.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/utils/
--rw-r--r--   0 runner    (1001) docker     (121)      268 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    14380 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/utils/config.py
--rw-r--r--   0 runner    (1001) docker     (121)     5485 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/utils/example_models.py
--rw-r--r--   0 runner    (1001) docker     (121)     8789 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/utils/plot.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.382328 hls4ml-0.6.0/hls4ml/writer/
--rw-r--r--   0 runner    (1001) docker     (121)      342 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/writer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    20850 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/writer/vivado_accelerator_writer.py
--rw-r--r--   0 runner    (1001) docker     (121)    32743 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/writer/vivado_writer.py
--rw-r--r--   0 runner    (1001) docker     (121)      371 2021-11-12 12:26:42.000000 hls4ml-0.6.0/hls4ml/writer/writers.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.358327 hls4ml-0.6.0/hls4ml.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)     3002 2021-11-12 12:26:47.000000 hls4ml-0.6.0/hls4ml.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     8901 2021-11-12 12:26:47.000000 hls4ml-0.6.0/hls4ml.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2021-11-12 12:26:47.000000 hls4ml-0.6.0/hls4ml.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)       56 2021-11-12 12:26:47.000000 hls4ml-0.6.0/hls4ml.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (121)       73 2021-11-12 12:26:47.000000 hls4ml-0.6.0/hls4ml.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)        7 2021-11-12 12:26:47.000000 hls4ml-0.6.0/hls4ml.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.386328 hls4ml-0.6.0/scripts/
--rwxr-xr-x   0 runner    (1001) docker     (121)     8480 2021-11-12 12:26:42.000000 hls4ml-0.6.0/scripts/hls4ml
--rw-r--r--   0 runner    (1001) docker     (121)      160 2021-11-12 12:26:47.390328 hls4ml-0.6.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (121)     1841 2021-11-12 12:26:42.000000 hls4ml-0.6.0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.386328 hls4ml-0.6.0/test/
--rwxr-xr-x   0 runner    (1001) docker     (121)     3301 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/build-prj.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     1040 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/cleanup.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     2955 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/compare-reports.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     2600 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/convert-keras-models.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     2173 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/convert-onnx-models.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     2180 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/convert-pytorch-models.sh
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.390328 hls4ml-0.6.0/test/docker/
--rw-r--r--   0 runner    (1001) docker     (121)     2668 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/docker/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (121)     2962 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/docker/README.md
--rw-r--r--   0 runner    (1001) docker     (121)     1647 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/docker/install_config-2017.2.txt
--rw-r--r--   0 runner    (1001) docker     (121)     1743 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/docker/install_config.txt
--rwxr-xr-x   0 runner    (1001) docker     (121)     1519 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/gather-reports.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)      804 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/hls4ml-keras-test.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     1146 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/hls4ml-onnx-test.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     1170 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/hls4ml-pytorch-test.sh
--rw-r--r--   0 runner    (1001) docker     (121)     1706 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/keras-models.txt
--rwxr-xr-x   0 runner    (1001) docker     (121)     3747 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/keras-to-hls.sh
--rw-r--r--   0 runner    (1001) docker     (121)      792 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/onnx-models.txt
--rwxr-xr-x   0 runner    (1001) docker     (121)     2379 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/onnx-to-hls.sh
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-11-12 12:26:47.390328 hls4ml-0.6.0/test/pytest/
--rw-r--r--   0 runner    (1001) docker     (121)      705 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/ci-template.yml
--rw-r--r--   0 runner    (1001) docker     (121)      784 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/generate_ci_yaml.py
--rw-r--r--   0 runner    (1001) docker     (121)     3051 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_cnn_mnist.py
--rw-r--r--   0 runner    (1001) docker     (121)     2728 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_conv1d.py
--rw-r--r--   0 runner    (1001) docker     (121)     1967 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_globalpooling1d.py
--rw-r--r--   0 runner    (1001) docker     (121)     6947 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_graph.py
--rw-r--r--   0 runner    (1001) docker     (121)    19189 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_keras_api.py
--rw-r--r--   0 runner    (1001) docker     (121)    10463 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_qkeras.py
--rw-r--r--   0 runner    (1001) docker     (121)     2810 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytest/test_softmax.py
--rw-r--r--   0 runner    (1001) docker     (121)      616 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytorch-models.txt
--rwxr-xr-x   0 runner    (1001) docker     (121)     2381 2021-11-12 12:26:42.000000 hls4ml-0.6.0/test/pytorch-to-hls.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.091124 hls4ml-0.7.0rc1/
+-rw-r--r--   0 runner    (1001) docker     (123)     5421 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.clang-format
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.031124 hls4ml-0.7.0rc1/.github/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.031124 hls4ml-0.7.0rc1/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/ISSUE_TEMPLATE/bug_report.md
+-rw-r--r--   0 runner    (1001) docker     (123)      378 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/ISSUE_TEMPLATE/config.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      990 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/ISSUE_TEMPLATE/feature_request.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1399 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/PULL_REQUEST_TEMPLATE.md
+-rw-r--r--   0 runner    (1001) docker     (123)      163 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/dependabot.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.031124 hls4ml-0.7.0rc1/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)     1246 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/workflows/build-sphinx.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      467 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/workflows/pre-commit.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      693 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/workflows/pypi-publish.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      692 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/workflows/test-sphinx.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      775 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.github/workflows/update-branch-on-pr.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      144 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)     1797 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1517 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/CITATION.cff
+-rw-r--r--   0 runner    (1001) docker     (123)     2810 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/CONTRIBUTING.md
+-rw-r--r--   0 runner    (1001) docker     (123)     2719 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/Jenkinsfile
+-rw-r--r--   0 runner    (1001) docker     (123)    11357 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      298 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     6035 2023-04-15 01:45:10.091124 hls4ml-0.7.0rc1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     5263 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.031124 hls4ml-0.7.0rc1/contrib/
+-rw-r--r--   0 runner    (1001) docker     (123)      530 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/contrib/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/contrib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14507 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/contrib/garnet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.031124 hls4ml-0.7.0rc1/contrib/kl_layer/
+-rw-r--r--   0 runner    (1001) docker     (123)      537 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/contrib/kl_layer/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     3367 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/contrib/kl_layer/kl_layer.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6006 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/contrib/kl_layer/kl_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.035124 hls4ml-0.7.0rc1/docs/
+-rw-r--r--   0 runner    (1001) docker     (123)      678 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/Makefile
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.035124 hls4ml-0.7.0rc1/docs/advanced/
+-rw-r--r--   0 runner    (1001) docker     (123)     3922 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/advanced/accelerator.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6561 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/advanced/extension.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2985 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/advanced/fifo_depth.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.035124 hls4ml-0.7.0rc1/docs/api/
+-rw-r--r--   0 runner    (1001) docker     (123)    10243 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/api/configuration.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2601 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/api/hls-model.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4666 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/api/profiling.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3832 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/command.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6176 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/concepts.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4090 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/details.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5608 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/flows.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.039124 hls4ml-0.7.0rc1/docs/img/
+-rw-r--r--   0 runner    (1001) docker     (123)     9354 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/act_hls4ml.png
+-rw-r--r--   0 runner    (1001) docker     (123)    10406 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/act_keras.png
+-rw-r--r--   0 runner    (1001) docker     (123)    26453 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/hls4ml_logo.png
+-rw-r--r--   0 runner    (1001) docker     (123)    21083 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/hls4ml_logo.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    25377 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/hls4ml_logo_lightgrey.png
+-rw-r--r--   0 runner    (1001) docker     (123)    23796 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/hls4ml_logo_lightgrey.svg
+-rw-r--r--   0 runner    (1001) docker     (123)     7506 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/hls4ml_logo_navbar.png
+-rw-r--r--   0 runner    (1001) docker     (123)    64144 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/logo.jpg
+-rw-r--r--   0 runner    (1001) docker     (123)    23360 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/logo.png
+-rw-r--r--   0 runner    (1001) docker     (123)    98357 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/nn_map_paper_fig_2.png
+-rw-r--r--   0 runner    (1001) docker     (123)   202059 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/overview.jpg
+-rw-r--r--   0 runner    (1001) docker     (123)   642072 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/overview.pdf
+-rw-r--r--   0 runner    (1001) docker     (123)   335589 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/pynqframe.png
+-rw-r--r--   0 runner    (1001) docker     (123)    82654 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/reuse_factor_paper_fig_8.png
+-rw-r--r--   0 runner    (1001) docker     (123)    10231 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/weights_hls4ml.png
+-rw-r--r--   0 runner    (1001) docker     (123)    10969 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/weights_keras.png
+-rw-r--r--   0 runner    (1001) docker     (123)    76029 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/img/zynq_interfaces.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1639 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2858 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/reference.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      271 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/release_notes.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     6249 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/setup.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/docs/status.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.039124 hls4ml-0.7.0rc1/example-models/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.023124 hls4ml-0.7.0rc1/example-models/.github/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.039124 hls4ml-0.7.0rc1/example-models/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)      514 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/.github/workflows/update-model-list.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      159 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      435 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     2582 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/available_data_config.json
+-rw-r--r--   0 runner    (1001) docker     (123)      988 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/available_models.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.039124 hls4ml-0.7.0rc1/example-models/config-files/
+-rw-r--r--   0 runner    (1001) docker     (123)      877 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/config-files/garnet_1layer_config.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      949 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/config-files/garnet_3layer_config.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1409 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/config-files/qkeras_3layer_config.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1361 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/config-files/qkeras_mnist_cnn_config.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1100 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/config-files/qkeras_mnist_dense_config.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.039124 hls4ml-0.7.0rc1/example-models/data/
+-rw-r--r--   0 runner    (1001) docker     (123)  1188502 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/data/garnet_1layer_input.dat
+-rw-r--r--   0 runner    (1001) docker     (123)     2304 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/data/garnet_1layer_output.dat
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.051124 hls4ml-0.7.0rc1/example-models/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)     4042 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_1layer.json
+-rw-r--r--   0 runner    (1001) docker     (123)    13944 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_1layer_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     8084 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer.json
+-rw-r--r--   0 runner    (1001) docker     (123)    36096 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_70pruned_retrained_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    36096 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_95pruned_retrained_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    15426 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_batch_norm.json
+-rw-r--r--   0 runner    (1001) docker     (123)    55224 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_batch_norm_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     9912 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_binary_smaller.json
+-rw-r--r--   0 runner    (1001) docker     (123)    52240 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_binary_smaller_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    11160 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_binarydense_relu_max.json
+-rw-r--r--   0 runner    (1001) docker     (123)    52224 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_binarydense_relu_max_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     9919 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_ternary_small.json
+-rw-r--r--   0 runner    (1001) docker     (123)    52240 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_ternary_small_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    36096 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_3layer_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    10545 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_conv1d.json
+-rw-r--r--   0 runner    (1001) docker     (123)    11331 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_conv1d_small.json
+-rw-r--r--   0 runner    (1001) docker     (123)    23232 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_conv1d_small_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)  1026808 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_conv1d_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     3342 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_conv2d_model.json
+-rw-r--r--   0 runner    (1001) docker     (123)    19904 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_conv2d_model_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    12760 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_dense_16x100x100x100x100x100x5.json
+-rw-r--r--   0 runner    (1001) docker     (123)   197336 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_dense_16x100x100x100x100x100x5_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    13774 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_dense_16x200x200x200x200x200x5.json
+-rw-r--r--   0 runner    (1001) docker     (123)   690872 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_dense_16x200x200x200x200x200x5_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    12760 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_dense_16x500x500x500x500x500x5.json
+-rw-r--r--   0 runner    (1001) docker     (123)  4080168 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/KERAS_dense_16x500x500x500x500x500x5_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     1931 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/garnet_1layer.json
+-rw-r--r--   0 runner    (1001) docker     (123)    20144 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/garnet_1layer_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     3004 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/garnet_3layer.json
+-rw-r--r--   0 runner    (1001) docker     (123)    42488 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/garnet_3layer_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     4950 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/jetTagger_Conv2D_Small.json
+-rw-r--r--   0 runner    (1001) docker     (123)     3829 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/jetTagger_Conv2D_Small_NoBatchNorm.json
+-rw-r--r--   0 runner    (1001) docker     (123)    25392 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/jetTagger_Conv2D_Small_NoBatchNorm_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)    32528 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/jetTagger_Conv2D_Small_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     5429 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/keras_bnn.json
+-rw-r--r--   0 runner    (1001) docker     (123)    71320 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/keras_bnn_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     6981 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/qkeras_3layer.json
+-rw-r--r--   0 runner    (1001) docker     (123)    38344 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/qkeras_3layer_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     6020 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/qkeras_mnist_cnn.json
+-rw-r--r--   0 runner    (1001) docker     (123)    47424 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/qkeras_mnist_cnn_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)     8220 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/qkeras_mnist_dense.json
+-rw-r--r--   0 runner    (1001) docker     (123)    79416 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras/qkeras_mnist_dense_weights.h5
+-rw-r--r--   0 runner    (1001) docker     (123)      513 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/keras-config.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.051124 hls4ml-0.7.0rc1/example-models/onnx/
+-rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/conv1d_small_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)     6593 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/conv2d_small_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)     4757 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/conv2d_small_mp_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)   172539 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/dense_big_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)    23520 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/three_layer_bn_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)    42287 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/three_layer_bn_pytorch.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)    18824 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/three_layer_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)    32007 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/three_layer_pytorch.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)     2242 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/two_layer_keras.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)     8688 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx/two_layer_pytorch.onnx
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/onnx-config.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/example-models/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)    26028 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/pytorch/three_layer_model.pt
+-rw-r--r--   0 runner    (1001) docker     (123)     8356 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/pytorch/two_layer_model.pt
+-rw-r--r--   0 runner    (1001) docker     (123)      465 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/pytorch-config.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/scan.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/example-models/tensorflow/
+-rw-r--r--   0 runner    (1001) docker     (123)    19927 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/tensorflow/3layer.pb
+-rw-r--r--   0 runner    (1001) docker     (123)      475 2023-04-15 01:44:59.000000 hls4ml-0.7.0rc1/example-models/tf-config.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/hls4ml/
+-rw-r--r--   0 runner    (1001) docker     (123)      611 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      163 2023-04-15 01:45:09.000000 hls4ml-0.7.0rc1/hls4ml/_version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/hls4ml/backends/
+-rw-r--r--   0 runner    (1001) docker     (123)      794 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6207 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/hls4ml/backends/fpga/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31806 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/fpga_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3311 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/fpga_layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15575 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/fpga_types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7331 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/bn_quant.py
+-rw-r--r--   0 runner    (1001) docker     (123)      633 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/bram_weights.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3197 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/clone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2055 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/codegen.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1348 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)      952 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/final_reshape.py
+-rw-r--r--   0 runner    (1001) docker     (123)      821 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/inplace_parallel_reshape.py
+-rw-r--r--   0 runner    (1001) docker     (123)      957 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/inplace_stream_flatten.py
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/remove_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/repack_stream.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/hls4ml/backends/quartus/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7592 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/convolution_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9303 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/convolution_winograd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8717 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/core_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4077 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/merge_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/pointwise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4533 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/pooling_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/quantization_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13910 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/recurrent_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4962 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/reshaping_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4005 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/resource_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2770 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/passes/transform_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16171 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/quartus/quartus_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2553 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/template.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/vitis/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vitis/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/vitis/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vitis/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vitis/passes/feature_check.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2301 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vitis/vitis_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/vivado/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4554 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/broadcast_stream.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3997 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/conv_same_pad.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2405 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/conv_stream.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20889 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/convolution_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8426 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/core_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3740 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/fifo_depth_optimization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11959 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/garnet_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3855 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/merge_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3450 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/pointwise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4728 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/pooling_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1320 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/quantization_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8300 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/recurrent_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4742 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/reshaping_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2353 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/resource_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2542 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/transform_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19054 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado/vivado_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.059124 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2938 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/passes/fifo_depth_optimization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1448 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/supported_boards.json
+-rw-r--r--   0 runner    (1001) docker     (123)     6517 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/vivado_accelerator_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6761 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/vivado_accelerator_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/converters/
+-rw-r--r--   0 runner    (1001) docker     (123)    14404 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/converters/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2790 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4442 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4130 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6535 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/qkeras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1475 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/recurrent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3236 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/reshape.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3590 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras/reshaping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11658 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/keras_to_hls.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/converters/onnx/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3012 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3597 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1559 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx/merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4011 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx/reshape.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10799 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/onnx_to_hls.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/converters/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3176 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/pytorch/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2388 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/pytorch/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6191 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/pytorch_to_hls.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14698 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/tf_to_hls.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5647 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/converters/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/model/
+-rw-r--r--   0 runner    (1001) docker     (123)      214 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8366 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/attributes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/model/flow/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/flow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4466 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/flow/flow.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33950 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    52343 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.063124 hls4ml-0.7.0rc1/hls4ml/model/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (123)     1357 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9492 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.067124 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1469 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/bn_fuse.py
+-rw-r--r--   0 runner    (1001) docker     (123)      618 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/fuse_biasadd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2568 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/multi_dense.py
+-rw-r--r--   0 runner    (1001) docker     (123)      528 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/nop.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1762 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/precision_merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11642 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/qkeras.py
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/stamp.py
+-rw-r--r--   0 runner    (1001) docker     (123)      815 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/transpose_opt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24057 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/profiling.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22807 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/model/types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.067124 hls4ml-0.7.0rc1/hls4ml/report/
+-rw-r--r--   0 runner    (1001) docker     (123)      372 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/report/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9341 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/report/quartus_report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25795 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/report/vivado_report.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.027124 hls4ml-0.7.0rc1/hls4ml/templates/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.067124 hls4ml-0.7.0rc1/hls4ml/templates/quartus/
+-rw-r--r--   0 runner    (1001) docker     (123)      798 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/Makefile
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.067124 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/
+-rw-r--r--   0 runner    (1001) docker     (123)    18367 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_channel.h
+-rw-r--r--   0 runner    (1001) docker     (123)    16534 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_complex.h
+-rw-r--r--   0 runner    (1001) docker     (123)    59595 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_fixed.h
+-rw-r--r--   0 runner    (1001) docker     (123)    44867 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_float.h
+-rw-r--r--   0 runner    (1001) docker     (123)   105429 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_int.h
+-rw-r--r--   0 runner    (1001) docker     (123)    17449 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_sc.h
+-rw-r--r--   0 runner    (1001) docker     (123)    84637 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/ac_std_float.h
+-rw-r--r--   0 runner    (1001) docker     (123)      791 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/ac_types/stream.h
+-rwxr-xr-x   0 runner    (1001) docker     (123)      551 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/build_lib.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.067124 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/
+-rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/defines.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/myproject.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)     1837 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/myproject.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.071124 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)    18055 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_activation.h
+-rw-r--r--   0 runner    (1001) docker     (123)    23323 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_activation_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3184 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_batchnorm.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3392 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_batchnorm_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_common.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2245 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv1d.h
+-rw-r--r--   0 runner    (1001) docker     (123)    10544 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv1d_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     7594 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv1d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv2d.h
+-rw-r--r--   0 runner    (1001) docker     (123)    14958 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv2d_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)    10556 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv2d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     7012 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_dense.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2791 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_dense_compressed.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1422 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_dense_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1259 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_embed.h
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_embed_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     5023 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_helpers.h
+-rw-r--r--   0 runner    (1001) docker     (123)    11047 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_merge.h
+-rw-r--r--   0 runner    (1001) docker     (123)    11820 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_merge_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3941 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_mult.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2845 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_padding.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2186 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_padding_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    12470 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_pooling.h
+-rw-r--r--   0 runner    (1001) docker     (123)    12279 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_pooling_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    25143 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_recurrent.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1805 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_recurrent_activation.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2025 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_recurrent_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_resize.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1555 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_resize_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3191 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_transpose.h
+-rw-r--r--   0 runner    (1001) docker     (123)      963 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_transpose_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1776 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_types.h
+-rw-r--r--   0 runner    (1001) docker     (123)      207 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/parameters.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/myproject_bridge.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)     3611 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/myproject_test_parallel.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)     3914 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/quartus/myproject_test_stream.cpp
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.027124 hls4ml-0.7.0rc1/hls4ml/templates/vitis/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.071124 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     4040 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_conv1d_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1239 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_conv1d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     4168 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_conv2d_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3572 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_conv2d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     9671 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_dense_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3921 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_dense_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    15306 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_pooling.h
+-rw-r--r--   0 runner    (1001) docker     (123)    12369 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_pooling_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     4246 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_sepconv1d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6188 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_sepconv2d_stream.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.075124 hls4ml-0.7.0rc1/hls4ml/templates/vivado/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.075124 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/
+-rw-r--r--   0 runner    (1001) docker     (123)    10180 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_common.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6473 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_decl.h
+-rw-r--r--   0 runner    (1001) docker     (123)    12185 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed.h
+-rw-r--r--   0 runner    (1001) docker     (123)    85182 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed_base.h
+-rw-r--r--   0 runner    (1001) docker     (123)    27302 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed_ref.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6985 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed_special.h
+-rw-r--r--   0 runner    (1001) docker     (123)    10012 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int.h
+-rw-r--r--   0 runner    (1001) docker     (123)    71735 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int_base.h
+-rw-r--r--   0 runner    (1001) docker     (123)    55218 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int_ref.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6301 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int_special.h
+-rw-r--r--   0 runner    (1001) docker     (123)     4894 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_shift_reg.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.075124 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/etc/
+-rw-r--r--   0 runner    (1001) docker     (123)   261465 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/etc/ap_private.h
+-rw-r--r--   0 runner    (1001) docker     (123)      194 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/hls_math.h
+-rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/hls_stream.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.075124 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     2227 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/utils/x_hls_utils.h
+-rwxr-xr-x   0 runner    (1001) docker     (123)      530 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/build_lib.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     7812 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/build_prj.tcl
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.075124 hls4ml-0.7.0rc1/hls4ml/templates/vivado/firmware/
+-rw-r--r--   0 runner    (1001) docker     (123)      259 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/firmware/defines.h
+-rw-r--r--   0 runner    (1001) docker     (123)      547 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/firmware/myproject.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)      257 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/firmware/myproject.h
+-rw-r--r--   0 runner    (1001) docker     (123)      311 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/firmware/parameters.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/myproject_bridge.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)     2749 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/myproject_test.cpp
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.079124 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)    28377 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_activation.h
+-rw-r--r--   0 runner    (1001) docker     (123)    26176 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_activation_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_array.h
+-rw-r--r--   0 runner    (1001) docker     (123)     4140 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_batchnorm.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3772 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_batchnorm_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)      937 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_code_gen.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2058 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_common.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2416 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv1d.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2972 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_latency.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3879 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3717 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv2d.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3010 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_latency.h
+-rw-r--r--   0 runner    (1001) docker     (123)     4007 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     5131 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    14664 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1464 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense_compressed.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2424 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense_latency.h
+-rw-r--r--   0 runner    (1001) docker     (123)     9970 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense_resource.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2318 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_embed.h
+-rw-r--r--   0 runner    (1001) docker     (123)      839 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_embed_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    30731 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_garnet.h
+-rw-r--r--   0 runner    (1001) docker     (123)    12524 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_helpers.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1293 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_image.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_image_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    10964 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_merge.h
+-rw-r--r--   0 runner    (1001) docker     (123)    11628 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_merge_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3703 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_mult.h
+-rw-r--r--   0 runner    (1001) docker     (123)     4327 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_padding.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2191 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_padding_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    15287 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_pooling.h
+-rw-r--r--   0 runner    (1001) docker     (123)    23670 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_pooling_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1889 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_recr_activations.h
+-rw-r--r--   0 runner    (1001) docker     (123)    27031 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_recurrent.h
+-rw-r--r--   0 runner    (1001) docker     (123)     5310 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_sepconv1d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6697 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_sepconv2d_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)    11285 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_sepconv_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)     6882 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_stream.h
+-rw-r--r--   0 runner    (1001) docker     (123)      724 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_types.h
+-rw-r--r--   0 runner    (1001) docker     (123)      217 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado/vivado_synth.tcl
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.027124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/
+-rw-r--r--   0 runner    (1001) docker     (123)    11385 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/krnl_rtl_axi_read_master.sv
+-rw-r--r--   0 runner    (1001) docker     (123)     9883 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/krnl_rtl_axi_write_master.sv
+-rw-r--r--   0 runner    (1001) docker     (123)    12999 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/krnl_rtl_control_s_axi.v
+-rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/krnl_rtl_counter.sv
+-rw-r--r--   0 runner    (1001) docker     (123)    18061 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/krnl_rtl_int.sv
+-rw-r--r--   0 runner    (1001) docker     (123)     8472 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/krnl_rtl_src/myproject_kernel.v
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/python_drivers/
+-rw-r--r--   0 runner    (1001) docker     (123)     4866 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/python_drivers/axi_stream_driver.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/tcl_scripts/
+-rw-r--r--   0 runner    (1001) docker     (123)     9028 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/alveo/tcl_scripts/axi_stream_design.tcl
+-rw-r--r--   0 runner    (1001) docker     (123)      617 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/build_lib.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/myproject_axi.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)      239 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/myproject_axi.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.027124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/python_drivers/
+-rw-r--r--   0 runner    (1001) docker     (123)     3463 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/python_drivers/axi_stream_driver.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/
+-rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_lite_design.tcl
+-rw-r--r--   0 runner    (1001) docker     (123)     3484 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_stream_design.tcl
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.027124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/python_drivers/
+-rw-r--r--   0 runner    (1001) docker     (123)     3463 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/python_drivers/axi_stream_driver.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/tcl_scripts/
+-rw-r--r--   0 runner    (1001) docker     (123)     3931 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/tcl_scripts/axi_stream_design.tcl
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      270 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13098 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5635 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/utils/example_models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3979 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/utils/fixed_point_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8204 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/utils/plot.py
+-rw-r--r--   0 runner    (1001) docker     (123)      534 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/utils/string_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/hls4ml/writer/
+-rw-r--r--   0 runner    (1001) docker     (123)      502 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/writer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    58247 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/writer/quartus_writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      892 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/writer/vitis_writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22595 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/writer/vivado_accelerator_writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30570 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/writer/vivado_writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      354 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/hls4ml/writer/writers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.055124 hls4ml-0.7.0rc1/hls4ml.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     6035 2023-04-15 01:45:09.000000 hls4ml-0.7.0rc1/hls4ml.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    19488 2023-04-15 01:45:10.000000 hls4ml-0.7.0rc1/hls4ml.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-15 01:45:09.000000 hls4ml-0.7.0rc1/hls4ml.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       55 2023-04-15 01:45:09.000000 hls4ml-0.7.0rc1/hls4ml.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      126 2023-04-15 01:45:09.000000 hls4ml-0.7.0rc1/hls4ml.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       15 2023-04-15 01:45:09.000000 hls4ml-0.7.0rc1/hls4ml.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/pyproject.toml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.083124 hls4ml-0.7.0rc1/scripts/
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12340 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/scripts/hls4ml
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-04-15 01:45:10.091124 hls4ml-0.7.0rc1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)       69 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.087124 hls4ml-0.7.0rc1/test/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3301 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/build-prj.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1039 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/cleanup.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2945 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/compare-reports.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2582 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/convert-keras-models.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2161 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/convert-onnx-models.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2168 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/convert-pytorch-models.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.087124 hls4ml-0.7.0rc1/test/docker/
+-rw-r--r--   0 runner    (1001) docker     (123)     2668 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/docker/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)     2963 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/docker/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1646 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/docker/install_config-2017.2.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1742 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/docker/install_config.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1519 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/gather-reports.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1049 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/hls4ml-keras-test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1145 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/hls4ml-onnx-test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1169 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/hls4ml-pytorch-test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     1707 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/keras-models.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4075 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/keras-to-hls.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      778 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/onnx-models.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2349 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/onnx-to-hls.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.091124 hls4ml-0.7.0rc1/test/pytest/
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/ci-template.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      775 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/generate_ci_yaml.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2156 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_activations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1579 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_batchnorm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1816 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_bram_factor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_causalpadding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1708 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_clone_flatten.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3587 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_cnn_mnist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3664 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_cnn_mnist_qkeras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3878 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_conv1d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1618 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_conv1d_narrow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1627 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_conv2d_narrow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1786 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_embed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5212 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_extensions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5556 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_flows.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1750 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_garnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4121 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_globalpooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9627 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20432 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_keras_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1206 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_keras_h5_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6100 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4473 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_pointwiseconv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4018 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)      816 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_precision_parsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13097 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_qkeras.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-15 01:45:10.091124 hls4ml-0.7.0rc1/test/pytest/test_report/
+-rw-r--r--   0 runner    (1001) docker     (123)    13881 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_report/myproject_csynth.rpt
+-rw-r--r--   0 runner    (1001) docker     (123)    21009 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_report/myproject_csynth.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      943 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_report/vivado_hls.app
+-rw-r--r--   0 runner    (1001) docker     (123)     7230 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_report/vivado_synth.rpt
+-rw-r--r--   0 runner    (1001) docker     (123)     2906 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_report.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1366 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_reshape.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5149 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_rnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2255 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_sepconv2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3656 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_softsign.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1776 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_trace.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1839 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_transpose_concat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1912 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_upsampling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1986 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytest/test_zeropadding.py
+-rw-r--r--   0 runner    (1001) docker     (123)      616 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytorch-models.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2351 2023-04-15 01:44:58.000000 hls4ml-0.7.0rc1/test/pytorch-to-hls.sh
```

### Comparing `hls4ml-0.6.0/LICENSE` & `hls4ml-0.7.0rc1/LICENSE`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/example-prjs/conv2d-1layer/firmware/parameters.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv1d.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,91 +1,66 @@
-#ifndef PARAMETERS_H_
-#define PARAMETERS_H_
+#ifndef NNET_CONV1D_H_
+#define NNET_CONV1D_H_
 
-#include <complex>
-#include "ap_int.h"
-#include "ap_fixed.h"
-#include "nnet_dense.h"
-#include "nnet_conv.h"
-#include "nnet_conv2d.h"
-#include "nnet_activation.h"
 #include "nnet_common.h"
+#include "nnet_conv1d_latency.h"
+#include "nnet_conv1d_resource.h"
+#include <cstdlib>
 
-//hls-fpga-machine-learning insert numbers
-typedef ap_fixed<18,8> accum_default_t;
-typedef ap_fixed<18,8> weight_default_t;
-typedef ap_fixed<18,8> bias_default_t;
-typedef ap_fixed<18,8> input_t;
-typedef ap_fixed<18,8> result_t;
-#define IN_HEIGHT 8
-#define IN_WIDTH 8
-#define N_CHAN 1
-#define FILT_HEIGHT 3
-#define FILT_WIDTH 3
-#define N_FILT 2
-#define STRIDE_HEIGHT 1
-#define STRIDE_WIDTH 1
-#define PAD_LEFT 1
-#define PAD_RIGHT 1
-#define PAD_TOP 1
-#define PAD_BOTTOM 1
-#define OUT_HEIGHT 8
-#define OUT_WIDTH 8
-#define N_OUTPUTS 10
-
-//hls-fpga-machine-learning insert layer-precision
-//typedef ap_fixed<18,8> conv1_t;
-
-//hls-fpga-machine-learning insert layer-config
-struct config1 : nnet::conv2d_config {
+namespace nnet {
 
+struct conv1d_config {
     // Internal data type definitions
-    typedef accum_default_t accum_t;
-    typedef bias_default_t bias_t;
-    typedef weight_default_t weight_t;
+    typedef float bias_t;
+    typedef float weight_t;
+    typedef float accum_t;
 
     // Convolutional parameters
-    static const unsigned pad_top = PAD_TOP;
-    static const unsigned pad_bottom = PAD_BOTTOM;
-    static const unsigned pad_left = PAD_LEFT;
-    static const unsigned pad_right = PAD_RIGHT;
-    static const unsigned in_height = IN_HEIGHT;
-    static const unsigned in_width = IN_WIDTH;
-    static const unsigned n_chan = N_CHAN;
-    static const unsigned filt_height = FILT_HEIGHT;
-    static const unsigned filt_width = FILT_WIDTH;
-    static const unsigned n_filt = N_FILT;
-    static const unsigned stride_height = STRIDE_HEIGHT;
-    static const unsigned stride_width = STRIDE_WIDTH;
-    static const unsigned out_height = OUT_HEIGHT;
-    static const unsigned out_width = OUT_WIDTH;
+    static const unsigned pad_left = 0;
+    static const unsigned pad_right = 0;
+    static const unsigned in_width = 10;
+    static const unsigned n_chan = 0;
+    static const unsigned filt_width = 1;
+    static const unsigned kernel_size = filt_width;
+    static const unsigned n_filt = 1;
+    static const unsigned stride_width = 1;
+    static const unsigned dilation = 1;
+    static const unsigned out_width = 10; //(N_IN + PAD_LEFT * PAD_RIGHT - (DILATION * (FILT_WIDTH - 1) + 1)) / STRIDE + 1
 
     static const unsigned reuse_factor = 1;
     static const bool store_weights_in_bram = false;
-    static const unsigned n_zeros = 0; // not used yet                                                                                                                           
-};
-
-struct relu_config1 : nnet::activ_config {
-    static const unsigned n_in = OUT_HEIGHT*OUT_WIDTH*N_FILT;
-    static const unsigned table_size = 1024;
-    static const unsigned io_type = nnet::io_parallel;
+    static const unsigned n_zeros = 0; // not used yet
 };
 
-struct config2 : nnet::dense_config {
-    static const unsigned n_in = OUT_HEIGHT*OUT_WIDTH*N_FILT;
-    static const unsigned n_out = N_OUTPUTS;
-    static const unsigned io_type = nnet::io_parallel;
-    static const unsigned reuse_factor = 1;
-    static const unsigned n_zeros = 0;
-    static const bool store_weights_in_bram = false;
-    typedef accum_default_t accum_t;
-    typedef bias_default_t bias_t;
-    typedef weight_default_t weight_t;
-};
-struct softmax_config2 : nnet::activ_config {
-    static const unsigned n_in = N_OUTPUTS;
-    static const unsigned table_size = 1024;
-    static const unsigned io_type = nnet::io_parallel;
-};
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_1d_cl(data_T data[CONFIG_T::in_width * CONFIG_T::n_chan], res_T res[CONFIG_T::out_width * CONFIG_T::n_filt],
+                typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    #pragma HLS INLINE region
+
+    if (CONFIG_T::strategy == nnet::latency) {
+        conv_1d_latency_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+    } else {
+        conv_1d_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+    }
+}
+
+template <class data_T, class res_T, typename CONFIG_T>
+void pointwise_conv_1d_cl(data_T data[CONFIG_T::in_width * CONFIG_T::n_chan],
+                          res_T res[CONFIG_T::out_width * CONFIG_T::n_filt],
+                          typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
+                          typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    assert(CONFIG_T::filt_width == 1);
+
+    #pragma HLS INLINE region
+
+    // Nothing special to be done for io_parallel implementation
+    if (CONFIG_T::strategy == nnet::latency) {
+        conv_1d_latency_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+    } else {
+        conv_1d_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+    }
+}
 
+} // namespace nnet
 
-#endif 
+#endif
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/__init__.py` & `hls4ml-0.7.0rc1/hls4ml/converters/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,77 +1,90 @@
-from __future__ import absolute_import
-import os
-import yaml
 import importlib
+import os
 import warnings
 
-from hls4ml.utils.config import create_config
+import yaml
 
-from hls4ml.converters.keras_to_hls import keras_to_hls, get_supported_keras_layers, register_keras_layer_handler
+from hls4ml.converters.keras_to_hls import KerasFileReader  # noqa: F401
+from hls4ml.converters.keras_to_hls import KerasModelReader  # noqa: F401
+from hls4ml.converters.keras_to_hls import get_supported_keras_layers  # noqa: F401
+from hls4ml.converters.keras_to_hls import parse_keras_model  # noqa: F401
+from hls4ml.converters.keras_to_hls import keras_to_hls, register_keras_layer_handler
+from hls4ml.utils.config import create_config
 
-#----------Make converters available if the libraries can be imported----------#       
+# ----------Make converters available if the libraries can be imported----------#
 try:
-    from hls4ml.converters.pytorch_to_hls import pytorch_to_hls, get_supported_pytorch_layers, register_pytorch_layer_handler
+    from hls4ml.converters.pytorch_to_hls import (  # noqa: F401
+        get_supported_pytorch_layers,
+        pytorch_to_hls,
+        register_pytorch_layer_handler,
+    )
+
     __pytorch_enabled__ = True
 except ImportError:
-    warnings.warn("WARNING: Pytorch converter is not enabled!")
+    warnings.warn("WARNING: Pytorch converter is not enabled!", stacklevel=1)
     __pytorch_enabled__ = False
 
 try:
-    from hls4ml.converters.onnx_to_hls import onnx_to_hls, get_supported_onnx_layers, register_onnx_layer_handler
+    from hls4ml.converters.onnx_to_hls import get_supported_onnx_layers  # noqa: F401
+    from hls4ml.converters.onnx_to_hls import onnx_to_hls, register_onnx_layer_handler
+
     __onnx_enabled__ = True
 except ImportError:
-    warnings.warn("WARNING: ONNX converter is not enabled!")
+    warnings.warn("WARNING: ONNX converter is not enabled!", stacklevel=1)
     __onnx_enabled__ = False
 
 try:
     from hls4ml.converters.tf_to_hls import tf_to_hls
+
     __tensorflow_enabled__ = True
 except ImportError:
-    warnings.warn("WARNING: Tensorflow converter is not enabled!")
+    warnings.warn("WARNING: Tensorflow converter is not enabled!", stacklevel=1)
     __tensorflow_enabled__ = False
 
-#----------Layer handling register----------#
+# ----------Layer handling register----------#
 model_types = ['keras', 'pytorch', 'onnx']
 
 for model_type in model_types:
-    for module in os.listdir(os.path.dirname(__file__) + '/{}'.format(model_type)):
+    for module in os.listdir(os.path.dirname(__file__) + f'/{model_type}'):
         if module == '__init__.py' or module[-3:] != '.py':
             continue
         try:
-            lib = importlib.import_module(__name__ + '.{}.'.format(model_type) + module[:-3])
-            for name, func in list(lib.__dict__.items()):
+            lib = importlib.import_module(__name__ + f'.{model_type}.' + module[:-3])
+            for _, func in list(lib.__dict__.items()):
                 # if 'func' is callable (i.e., function, class...)
                 # and has 'handles' attribute
                 # and is defined in this module (i.e., not imported)
                 if callable(func) and hasattr(func, 'handles') and func.__module__ == lib.__name__:
                     for layer in func.handles:
-                        
+
                         if model_type == 'keras':
                             register_keras_layer_handler(layer, func)
                         elif model_type == 'pytorch':
                             register_pytorch_layer_handler(layer, func)
                         elif model_type == 'onnx':
                             register_onnx_layer_handler(layer, func)
-                            
-        except ImportError:
+
+        except ImportError as err:
+            print(f'WARNING: Failed to import handlers from {module}: {err.msg}.')
             continue
 
+
 def parse_yaml_config(config_file):
     """Parse conversion configuration from the provided YAML file.
 
     This function parses the conversion configuration contained in the YAML
     file provided as an argument. It ensures proper serialization of hls4ml
     objects and should be called on YAML files created by hls4ml. A minimal
     valid YAML file may look like this::
 
         KerasH5: my_keras_model.h5
         OutputDir: my-hls-test
         ProjectName: myproject
-        XilinxPart: xcku115-flvb2104-2-i
+        Part: xcku115-flvb2104-2-i
         ClockPeriod: 5
         IOType: io_stream
         HLSConfig:
             Model:
             Precision: ap_fixed<16,6>
             ReuseFactor: 10
 
@@ -79,43 +92,45 @@
 
     Arguments:
         config_file (str): Location of the file on the filesystem.
 
     Returns:
         dict: Parsed configuration.
     """
+
     def construct_keras_model(loader, node):
         from tensorflow.keras.models import load_model
 
         model_str = loader.construct_scalar(node)
         return load_model(model_str)
 
-    yaml.add_constructor(u'!keras_model', construct_keras_model, Loader=yaml.SafeLoader)
+    yaml.add_constructor('!keras_model', construct_keras_model, Loader=yaml.SafeLoader)
 
     print('Loading configuration from', config_file)
-    with open(config_file, 'r') as file:
+    with open(config_file) as file:
         parsed_config = yaml.safe_load(file)
     return parsed_config
 
+
 def convert_from_config(config):
     """Convert to hls4ml model based on the provided configuration.
 
     Arguments:
         config: A string containing the path to the YAML configuration file on
-            the filesystem or a dict containig the parsed configuration.
+            the filesystem or a dict containing the parsed configuration.
 
     Returns:
-        HLSModel: hls4ml model.
+        ModelGraph: hls4ml model.
     """
 
     if isinstance(config, str):
         yamlConfig = parse_yaml_config(config)
     else:
         yamlConfig = config
-        
+
     model = None
     if 'OnnxModel' in yamlConfig:
         if __onnx_enabled__:
             model = onnx_to_hls(yamlConfig)
         else:
             raise Exception("ONNX not found. Please install ONNX.")
     elif 'PytorchModel' in yamlConfig:
@@ -128,49 +143,63 @@
             model = tf_to_hls(yamlConfig)
         else:
             raise Exception("TensorFlow not found. Please install TensorFlow.")
     else:
         model = keras_to_hls(yamlConfig)
 
     return model
-  
-def _check_hls_config(config, hls_config):  
+
+
+def _check_hls_config(config, hls_config):
     """
     Check hls_config for to set appropriate parameters for config.
     """
-    
+
     if 'LayerName' in hls_config:
         config['HLSConfig']['LayerName'] = hls_config['LayerName']
 
     if 'LayerType' in hls_config:
         config['HLSConfig']['LayerType'] = hls_config['LayerType']
 
+    if 'Flows' in hls_config:
+        config['HLSConfig']['Flows'] = hls_config['Flows']
+
     if 'Optimizers' in hls_config:
         config['HLSConfig']['Optimizers'] = hls_config['Optimizers']
 
     if 'SkipOptimizers' in hls_config:
         config['HLSConfig']['SkipOptimizers'] = hls_config['SkipOptimizers']
-    
+
     return
 
-def _check_model_config(model_config):    
+
+def _check_model_config(model_config):
     if model_config is not None:
         if not all(k in model_config for k in ('Precision', 'ReuseFactor')):
             raise Exception('Precision and ReuseFactor must be provided in the hls_config')
     else:
         model_config = {}
         model_config['Precision'] = 'ap_fixed<16,6>'
-        model_config['ReuseFactor'] = '1'
-        
+        model_config['ReuseFactor'] = 1
+
     return model_config
-  
-def convert_from_keras_model(model, output_dir='my-hls-test', project_name='myproject', input_data_tb=None,
-                             output_data_tb=None, backend='Vivado', board=None, part=None, clock_period=5, io_type='io_parallel', 
-                             hls_config={}, **kwargs):
+
+
+def convert_from_keras_model(
+    model,
+    output_dir='my-hls-test',
+    project_name='myproject',
+    input_data_tb=None,
+    output_data_tb=None,
+    backend='Vivado',
+    hls_config=None,
+    **kwargs,
+):
     """Convert to hls4ml model based on the provided configuration.
+
     Args:
         model: Keras model to convert
         output_dir (str, optional): Output directory of the generated HLS
             project. Defaults to 'my-hls-test'.
         project_name (str, optional): Name of the HLS project.
             Defaults to 'myproject'.
         input_data_tb (str, optional): String representing the path of input data in .npy or .dat format that will be
@@ -183,54 +212,58 @@
             device of a backend will be used. See documentation of the backend used.
         part (str, optional): The FPGA part. If set to `None` a default part of a backend will be used.
             See documentation of the backend used. Note that if `board` is specified, the part associated to that board
             will overwrite any part passed as a parameter.
         clock_period (int, optional): Clock period of the design.
             Defaults to 5.
         io_type (str, optional): Type of implementation used. One of
-            'io_parallel' or 'io_serial'. Defaults to 'io_parallel'.
+            'io_parallel' or 'io_stream'. Defaults to 'io_parallel'.
         hls_config (dict, optional): The HLS config.
         kwargs** (dict, optional): Additional parameters that will be used to create the config of the specified backend
+
     Raises:
         Exception: If precision and reuse factor are not present in 'hls_config'
+
     Returns:
-        HLSModel: hls4ml model.
+        ModelGraph: hls4ml model.
     """
 
-    config = create_config(
-        output_dir=output_dir,
-        project_name=project_name,
-        board=board,
-        part=part,
-        clock_period=clock_period,
-        io_type=io_type,
-        backend=backend,
-        **kwargs
-    )
+    config = create_config(output_dir=output_dir, project_name=project_name, backend=backend, **kwargs)
 
     config['KerasModel'] = model
     config['InputData'] = input_data_tb
     config['OutputPredictions'] = output_data_tb
     config['HLSConfig'] = {}
 
+    if hls_config is None:
+        hls_config = {}
+
     model_config = hls_config.get('Model', None)
     config['HLSConfig']['Model'] = _check_model_config(model_config)
-    
+
     _check_hls_config(config, hls_config)
 
     return keras_to_hls(config)
 
 
-def convert_from_pytorch_model(model, input_shape, output_dir='my-hls-test', project_name='myproject', input_data_tb=None,
-                             output_data_tb=None, backend='Vivado', board=None, part=None, clock_period=5, io_type='io_parallel',
-                             hls_config={}, **kwargs):
+def convert_from_pytorch_model(
+    model,
+    input_shape,
+    output_dir='my-hls-test',
+    project_name='myproject',
+    input_data_tb=None,
+    output_data_tb=None,
+    backend='Vivado',
+    hls_config=None,
+    **kwargs,
+):
     """
-    
+
     Convert a Pytorch model to a hls model.
-    
+
     Parameters
     ----------
     model : Pytorch model object.
         Model to be converted to hls model object.
     input_shape : @todo: to be filled
     output_dir (str, optional): Output directory of the generated HLS
         project. Defaults to 'my-hls-test'.
@@ -246,69 +279,70 @@
         device of a backend will be used. See documentation of the backend used.
     part (str, optional): The FPGA part. If set to `None` a default part of a backend will be used.
         See documentation of the backend used. Note that if `board` is specified, the part associated to that board
         will overwrite any part passed as a parameter.
     clock_period (int, optional): Clock period of the design.
         Defaults to 5.
     io_type (str, optional): Type of implementation used. One of
-        'io_parallel' or 'io_serial'. Defaults to 'io_parallel'.
+        'io_parallel' or 'io_stream'. Defaults to 'io_parallel'.
     hls_config (dict, optional): The HLS config.
     kwargs** (dict, optional): Additional parameters that will be used to create the config of the specified backend
-        
+
     Returns
     -------
-    hls_model : hls4ml model object.
-        
+    ModelGraph : hls4ml model object.
+
     See Also
     --------
     hls4ml.convert_from_keras_model, hls4ml.convert_from_onnx_model
-    
+
     Examples
     --------
     >>> import hls4ml
     >>> config = hls4ml.utils.config_from_pytorch_model(model, granularity='model')
     >>> hls_model = hls4ml.converters.convert_from_pytorch_model(model, hls_config=config)
-    
+
     Notes
     -----
     Only sequential Pytorch models are supported for now.
     """
 
-    config = create_config(
-        output_dir=output_dir,
-        project_name=project_name,
-        board=board,
-        part=part,
-        clock_period=clock_period,
-        io_type=io_type,
-        backend=backend,
-        **kwargs
-    )
-    
+    config = create_config(output_dir=output_dir, project_name=project_name, backend=backend, **kwargs)
+
     config['PytorchModel'] = model
     config['InputShape'] = input_shape
     config['InputData'] = input_data_tb
     config['OutputPredictions'] = output_data_tb
     config['HLSConfig'] = {}
 
+    if hls_config is None:
+        hls_config = {}
+
     model_config = hls_config.get('Model', None)
     config['HLSConfig']['Model'] = _check_model_config(model_config)
-    
+
     _check_hls_config(config, hls_config)
-    
+
     return pytorch_to_hls(config)
 
 
-def convert_from_onnx_model(model, output_dir='my-hls-test', project_name='myproject', input_data_tb=None,
-                             output_data_tb=None, backend='Vivado', board=None, part=None, clock_period=5, io_type='io_parallel',
-                             hls_config={}, **kwargs):
+def convert_from_onnx_model(
+    model,
+    output_dir='my-hls-test',
+    project_name='myproject',
+    input_data_tb=None,
+    output_data_tb=None,
+    backend='Vivado',
+    hls_config=None,
+    **kwargs,
+):
     """
-    
+
     Convert an ONNX model to a hls model.
-    
+
     Parameters
     ----------
     model : ONNX model object.
         Model to be converted to hls model object.
     output_dir (str, optional): Output directory of the generated HLS
         project. Defaults to 'my-hls-test'.
     project_name (str, optional): Name of the HLS project.
@@ -323,50 +357,42 @@
         device of a backend will be used. See documentation of the backend used.
     part (str, optional): The FPGA part. If set to `None` a default part of a backend will be used.
         See documentation of the backend used. Note that if `board` is specified, the part associated to that board
         will overwrite any part passed as a parameter.
     clock_period (int, optional): Clock period of the design.
         Defaults to 5.
     io_type (str, optional): Type of implementation used. One of
-        'io_parallel' or 'io_serial'. Defaults to 'io_parallel'.
+        'io_parallel' or 'io_stream'. Defaults to 'io_parallel'.
     hls_config (dict, optional): The HLS config.
     kwargs** (dict, optional): Additional parameters that will be used to create the config of the specified backend
-        
+
     Returns
     -------
-    hls_model : hls4ml model object.
-        
+    ModelGraph : hls4ml model object.
+
     See Also
     --------
     hls4ml.convert_from_keras_model, hls4ml.convert_from_pytorch_model
-    
+
     Examples
     --------
     >>> import hls4ml
     >>> config = hls4ml.utils.config_from_onnx_model(model, granularity='model')
     >>> hls_model = hls4ml.converters.convert_from_onnx_model(model, hls_config=config)
     """
 
-    config = create_config(
-        output_dir=output_dir,
-        project_name=project_name,
-        board=board,
-        part=part,
-        clock_period=clock_period,
-        io_type=io_type,
-        backend=backend,
-        **kwargs
-    )
+    config = create_config(output_dir=output_dir, project_name=project_name, backend=backend, **kwargs)
 
     config['OnnxModel'] = model
     config['InputData'] = input_data_tb
     config['OutputPredictions'] = output_data_tb
     config['HLSConfig'] = {}
 
+    if hls_config is None:
+        hls_config = {}
+
     model_config = hls_config.get('Model', None)
     config['HLSConfig']['Model'] = _check_model_config(model_config)
-    
-    _check_hls_config(config, hls_config)
-    
-    return onnx_to_hls(config)
 
+    _check_hls_config(config, hls_config)
 
+    return onnx_to_hls(config)
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/convolution.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras/convolution.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,84 +1,69 @@
-import math
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
-from hls4ml.converters.utils import parse_data_format, compute_padding_1d, compute_padding_2d
+from hls4ml.converters.keras_to_hls import keras_handler, parse_default_keras_layer
+from hls4ml.converters.utils import compute_padding_1d, compute_padding_2d, parse_data_format
+
 
 @keras_handler('Conv1D', 'SeparableConv1D')
-def parse_conv1d_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert('Conv1D' in keras_layer['class_name'])
+def parse_conv1d_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'Conv1D' in keras_layer['class_name']
 
     layer = parse_default_keras_layer(keras_layer, input_names)
-    
-    (
-        layer['in_width'],
-        layer['n_chan']
-    ) = parse_data_format(input_shapes[0], layer['data_format'])
+
+    (layer['in_width'], layer['n_chan']) = parse_data_format(input_shapes[0], layer['data_format'])
 
     layer['n_filt'] = keras_layer['config']['filters']
     layer['filt_width'] = keras_layer['config']['kernel_size'][0]
     layer['stride_width'] = keras_layer['config']['strides'][0]
     layer['padding'] = keras_layer['config']['padding']
 
-    (
-        layer['out_width'],
-        layer['pad_left'],
-        layer['pad_right']
-    ) = compute_padding_1d(
-        layer['padding'],
-        layer['in_width'],
-        layer['stride_width'],
-        layer['filt_width']
+    (layer['out_width'], layer['pad_left'], layer['pad_right']) = compute_padding_1d(
+        layer['padding'], layer['in_width'], layer['stride_width'], layer['filt_width']
     )
 
     if layer['data_format'] == 'channels_last':
         output_shape = [input_shapes[0][0], layer['out_width'], layer['n_filt']]
     elif layer['data_format'] == 'channels_first':
         output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_width']]
 
     return layer, output_shape
 
 
 @keras_handler('Conv2D', 'SeparableConv2D', 'DepthwiseConv2D')
-def parse_conv2d_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert('Conv2D' in keras_layer['class_name'])
+def parse_conv2d_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'Conv2D' in keras_layer['class_name']
 
     layer = parse_default_keras_layer(keras_layer, input_names)
-    
-    (
-        layer['in_height'],
-        layer['in_width'],
-        layer['n_chan']
-    ) = parse_data_format(input_shapes[0], layer['data_format'])
+
+    (layer['in_height'], layer['in_width'], layer['n_chan']) = parse_data_format(input_shapes[0], layer['data_format'])
 
     if 'filters' in keras_layer['config']:
         layer['n_filt'] = keras_layer['config']['filters']
-    else:    
+    else:
         layer['n_filt'] = layer['n_chan']
     layer['filt_height'] = keras_layer['config']['kernel_size'][0]
     layer['filt_width'] = keras_layer['config']['kernel_size'][1]
     layer['stride_height'] = keras_layer['config']['strides'][0]
     layer['stride_width'] = keras_layer['config']['strides'][1]
     layer['padding'] = keras_layer['config']['padding']
-    
+
     (
         layer['out_height'],
         layer['out_width'],
         layer['pad_top'],
         layer['pad_bottom'],
         layer['pad_left'],
-        layer['pad_right']
+        layer['pad_right'],
     ) = compute_padding_2d(
         layer['padding'],
         layer['in_height'],
         layer['in_width'],
         layer['stride_height'],
         layer['stride_width'],
         layer['filt_height'],
-        layer['filt_width']
+        layer['filt_width'],
     )
 
     if layer['data_format'] == 'channels_first':
         output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_height'], layer['out_width']]
     else:
         output_shape = [input_shapes[0][0], layer['out_height'], layer['out_width'], layer['n_filt']]
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/graph.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras/graph.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
 from hls4ml.converters.keras.core import TernaryQuantizer
+from hls4ml.converters.keras_to_hls import keras_handler, parse_default_keras_layer
+
 
 @keras_handler('GarNet', 'GarNetStack')
-def parse_garnet_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer['class_name'] in ['GarNet', 'GarNetStack'])
+def parse_garnet_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer['class_name'] in ['GarNet', 'GarNetStack']
 
     if not keras_layer['config']['simplified']:
         raise Exception('HLS GarNet is compatible only with keras GarNet with simplified=True')
-    if keras_layer['config']['output_activation'] is not None:
-        raise Exception('HLS GarNet cannot have output activation')
+    if keras_layer['config']['output_activation'] not in [None, 'linear']:
+        raise Exception('HLS GarNet cannot have nonlinear output activation')
 
     layer = parse_default_keras_layer(keras_layer, input_names)
 
     layer['input_format'] = keras_layer['config']['input_format']
     if layer['input_format'] != 'xn':
         raise NotImplementedError('HLS GarNet currently only implements signed inputs (input_format="xn")')
 
     layer['n_vertices'] = input_shapes[0][1]
     layer['collapse'] = keras_layer['config']['collapse']
     layer['mean_by_nvert'] = keras_layer['config']['mean_by_nvert']
     if keras_layer['config']['quantize_transforms']:
         layer['quantizer'] = TernaryQuantizer()
 
     layer['n_aggregators'] = keras_layer['config']['n_aggregators']
-    layer['n_out_features'] = keras_layer['config']['n_filters'] # number of output features
-    layer['n_propagate'] = keras_layer['config']['n_propagate'] # number of latent features
+    layer['n_out_features'] = keras_layer['config']['n_filters']  # number of output features
+    layer['n_propagate'] = keras_layer['config']['n_propagate']  # number of latent features
 
     if layer['class_name'] == 'GarNet':
         layer['n_in_features'] = input_shapes[0][2]
         n_out_features = layer['n_out_features']
 
     elif layer['class_name'] == 'GarNetStack':
         layer['n_sublayers'] = keras_layer['config']['n_sublayers']
         layer['n_in_features'] = [input_shapes[0][2]]
 
         for il in range(1, layer['n_sublayers']):
             layer['n_in_features'].append(layer['n_out_features'][il - 1])
 
         n_out_features = layer['n_out_features'][-1]
-        
+
     if layer['collapse'] in ['mean', 'sum', 'max']:
         output_shape = [input_shapes[0][0], n_out_features]
     else:
         output_shape = input_shapes[0][:2] + [n_out_features]
 
     return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/merge.py` & `hls4ml-0.7.0rc1/hls4ml/converters/onnx/merge.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,31 +1,43 @@
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
+from hls4ml.converters.onnx_to_hls import get_onnx_attribute, get_onnx_input_name, onnx_handler
 
-merge_layers = ['Add', 'Subtract', 'Multiply', 'Average', 'Maximum', 'Minimum', 'Concatenate', 'Dot']
-@keras_handler(*merge_layers)
-def parse_merge_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer['class_name'] in merge_layers)
+merge_layers = ['Add', 'Sub', 'Mul', 'Average', 'Max', 'Min', 'Concat', 'Sum']
 
-    layer = parse_default_keras_layer(keras_layer, input_names)
 
+@onnx_handler(*merge_layers)
+def parse_merge_layer(reader, node, inputs_map, input_shapes, graph, config):
+
+    layer = {}
+    layer['class_name'] = node.op_type
+    layer['name'] = node.name
     layer['op'] = layer['class_name'].lower()
+    layer['inputs'] = get_onnx_input_name(node, graph)
+    output_shape = input_shapes[0]
 
-    output_shape = input_shapes[0][:]
-    if layer['class_name'] == 'Concatenate':
+    if layer['class_name'] == 'Concat':
         rank = len(input_shapes[0][1:])
         if rank > 3:
             raise Exception('ERROR: Concatenation of tensors with rank > 3 is not yet supported.')
-        layer['op'] = layer['class_name'].lower() + '{}d'.format(rank)
-        layer['axis'] = keras_layer['config']['axis']
-        output_shape[layer['axis']] += input_shapes[1][layer['axis']]
-    elif layer['class_name'] == 'Dot':
-        rank = len(input_shapes[0][1:])
-        if rank > 1:
-            raise Exception('ERROR: Dot of tensors with rank > 1 is not yet supported.')
-        layer['op'] = layer['class_name'].lower() + '{}d'.format(rank) 
+
+        layer['class_name'] = 'Concatenate'
+        layer['op'] = layer['class_name'].lower() + f'{rank}d'
+        layer['axis'] = get_onnx_attribute(node, 'axis')
+
+        # Calculate output shape
+        new_dim = sum(
+            [x.type.tensor_type.shape.dim[layer['axis']].dim_value for x in graph.value_info if x.name in node.input]
+        )
+        output_shape[layer['axis']] = new_dim
+
+    elif layer['class_name'] == 'Add':
+        # Check if the layer is an AddBias
+        for input in node.input:
+            if "bias" in input:
+                layer['class_name'] = 'BiasAdd'
+                reader.add_input(layer['name'], node.input)
     else:
         layer['class_name'] = 'Merge'
+
     if len(layer['inputs']) > 2:
         raise Exception('ERROR: Merging more than two tensors is not yet supported.')
 
     return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/pooling.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras/pooling.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,98 +1,93 @@
-import math
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
-from hls4ml.converters.utils import parse_data_format, compute_padding_1d, compute_padding_2d
+from hls4ml.converters.keras_to_hls import keras_handler, parse_default_keras_layer
+from hls4ml.converters.utils import compute_padding_1d, compute_padding_2d, parse_data_format
 
 pooling_layers = ['MaxPooling1D', 'MaxPooling2D', 'AveragePooling1D', 'AveragePooling2D']
+
+
 @keras_handler(*pooling_layers)
-def parse_pooling_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert('Pooling' in keras_layer['class_name'])
+def parse_pooling_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'Pooling' in keras_layer['class_name']
 
     layer = parse_default_keras_layer(keras_layer, input_names)
 
     if int(layer['class_name'][-2]) == 1:
-        (
-            layer['n_in'],
-            layer['n_filt']
-        ) = parse_data_format(input_shapes[0], layer['data_format'])
-        
-        layer['pool_width']=keras_layer['config']['pool_size'][0]
-        layer['stride_width']=keras_layer['config']['strides'][0]
-        layer['padding']=keras_layer['config']['padding']
+        (layer['n_in'], layer['n_filt']) = parse_data_format(input_shapes[0], layer['data_format'])
 
-        (
-            layer['n_out'],
-            layer['pad_left'],
-            layer['pad_right']
-        ) = compute_padding_1d(
-            layer['padding'],
-            layer['n_in'],
-            layer['stride_width'],
-            layer['pool_width']
+        layer['pool_width'] = keras_layer['config']['pool_size'][0]
+        layer['stride_width'] = keras_layer['config']['strides'][0]
+        layer['padding'] = keras_layer['config']['padding']
+
+        (layer['n_out'], layer['pad_left'], layer['pad_right']) = compute_padding_1d(
+            layer['padding'], layer['n_in'], layer['stride_width'], layer['pool_width']
         )
 
         if layer['data_format'] == 'channels_last':
-            output_shape=[input_shapes[0][0], layer['n_out'], layer['n_filt']]
+            output_shape = [input_shapes[0][0], layer['n_out'], layer['n_filt']]
         elif layer['data_format'] == 'channels_first':
-            output_shape=[input_shapes[0][0], layer['n_filt'], layer['n_out']]
+            output_shape = [input_shapes[0][0], layer['n_filt'], layer['n_out']]
     elif int(layer['class_name'][-2]) == 2:
-        (
-            layer['in_height'],
-            layer['in_width'],
-            layer['n_filt']
-        ) = parse_data_format(input_shapes[0], layer['data_format'])
+        (layer['in_height'], layer['in_width'], layer['n_filt']) = parse_data_format(input_shapes[0], layer['data_format'])
 
-        layer['stride_height']=keras_layer['config']['strides'][0]
-        layer['stride_width']=keras_layer['config']['strides'][1]
-        layer['pool_height']=keras_layer['config']['pool_size'][0]
-        layer['pool_width']=keras_layer['config']['pool_size'][1]
-        layer['padding']=keras_layer['config']['padding']
+        layer['stride_height'] = keras_layer['config']['strides'][0]
+        layer['stride_width'] = keras_layer['config']['strides'][1]
+        layer['pool_height'] = keras_layer['config']['pool_size'][0]
+        layer['pool_width'] = keras_layer['config']['pool_size'][1]
+        layer['padding'] = keras_layer['config']['padding']
 
         (
             layer['out_height'],
             layer['out_width'],
             layer['pad_top'],
             layer['pad_bottom'],
             layer['pad_left'],
-            layer['pad_right']
+            layer['pad_right'],
         ) = compute_padding_2d(
             layer['padding'],
             layer['in_height'],
             layer['in_width'],
             layer['stride_height'],
             layer['stride_width'],
             layer['pool_height'],
-            layer['pool_width']
+            layer['pool_width'],
         )
 
         if layer['data_format'] == 'channels_last':
-            output_shape=[input_shapes[0][0], layer['out_height'], layer['out_width'], layer['n_filt']]
+            output_shape = [input_shapes[0][0], layer['out_height'], layer['out_width'], layer['n_filt']]
         elif layer['data_format'] == 'channels_first':
-            output_shape=[input_shapes[0][0], layer['n_filt'], layer['out_height'], layer['out_width']]
-    
+            output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_height'], layer['out_width']]
+
     return layer, output_shape
 
+
 global_pooling_layers = ['GlobalMaxPooling1D', 'GlobalMaxPooling2D', 'GlobalAveragePooling1D', 'GlobalAveragePooling2D']
+
+
 @keras_handler(*global_pooling_layers)
-def parse_global_pooling_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert('Pooling' in keras_layer['class_name'])
+def parse_global_pooling_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'Pooling' in keras_layer['class_name']
 
     layer = parse_default_keras_layer(keras_layer, input_names)
+    layer['keepdims'] = keras_layer['config']['keepdims']
 
     if int(layer['class_name'][-2]) == 1:
-        (
-            layer['n_in'],
-            layer['n_filt']
-        ) = parse_data_format(input_shapes[0], layer['data_format'])
-        
-        output_shape=[input_shapes[0][0], layer['n_filt']]
+        (layer['n_in'], layer['n_filt']) = parse_data_format(input_shapes[0], layer['data_format'])
+
+        if layer['keepdims']:
+            if layer['data_format'] == 'channels_last':
+                output_shape = [input_shapes[0][0], 1, layer['n_filt']]
+            elif layer['data_format'] == 'channels_first':
+                output_shape = [input_shapes[0][0], layer['n_filt'], 1]
+        else:
+            output_shape = [input_shapes[0][0], layer['n_filt']]
     elif int(layer['class_name'][-2]) == 2:
-        (
-            layer['in_height'],
-            layer['in_width'],
-            layer['n_filt']
-        ) = parse_data_format(input_shapes[0], layer['data_format'])
+        (layer['in_height'], layer['in_width'], layer['n_filt']) = parse_data_format(input_shapes[0], layer['data_format'])
+
+        if layer['keepdims']:
+            if layer['data_format'] == 'channels_last':
+                output_shape = [input_shapes[0][0], 1, 1, layer['n_filt']]
+            elif layer['data_format'] == 'channels_first':
+                output_shape = [input_shapes[0][0], layer['n_filt'], 1, 1]
+        else:
+            output_shape = [input_shapes[0][0], layer['n_filt']]
 
-        output_shape=[input_shapes[0][0], layer['n_filt']]
-    
     return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/qkeras_layers.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras/qkeras.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,63 +1,79 @@
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
+from qkeras.quantizers import get_quantizer
 
-from hls4ml.converters.keras.core import parse_dense_layer
-from hls4ml.converters.keras.core import parse_batchnorm_layer
-from hls4ml.converters.keras.convolution import parse_conv1d_layer
-from hls4ml.converters.keras.convolution import parse_conv2d_layer
-from hls4ml.converters.keras.qkeras import *
-
-import tensorflow as tf
+from hls4ml.converters.keras.convolution import parse_conv1d_layer, parse_conv2d_layer
+from hls4ml.converters.keras.core import parse_batchnorm_layer, parse_dense_layer
+from hls4ml.converters.keras_to_hls import keras_handler, parse_default_keras_layer
+from hls4ml.model.types import FixedPrecisionType, QKerasBinaryQuantizer, QKerasPO2Quantizer, QKerasQuantizer
+
+
+def get_quantizer_from_config(keras_layer, quantizer_var):
+    quantizer_config = keras_layer['config'][f'{quantizer_var}_quantizer']
+    if keras_layer['class_name'] == 'QBatchNormalization':
+        return QKerasQuantizer(quantizer_config)
+    elif 'binary' in quantizer_config['class_name']:
+        return QKerasBinaryQuantizer(quantizer_config, xnor=(quantizer_var == 'kernel'))
+    elif quantizer_config['class_name'] == 'quantized_po2':
+        return QKerasPO2Quantizer(quantizer_config)
+    else:
+        return QKerasQuantizer(quantizer_config)
 
 
 @keras_handler('QDense')
-def parse_qdense_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    
-    
-    layer, output_shape = parse_dense_layer(keras_layer, input_names, input_shapes, data_reader, config)
+def parse_qdense_layer(keras_layer, input_names, input_shapes, data_reader):
+
+    layer, output_shape = parse_dense_layer(keras_layer, input_names, input_shapes, data_reader)
 
     layer['weight_quantizer'] = get_quantizer_from_config(keras_layer, 'kernel')
     if keras_layer['config']['bias_quantizer'] is not None:
         layer['bias_quantizer'] = get_quantizer_from_config(keras_layer, 'bias')
     else:
         layer['bias_quantizer'] = None
 
     return layer, output_shape
 
 
 @keras_handler('QConv1D', 'QConv2D')
-def parse_qconv_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert('QConv' in keras_layer['class_name'])
-    
+def parse_qconv_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'QConv' in keras_layer['class_name']
+
     if '1D' in keras_layer['class_name']:
-        layer, output_shape = parse_conv1d_layer(keras_layer, input_names, input_shapes, data_reader, config)
+        layer, output_shape = parse_conv1d_layer(keras_layer, input_names, input_shapes, data_reader)
     elif '2D' in keras_layer['class_name']:
-        layer, output_shape = parse_conv2d_layer(keras_layer, input_names, input_shapes, data_reader, config)
+        layer, output_shape = parse_conv2d_layer(keras_layer, input_names, input_shapes, data_reader)
 
     layer['weight_quantizer'] = get_quantizer_from_config(keras_layer, 'kernel')
     if keras_layer['config']['bias_quantizer'] is not None:
         layer['bias_quantizer'] = get_quantizer_from_config(keras_layer, 'bias')
     else:
         layer['bias_quantizer'] = None
-    
+
     return layer, output_shape
 
 
 @keras_handler('QActivation')
-def parse_qactivation_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer['class_name'] == 'QActivation')
-    supported_activations = ['quantized_relu', 'quantized_tanh', 'binary_tanh', 'ternary_tanh', 'quantized_bits', 'binary', 'ternary']
-    
+def parse_qactivation_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer['class_name'] == 'QActivation'
+    supported_activations = [
+        'quantized_relu',
+        'quantized_tanh',
+        'binary_tanh',
+        'ternary_tanh',
+        'quantized_sigmoid',
+        'quantized_bits',
+        'binary',
+        'ternary',
+    ]
+
     layer = parse_default_keras_layer(keras_layer, input_names)
 
     activation_config = keras_layer['config']['activation']
     quantizer_obj = get_quantizer(activation_config)
     activation_config = {}
-    # some activations are classes 
+    # some activations are classes
     if hasattr(quantizer_obj, 'get_config'):
         activation_config['class_name'] = quantizer_obj.__class__.__name__
         if activation_config['class_name'] == 'ternary' or activation_config['class_name'] == 'binary':
             activation_config['class_name'] += '_tanh'
         activation_config['config'] = quantizer_obj.get_config()
     # some activation quantizers are just functions with no config
     else:
@@ -68,45 +84,62 @@
             activation_config['config']['integer'] = 1
         elif 'ternary' in quantizer_obj.__name__:
             activation_config['class_name'] = 'ternary_tanh'
             activation_config['config']['bits'] = 2
             activation_config['config']['integer'] = 2
         else:
             activation_config['class_name'] = 'unknown'
-    
+
     if activation_config['class_name'] not in supported_activations:
         raise Exception('Unsupported QKeras activation: {}'.format(activation_config['class_name']))
 
+    if activation_config['class_name'] == 'quantized_bits':
+        activation_config['class_name'] = 'linear'
 
     if activation_config['class_name'] == 'ternary_tanh':
         layer['class_name'] = 'TernaryTanh'
         layer['threshold'] = activation_config.get('config', {}).get('threshold', 0.33)
         if layer['threshold'] is None:
-            layer['threshold'] = 0.33 # the default ternary tanh threshold for QKeras
+            layer['threshold'] = 0.33  # the default ternary tanh threshold for QKeras
+        layer['activation'] = 'ternary_tanh'
+    elif (
+        activation_config['class_name'] == 'quantized_sigmoid'
+        and not activation_config['config'].get('use_real_sigmoid', False)
+    ) or (
+        activation_config['class_name'] == 'quantized_tanh' and not activation_config['config'].get('use_real_tanh', False)
+    ):
+        layer['class_name'] = 'HardActivation'
+        layer['slope'] = 0.5  # the default values in QKeras
+        layer['shift'] = 0.5
+        # Quartus seems to have trouble if the width is 1.
+        layer['slope_prec'] = FixedPrecisionType(width=2, integer=0, signed=False)
+        layer['shift_prec'] = FixedPrecisionType(width=2, integer=0, signed=False)
+        layer['activation'] = activation_config['class_name'].replace('quantized_', 'hard_')
     else:
         layer['class_name'] = 'Activation'
-    if activation_config['class_name'] == 'quantized_bits':
-        activation_config['class_name'] = 'linear'
-    layer['activation'] = activation_config['class_name'].replace('quantized_', '')
+        layer['activation'] = activation_config['class_name'].replace('quantized_', '')
+
+    layer['activation_quantizer'] = activation_config
     return layer, [shape for shape in input_shapes[0]]
 
+
 @keras_handler('QBatchNormalization')
-def parse_qbatchnorm_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    
-    layer, output_shape = parse_batchnorm_layer(keras_layer, input_names, input_shapes, data_reader, config)
+def parse_qbatchnorm_layer(keras_layer, input_names, input_shapes, data_reader):
+
+    layer, output_shape = parse_batchnorm_layer(keras_layer, input_names, input_shapes, data_reader)
 
     layer['mean_quantizer'] = get_quantizer_from_config(keras_layer, 'mean')
     layer['variance_quantizer'] = get_quantizer_from_config(keras_layer, 'variance')
     layer['beta_quantizer'] = get_quantizer_from_config(keras_layer, 'beta')
     layer['gamma_quantizer'] = get_quantizer_from_config(keras_layer, 'gamma')
 
     return layer, output_shape
 
 
 @keras_handler('QConv2DBatchnorm')
-def parse_qconv2dbatchnorm_layer(keras_layer, input_names, input_shapes, data_reader, config):
+def parse_qconv2dbatchnorm_layer(keras_layer, input_names, input_shapes, data_reader):
     intermediate_shape = list()
-    conv_layer, shape_qconv = parse_qconv_layer(keras_layer, input_names, input_shapes, data_reader, config)
+    conv_layer, shape_qconv = parse_qconv_layer(keras_layer, input_names, input_shapes, data_reader)
     intermediate_shape.append(shape_qconv)
     temp_shape = intermediate_shape
-    batch_layer, out_shape = parse_batchnorm_layer(keras_layer, input_names, temp_shape, data_reader, config)
+    batch_layer, out_shape = parse_batchnorm_layer(keras_layer, input_names, temp_shape, data_reader)
     return {**conv_layer, **batch_layer}, out_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/reshape.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras/reshape.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,70 +1,92 @@
 import numpy as np
 
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
+from hls4ml.converters.keras_to_hls import keras_handler, parse_default_keras_layer
 from hls4ml.converters.utils import parse_data_format
 
+
 @keras_handler('Flatten')
-def parse_flatten_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer["class_name"] == 'Flatten')
+def parse_flatten_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer["class_name"] == 'Flatten'
 
     layer = parse_default_keras_layer(keras_layer, input_names)
-    
+
     layer['class_name'] = 'Reshape'
     layer['target_shape'] = [input_shapes[0][0], np.prod(input_shapes[0][1:])]
     output_shape = layer['target_shape']
-    
+
     return layer, output_shape
 
+
 @keras_handler('Reshape')
-def parse_reshape_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer["class_name"] == 'Reshape')
+def parse_reshape_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer["class_name"] == 'Reshape'
 
     layer = parse_default_keras_layer(keras_layer, input_names)
-    
+
     layer['target_shape'] = keras_layer['config']['target_shape']
     output_shape = input_shapes[0][:1] + keras_layer['config']['target_shape']
-    
+
+    return layer, output_shape
+
+
+@keras_handler('UpSampling1D')
+def parse_upsampling1d_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'UpSampling' in keras_layer['class_name']
+
+    layer = parse_default_keras_layer(keras_layer, input_names)
+
+    layer['in_height'] = 1
+    (layer['in_width'], layer['n_chan']) = parse_data_format(input_shapes[0], layer['data_format'])
+
+    layer['algorithm'] = 'nearest'
+
+    layer['width_factor'] = keras_layer['config']['size']
+
+    layer['out_height'] = 1
+    layer['out_width'] = layer['in_width'] * layer['width_factor']
+
+    if layer['data_format'] == 'channels_first':
+        output_shape = [input_shapes[0][0], layer['n_chan'], layer['out_width']]
+    else:
+        output_shape = [input_shapes[0][0], layer['out_width'], layer['n_chan']]
+
     return layer, output_shape
 
 
 @keras_handler('UpSampling2D')
-def parse_conv2d_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert('UpSampling2D' in keras_layer['class_name'])
+def parse_upsampling2d_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert 'UpSampling2D' in keras_layer['class_name']
 
     layer = parse_default_keras_layer(keras_layer, input_names)
-    
-    (
-        layer['in_height'],
-        layer['in_width'],
-        layer['n_chan']
-    ) = parse_data_format(input_shapes[0], layer['data_format'])
+
+    (layer['in_height'], layer['in_width'], layer['n_chan']) = parse_data_format(input_shapes[0], layer['data_format'])
 
     layer['algorithm'] = keras_layer['config']['interpolation']
 
     layer['height_factor'] = keras_layer['config']['size'][0]
     layer['width_factor'] = keras_layer['config']['size'][1]
 
     layer['out_height'] = layer['in_height'] * layer['height_factor']
     layer['out_width'] = layer['in_width'] * layer['width_factor']
-    
+
     if layer['data_format'] == 'channels_first':
         output_shape = [input_shapes[0][0], layer['n_chan'], layer['out_height'], layer['out_width']]
     else:
         output_shape = [input_shapes[0][0], layer['out_height'], layer['out_width'], layer['n_chan']]
 
     return layer, output_shape
 
+
 @keras_handler('Permute')
-def parse_permute_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer['class_name'] == 'Permute')
+def parse_permute_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer['class_name'] == 'Permute'
 
     layer = parse_default_keras_layer(keras_layer, input_names)
 
     layer['class_name'] = 'Transpose'
     dims = keras_layer['config']['dims']
     layer['perm'] = [dim - 1 for dim in keras_layer['config']['dims']]
-    
+
     output_shape = [input_shapes[0][0]] + [input_shapes[0][s] for s in dims]
 
     return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras/reshaping.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras/reshaping.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,49 +1,53 @@
 import collections.abc
 
-from hls4ml.converters.keras_to_hls import parse_default_keras_layer
-from hls4ml.converters.keras_to_hls import keras_handler
+from hls4ml.converters.keras_to_hls import keras_handler, parse_default_keras_layer
 
 
 @keras_handler('ZeroPadding1D')
-def parse_zeropadding1d_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer['class_name'] == 'ZeroPadding1D')
+def parse_zeropadding1d_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer['class_name'] == 'ZeroPadding1D'
 
     layer = parse_default_keras_layer(keras_layer, input_names)
 
     padding = keras_layer['config']['padding']
     if isinstance(padding, int):
         layer['pad_left'] = padding
         layer['pad_right'] = padding
     elif isinstance(padding, collections.abc.Sequence):
         layer['pad_left'] = padding[0]
         layer['pad_right'] = padding[1]
-        
+
     if layer['data_format'] == 'channels_first':
         output_shape = [
-            input_shapes[0][0], # Batch
-            input_shapes[0][1], # Channels
-            layer['pad_left'] + input_shapes[0][2] + layer['pad_right']  # Width
+            input_shapes[0][0],  # Batch
+            input_shapes[0][1],  # Channels
+            layer['pad_left'] + input_shapes[0][2] + layer['pad_right'],  # Width
         ]
         layer['out_width'] = output_shape[2]
         layer['n_chan'] = output_shape[1]
+
+        layer['in_width'] = input_shapes[0][2]
     else:
         output_shape = [
-            input_shapes[0][0], # Batch
-            layer['pad_left'] + input_shapes[0][1] + layer['pad_right'], # Width
-            input_shapes[0][2] # Channels
+            input_shapes[0][0],  # Batch
+            layer['pad_left'] + input_shapes[0][1] + layer['pad_right'],  # Width
+            input_shapes[0][2],  # Channels
         ]
         layer['out_width'] = output_shape[1]
         layer['n_chan'] = output_shape[2]
 
+        layer['in_width'] = input_shapes[0][1]
+
     return layer, output_shape
 
+
 @keras_handler('ZeroPadding2D')
-def parse_zeropadding2d_layer(keras_layer, input_names, input_shapes, data_reader, config):
-    assert(keras_layer['class_name'] == 'ZeroPadding2D')
+def parse_zeropadding2d_layer(keras_layer, input_names, input_shapes, data_reader):
+    assert keras_layer['class_name'] == 'ZeroPadding2D'
 
     layer = parse_default_keras_layer(keras_layer, input_names)
 
     padding = keras_layer['config']['padding']
     if isinstance(padding, int):
         layer['pad_top'] = padding
         layer['pad_bottom'] = padding
@@ -62,27 +66,33 @@
             layer['pad_right'] = width_pad[1]
         else:
             layer['pad_left'] = width_pad
             layer['pad_bottom'] = width_pad
 
     if layer['data_format'] == 'channels_first':
         output_shape = [
-            input_shapes[0][0], # Batch
-            input_shapes[0][1], # Channels
-            layer['pad_top'] + input_shapes[0][2] + layer['pad_bottom'], # Height
-            layer['pad_left'] + input_shapes[0][3] + layer['pad_right']  # Width
+            input_shapes[0][0],  # Batch
+            input_shapes[0][1],  # Channels
+            layer['pad_top'] + input_shapes[0][2] + layer['pad_bottom'],  # Height
+            layer['pad_left'] + input_shapes[0][3] + layer['pad_right'],  # Width
         ]
         layer['out_height'] = output_shape[2]
         layer['out_width'] = output_shape[3]
         layer['n_chan'] = output_shape[1]
+
+        layer['in_height'] = input_shapes[0][2]
+        layer['in_width'] = input_shapes[0][3]
     else:
         output_shape = [
-            input_shapes[0][0], # Batch
-            layer['pad_top'] + input_shapes[0][1] + layer['pad_bottom'], # Height
-            layer['pad_left'] + input_shapes[0][2] + layer['pad_right'], # Width
-            input_shapes[0][3] # Channels
+            input_shapes[0][0],  # Batch
+            layer['pad_top'] + input_shapes[0][1] + layer['pad_bottom'],  # Height
+            layer['pad_left'] + input_shapes[0][2] + layer['pad_right'],  # Width
+            input_shapes[0][3],  # Channels
         ]
         layer['out_height'] = output_shape[1]
         layer['out_width'] = output_shape[2]
         layer['n_chan'] = output_shape[3]
 
+        layer['in_height'] = input_shapes[0][1]
+        layer['in_width'] = input_shapes[0][2]
+
     return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/keras_to_hls.py` & `hls4ml-0.7.0rc1/hls4ml/converters/keras_to_hls.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,39 +1,38 @@
-from __future__ import print_function
-import numpy as np
-import h5py
 import json
-import math
 
-from hls4ml.model import HLSModel
+import h5py
+
+from hls4ml.model import ModelGraph
 
 MAXMULT = 4096
 
-class KerasFileReader(object):
+
+class KerasFileReader:
     def __init__(self, config):
         self.config = config
         self.h5file = h5py.File(config['KerasH5'], mode='r')
 
     def __del__(self):
         if self.h5file:
             self.h5file.close()
 
     def _find_data(self, layer_name, var_name):
         def h5_visitor_func(name):
             if var_name in name:
                 return name
 
-        if 'model_weights' in list(self.h5file.keys()): # h5 file comes from model.save()
-            layer_path = 'model_weights/{}'.format(layer_name)
+        if 'model_weights' in list(self.h5file.keys()):  # h5 file comes from model.save()
+            layer_path = f'model_weights/{layer_name}'
         else:
             layer_path = layer_name
 
         data_path = self.h5file[layer_path].visit(h5_visitor_func)
         if data_path:
-            return self.h5file['/{}/{}'.format(layer_path, data_path)]
+            return self.h5file[f'/{layer_path}/{data_path}']
         else:
             return None
 
     def get_weights_data(self, layer_name, var_name):
         data = self._find_data(layer_name, var_name)
         if data:
             return data[()]
@@ -43,74 +42,85 @@
     def get_weights_shape(self, layer_name, var_name):
         data = self._find_data(layer_name, var_name)
         if data is not None:
             return data.shape
         else:
             return None
 
-class KerasModelReader(object):
+
+class KerasModelReader:
     def __init__(self, keras_model):
         self.model = keras_model
 
     def get_weights_data(self, layer_name, var_name):
         layer = self.model.get_layer(layer_name)
         for i, w in enumerate(layer.weights):
             if var_name in w.name:
                 try:
-                    return w.numpy() # TF 2.x
-                except:
-                    return layer.get_weights()[i] # TF 1.x
+                    return w.numpy()  # TF 2.x
+                except Exception:
+                    return layer.get_weights()[i]  # TF 1.x
 
         return None
 
     def get_weights_shape(self, layer_name, var_name):
         layer = self.model.get_layer(layer_name)
         for w in layer.weights:
             if var_name in w.name:
                 return w.shape.as_list()
 
         return None
 
-def get_qkeras_quantization(layer, keras_layer):
-    if not layer['class_name'].startswith('Q'): # Not a QKeras layer, nothing to do
-        return
-    kernel_quantizer = keras_layer['config']['kernel_quantizer']['class_name']
-    bias_quantizer = keras_layer['config']['bias_quantizer']['class_name']
-
-    if kernel_quantizer != bias_quantizer:
-        raise Exception('Mixing quantizers within QKeras layers is not supported')
-    if kernel_quantizer == 'binary':
-        layer['quantize'] = 2
-    elif kernel_quantizer == 'ternary':
-        layer['quantize'] = 3
-    else:
-        raise Exception('Unsupported quantizer {} in {} layer {}'.format(kernel_quantizer, layer['class_name'], layer['name']))
-
 
 layer_handlers = {}
 
-def register_keras_layer_handler(layer_name, handler_func):
-    if layer_name in layer_handlers:
-        raise Exception('Layer {} already registered'.format(layer_name))
+
+def register_keras_layer_handler(layer_cname, handler_func):
+    """Register a handler function for the given layer class name.
+
+    The handler function should have the following signature:
+        parse_func(keras_layer, input_names, input_shapes, data_reader, config):
+
+    Args:
+        layer_cname (str): The name of Keras layer (the 'class_name' property in the layer's config)
+        handler_func (callable): The handler function
+
+    Raises:
+        Exception: If the layer class has already been registered.
+    """
+    if layer_cname in layer_handlers:
+        raise Exception(f'Layer {layer_cname} already registered')
     else:
-        layer_handlers[layer_name] = handler_func
+        layer_handlers[layer_cname] = handler_func
+
 
 def get_supported_keras_layers():
+    """Returns the list of Keras layers that the converter can parse.
+
+    The returned list contains all Keras layers that can be parsed into the hls4ml internal representation. Support for
+    computation of these layers may vary across hls4ml backends and conversion configuration.
+
+    Returns:
+        list: The names of supported Keras layers.
+    """
     return list(layer_handlers.keys())
 
+
 def keras_handler(*args):
     def decorator(function):
         function.handles = [arg for arg in args]
         return function
+
     return decorator
 
+
 def parse_default_keras_layer(keras_layer, input_names):
     layer = {}
 
-    #Extract name for finding weights and biases
+    # Extract name for finding weights and biases
     layer['name'] = keras_layer['config']['name']
     layer['class_name'] = keras_layer['class_name']
     if input_names is not None:
         layer['inputs'] = input_names
 
     layer['data_format'] = keras_layer['config'].get('data_format', 'channels_last')
 
@@ -119,95 +129,22 @@
     if 'epsilon' in keras_layer['config']:
         layer['epsilon'] = keras_layer['config']['epsilon']
     if 'use_bias' in keras_layer['config']:
         layer['use_bias'] = keras_layer['config']['use_bias']
 
     return layer
 
-def parse_data_format(input_shape, data_format='channels_last'):
-    # Ignore batch size
-    input_shape = input_shape[1:]
-    
-    if data_format.lower() == 'channels_last':
-        if len(input_shape) == 2: # 1D, (n_in, n_filt)
-            return (input_shape[0], input_shape[1])
-        elif len(input_shape) == 3: # 2D, (in_height, in_width, n_filt)
-            return (input_shape[0], input_shape[1], input_shape[2])
-        
-    elif data_format.lower() == 'channels_first':
-        if len(input_shape) == 2: # 1D, (n_filt, n_in)
-            return (input_shape[1], input_shape[0])
-        elif len(input_shape) == 3: # 2D, (n_filt, in_height, in_width)
-            return (input_shape[1], input_shape[2], input_shape[0])
-    else:
-        raise Exception('Unknown data format: {}'.format(data_format))
-
-def compute_padding_1d(pad_type, in_size, stride, filt_size):
-    if pad_type.lower() == 'same':
-        n_out = int(math.ceil(float(in_size) / float(stride)))
-        if (in_size % stride == 0):
-            pad_along_size = max(filt_size - stride, 0)
-        else:
-            pad_along_size = max(filt_size - (in_size % stride), 0)
-        pad_left  = pad_along_size // 2
-        pad_right  = pad_along_size - pad_left
-    elif pad_type.lower() == 'valid':
-        n_out = int(math.ceil(float(in_size - filt_size + 1) / float(stride)))
-        pad_left = 0
-        pad_right = 0
-    else:
-        raise Exception('Unknown padding type: {}'.format(pad_type))
-
-    return (n_out, pad_left, pad_right)
-
-def compute_padding_2d(pad_type, in_height, in_width, stride_height, stride_width, filt_height, filt_width):
-    if pad_type.lower() == 'same':
-        #Height
-        out_height = int(math.ceil(float(in_height) / float(stride_height)))
-        if (in_height % stride_height == 0):
-            pad_along_height = max(filt_height - stride_height, 0)
-        else:
-            pad_along_height = max(filt_height - (in_height % stride_height), 0)
-        pad_top = pad_along_height // 2
-        pad_bottom = pad_along_height - pad_top
-        #Width
-        out_width = int(math.ceil(float(in_width) / float(stride_width)))
-        if (in_width % stride_width == 0):
-            pad_along_width = max(filt_width - stride_width, 0)
-        else:
-            pad_along_width = max(filt_width - (in_width % stride_width), 0)
-        pad_left = pad_along_width // 2
-        pad_right = pad_along_width - pad_left
-    elif pad_type.lower() == 'valid':
-        out_height = int(math.ceil(float(in_height - filt_height + 1) / float(stride_height)))
-        out_width = int(math.ceil(float(in_width - filt_width + 1) / float(stride_width)))
-        
-        pad_top = 0
-        pad_bottom = 0
-        pad_left = 0
-        pad_right = 0
-    else:
-        raise Exception('Unknown padding type: {}'.format(pad_type))
-
-    return (out_height, out_width, pad_top, pad_bottom, pad_left, pad_right)
-
-def keras_to_hls(config):
-
-    ######################
-    ##  Do translation
-    ######################
-
-    #This is a list of dictionaries to hold all the layer info we need to generate HLS
-    layer_list = []
 
+def get_model_arch(config):
     if 'KerasModel' in config:
         # Model instance passed in config from API
         keras_model = config['KerasModel']
         if isinstance(keras_model, str):
             from tensorflow.keras.models import load_model
+
             keras_model = load_model(keras_model)
         model_arch = json.loads(keras_model.to_json())
         reader = KerasModelReader(keras_model)
     elif 'KerasJson' in config:
         # Extract model architecture from json
         with open(config['KerasJson']) as json_file:
             model_arch = json.load(json_file)
@@ -216,54 +153,76 @@
         # Model arch and weights are in H5 file (from model.save() function)
         with h5py.File(config['KerasH5'], mode='r') as h5file:
             # Load the configuration from h5 using json's decode
             model_arch = h5file.attrs.get('model_config')
             if model_arch is None:
                 raise ValueError('No model found in config file.')
             else:
-                model_arch = json.loads(model_arch.decode('utf-8'))
+                # model_arch is string by default since h5py 3.0.0, keeping this condition for compatibility.
+                if isinstance(model_arch, bytes):
+                    model_arch = model_arch.decode('utf-8')
+                model_arch = json.loads(model_arch)
         reader = KerasFileReader(config)
     else:
         raise ValueError('No model found in config file.')
 
-    #print(model_arch)
+    return model_arch, reader
+
 
-    #Define layers to skip for conversion to HLS
+def parse_keras_model(model_arch, reader):
+
+    # This is a list of dictionaries to hold all the layer info we need to generate HLS
+    layer_list = []
+
+    # Define layers to skip for conversion to HLS
     skip_layers = ['Dropout']
-    #All supported layers
+    # Activation layers
+    activation_layers = [
+        'Activation',
+        'LeakyReLU',
+        'ThresholdedReLU',
+        'ELU',
+        'PReLU',
+        'Softmax',
+        'TernaryTanh',
+        'HardActivation',
+    ]
+    # Recurrent layers
+    recurrent_layers = ['SimpleRNN', 'LSTM', 'GRU']
+    # All supported layers
     supported_layers = get_supported_keras_layers() + skip_layers
 
-    #Map inputs of skipped and split (activation) layers
+    # Map inputs of skipped and split (activation) layers
     inputs_map = {}
 
-    #Loop through layers
+    # Loop through layers
     layer_counter = 0
 
     input_layers = None
     output_layers = None
 
     layer_config = None
     if model_arch['class_name'] == 'Sequential':
         print('Interpreting Sequential')
         layer_config = model_arch['config']
-        if 'layers' in layer_config: # Newer Keras versions have 'layers' in 'config' key
+        if 'layers' in layer_config:  # Newer Keras versions have 'layers' in 'config' key
             layer_config = layer_config['layers']
         # Sequential doesn't have InputLayer in TF < 2.3 (Keras 2.4.0)
         if layer_config[0]['class_name'] != 'InputLayer':
             input_layer = {}
             input_layer['name'] = 'input1'
             input_layer['class_name'] = 'InputLayer'
             input_layer['input_shape'] = layer_config[0]['config']['batch_input_shape'][1:]
             layer_list.append(input_layer)
             print('Input shape:', input_layer['input_shape'])
-    elif model_arch['class_name'] in ['Model', 'Functional']: # TF >= 2.3 calls it 'Funcational' API
+    elif model_arch['class_name'] in ['Model', 'Functional']:  # TF >= 2.3 calls it 'Funcational' API
         print('Interpreting Model')
         layer_config = model_arch['config']['layers']
-        input_layers = [ inp[0] for inp in model_arch['config']['input_layers'] ]
-        output_layers = [ out[0] for out in model_arch['config']['output_layers'] ]
+        input_layers = [inp[0] for inp in model_arch['config']['input_layers']]
+        output_layers = [out[0] for out in model_arch['config']['output_layers']]
 
     # Get input shape and check for unsupported layer type
     for keras_layer in layer_config:
         if keras_layer['class_name'] not in supported_layers:
             raise Exception('ERROR: Unsupported layer type: {}'.format(keras_layer['class_name']))
 
     output_shapes = {}
@@ -276,65 +235,81 @@
                 input_shapes = [output_shapes[inbound_node[0]] for inbound_node in keras_layer['inbound_nodes'][0]]
             else:
                 input_shapes = [keras_layer['config']['batch_input_shape']]
         else:
             if 'inbound_nodes' in keras_layer:
                 input_shapes = [output_shapes[inbound_node[0]] for inbound_node in keras_layer['inbound_nodes'][0]]
             else:
-                # Sequential model, so output_shape from the previous layer is still valid 
+                # Sequential model, so output_shape from the previous layer is still valid
                 input_shapes = [output_shape]
 
         keras_class = keras_layer['class_name']
 
         if keras_class in skip_layers:
             if 'inbound_nodes' in keras_layer:
                 name = keras_layer['config']['name']
-                #Currently supported skipped layers have only one input
+                # Currently supported skipped layers have only one input
                 parent_input = keras_layer['inbound_nodes'][0][0][0]
-                #Skipped layers can follow each other (e.g., Dropout -> Flatten)
+                # Skipped layers can follow each other (e.g., Dropout -> Flatten)
                 inputs_map[name] = inputs_map.get(parent_input, parent_input)
 
             output_shapes[keras_layer['config']['name']] = input_shapes[0]
 
             continue
 
         if keras_class in supported_layers:
             layer_counter = layer_counter + 1
 
-        #Extract inbound nodes
+        # Extract inbound nodes
         if 'inbound_nodes' in keras_layer and len(keras_layer['inbound_nodes']) > 0:
-            input_names = [ inputs_map.get(inp[0], inp[0]) for inp in keras_layer['inbound_nodes'][0] ]
+            input_names = [inputs_map.get(inp[0], inp[0]) for inp in keras_layer['inbound_nodes'][0]]
         else:
             input_names = None
 
-        layer, output_shape = layer_handlers[keras_class](keras_layer, input_names, input_shapes, reader, config)
+        layer, output_shape = layer_handlers[keras_class](keras_layer, input_names, input_shapes, reader)
 
-        print('Layer name: {}, layer type: {}, input shapes: {}, output shape: {}'.format(layer['name'], layer['class_name'], input_shapes, output_shape))
-        layer_list.append( layer )
-        if 'activation' in layer and layer['class_name'] not in ['Activation', 'LeakyReLU', 'ThresholdedReLU', 'ELU', 'PReLU', 'Softmax', 'TernaryTanh']:# + qkeras_layers:
+        print(
+            'Layer name: {}, layer type: {}, input shapes: {}, output shape: {}'.format(
+                layer['name'], layer['class_name'], input_shapes, output_shape
+            )
+        )
+        layer_list.append(layer)
+        if 'activation' in layer and layer['class_name'] not in activation_layers + recurrent_layers:  # + qkeras_layers:
             act_layer = {}
-            act_layer['name'] = layer['name'] + '_' + layer['activation']
-            act_layer['activation'] = layer['activation']
-            if 'activ_param' in layer:
-                act_layer['activ_param'] = layer['activ_param']
-                act_layer['class_name'] = layer['activation']
-            elif layer['activation'] == 'softmax':
-                act_layer['class_name'] = 'Softmax'
-                act_layer['axis'] = -1
+            # Workaround for QKeras activations passed as an argument
+            if isinstance(layer['activation'], dict):
+                act_details = layer['activation']
+                act_layer['class_name'] = 'QActivation'
+                act_layer['config'] = {
+                    'name': layer['name'] + '_' + act_details['class_name'],
+                    'activation': act_details['class_name'],
+                }
+                act_layer, output_shape = layer_handlers['QActivation'](act_layer, None, [output_shape], reader)
             else:
-                act_layer['class_name'] = 'Activation'
+                act_layer['name'] = layer['name'] + '_' + layer['activation']
+                act_layer['activation'] = layer['activation']
+                if 'activ_param' in layer:
+                    act_layer['activ_param'] = layer['activ_param']
+                    act_layer['class_name'] = layer['activation']
+                elif layer['activation'] == 'softmax':
+                    act_layer['class_name'] = 'Softmax'
+                    act_layer['axis'] = -1
+                else:
+                    act_layer['class_name'] = 'Activation'
             inputs_map[layer['name']] = act_layer['name']
             if output_layers is not None and layer['name'] in output_layers:
                 output_layers = [act_layer['name'] if name == layer['name'] else name for name in output_layers]
             layer_list.append(act_layer)
 
-        assert(output_shape is not None)
-        
+        assert output_shape is not None
+
         output_shapes[layer['name']] = output_shape
 
-    #################
-    ## Generate HLS
-    #################
+    return layer_list, input_layers, output_layers
 
+
+def keras_to_hls(config):
+    model_arch, reader = get_model_arch(config)
+    layer_list, input_layers, output_layers = parse_keras_model(model_arch, reader)
     print('Creating HLS model')
-    hls_model = HLSModel(config, reader, layer_list, input_layers, output_layers)
+    hls_model = ModelGraph(config, reader, layer_list, input_layers, output_layers)
     return hls_model
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/onnx/convolution.py` & `hls4ml-0.7.0rc1/hls4ml/converters/onnx/convolution.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,79 +1,87 @@
-import math
-from hls4ml.converters.onnx_to_hls import onnx_handler, get_onnx_attribute, get_onnx_input_name, compute_pads_1d, compute_pads_2d
+from hls4ml.converters.onnx_to_hls import (
+    compute_pads_1d,
+    compute_pads_2d,
+    get_onnx_attribute,
+    get_onnx_input_name,
+    onnx_handler,
+)
 from hls4ml.converters.utils import compute_padding_1d, compute_padding_2d
 
+
 @onnx_handler('Conv')
 def parse_conv_layer(reader, node, inputs_map, input_shapes, graph, config):
 
     layer = {}
     layer['name'] = node.name
-    layer['data_format'] = 'channels_first' #ONNX's default is channel first
+    layer['data_format'] = 'channels_first'  # ONNX's default is channel first
     layer['inputs'] = get_onnx_input_name(node, graph)
     reader.add_input(layer['name'], node.input)
 
     strides = get_onnx_attribute(node, 'strides')
     kernel_shape = get_onnx_attribute(node, 'kernel_shape')
 
-    if len(input_shapes[0]) == 3: # Conv1D
+    if len(input_shapes[0]) == 3:  # Conv1D
         layer['class_name'] = 'Conv1D'
 
-        layer['in_width']= input_shapes[0][2]
-        layer['n_chan']=input_shapes[0][1]
-        layer['filt_width']= kernel_shape[0]
-        layer['n_filt']= reader.get_weights_data(layer['name'], 'kernel').shape[2]
+        layer['in_width'] = input_shapes[0][2]
+        layer['n_chan'] = input_shapes[0][1]
+        layer['filt_width'] = kernel_shape[0]
+        layer['n_filt'] = reader.get_weights_data(layer['name'], 'kernel').shape[2]
         layer['stride_width'] = strides[0]
         pads = compute_pads_1d(node, layer)
 
         layer['pad_left'] = pads[0]
         layer['pad_right'] = pads[1]
-        
-        if all(x == 0 for x in pads): # No padding, i.e., 'VALID' padding
+
+        if all(x == 0 for x in pads):  # No padding, i.e., 'VALID' padding
             layer['padding'] = 'valid'
         else:
             layer['padding'] = 'same'
-        
-        (layer['out_width'],_,_) = compute_padding_1d(layer['padding'],
-                                                      layer['in_width'],
-                                                      layer['stride_width'],
-                                                      layer['filt_width'])
+
+        (layer['out_width'], _, _) = compute_padding_1d(
+            layer['padding'], layer['in_width'], layer['stride_width'], layer['filt_width']
+        )
 
         output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_width']]
-        
-    elif len(input_shapes[0]) == 4: # Conv2D
-        
+
+    elif len(input_shapes[0]) == 4:  # Conv2D
+
         layer['class_name'] = 'Conv2D'
 
-        layer['in_height']=input_shapes[0][2]
-        layer['in_width']=input_shapes[0][3]
-        layer['n_chan']=input_shapes[0][1]
-
-        layer['filt_height']=kernel_shape[0]
-        layer['filt_width']=kernel_shape[1]
-        
-        layer['n_filt']=next((x.type.tensor_type.shape.dim[1].dim_value for x in graph.value_info if x.name == node.output[0]), None)
+        layer['in_height'] = input_shapes[0][2]
+        layer['in_width'] = input_shapes[0][3]
+        layer['n_chan'] = input_shapes[0][1]
+
+        layer['filt_height'] = kernel_shape[0]
+        layer['filt_width'] = kernel_shape[1]
+
+        layer['n_filt'] = next(
+            (x.type.tensor_type.shape.dim[1].dim_value for x in graph.value_info if x.name == node.output[0]), None
+        )
         layer['stride_height'] = strides[0]
         layer['stride_width'] = strides[1]
         pads = compute_pads_2d(node, layer)
 
         layer['pad_top'] = pads[0]
         layer['pad_bottom'] = pads[2]
         layer['pad_left'] = pads[1]
         layer['pad_right'] = pads[3]
 
-        if all(x == 0 for x in pads): # No padding, i.e., 'VALID' padding in Keras/Tensorflow
+        if all(x == 0 for x in pads):  # No padding, i.e., 'VALID' padding in Keras/Tensorflow
             layer['padding'] = 'valid'
-        else: #Only 'valid' and 'same' padding are available in Keras
+        else:  # Only 'valid' and 'same' padding are available in Keras
             layer['padding'] = 'same'
-            
-        (layer['out_height'], layer['out_width'],_,_,_,_) = compute_padding_2d(layer['padding'],
-                                                                               layer['in_height'],
-                                                                               layer['in_width'],
-                                                                               layer['stride_height'],
-                                                                               layer['stride_width'],
-                                                                               layer['filt_height'],
-                                                                               layer['filt_width'])
+
+        (layer['out_height'], layer['out_width'], _, _, _, _) = compute_padding_2d(
+            layer['padding'],
+            layer['in_height'],
+            layer['in_width'],
+            layer['stride_height'],
+            layer['stride_width'],
+            layer['filt_height'],
+            layer['filt_width'],
+        )
 
         output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_height'], layer['out_width']]
 
     return layer, output_shape
-
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/onnx/core.py` & `hls4ml-0.7.0rc1/hls4ml/converters/onnx/core.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,102 +1,127 @@
-from hls4ml.converters.onnx_to_hls import onnx_handler, get_onnx_attribute, get_onnx_input_name
-from hls4ml.converters.utils import compute_padding_1d
+from hls4ml.converters.onnx_to_hls import get_onnx_attribute, get_onnx_input_name, onnx_handler
+
 
 @onnx_handler(*['Gemm', 'MatMul'])
 def parse_gemm_layer(reader, node, inputs_map, input_shapes, graph, config):
-    
+
     layer = {}
-   
+
     layer['class_name'] = 'Dense'
     layer['name'] = node.name
     layer['inputs'] = get_onnx_input_name(node, graph)
 
     tran_weight = get_onnx_attribute(node, 'transB', 0)
     reader.add_input(layer['name'], node.input, tran_weight)
 
     weights_shape = reader.get_weights_data(layer['name'], 'kernel').shape
     layer['n_in'] = weights_shape[0]
-    layer['n_out'] = weights_shape[1]      
+    layer['n_out'] = weights_shape[1]
 
     output_shape = input_shapes[0][:]
     output_shape[-1] = layer['n_out']
-    
+
     return layer, output_shape
 
-#------------------Global paras for activations
+
+# ------------------Global paras for activations
 # TODO: repair HardSigmoid support
-# https://github.com/fastmachinelearning/hls4ml/issues/409 
-#activation_layers = ['Relu', 'Tanh', 'Sigmoid', 'LeakyRelu', 'ThresholdedRelu', 'HardSigmoid', 'Elu', 'Selu', 'PRelu', 'Softmax', 'Softsign', 'Softplus', 'Clip']
-activation_layers = ['Relu', 'Tanh', 'Sigmoid', 'LeakyRelu', 'ThresholdedRelu', 'Elu', 'Selu', 'PRelu', 'Softmax', 'Softsign', 'Softplus', 'Clip']
-
-activation_map = {'Relu':'ReLU', 'Tanh':'Activation',
-                'Sigmoid':'Activation', 'LeakyRelu':'LeakyReLU',
-                'ThresholdedRelu':'ThresholdedReLU', 'HardSigmoid':'Activation',
-                'Elu':'ELU', 'Selu':'Activation', 'PRelu':'PReLU', 'Softmax':'Softmax',
-                'Softsign':'Activation', 'Softplus':'Activation', 'Clip':'Clip'}
-#---------
+# https://github.com/fastmachinelearning/hls4ml/issues/409
+activation_layers = [
+    'Relu',
+    'Tanh',
+    'Sigmoid',
+    'LeakyRelu',
+    'ThresholdedRelu',
+    'Elu',
+    'Selu',
+    'PRelu',
+    'Softmax',
+    'Softsign',
+    'Softplus',
+    'Clip',
+]
+
+activation_map = {
+    'Relu': 'ReLU',
+    'Tanh': 'Activation',
+    'Sigmoid': 'Activation',
+    'LeakyRelu': 'LeakyReLU',
+    'ThresholdedRelu': 'ThresholdedReLU',
+    'HardSigmoid': 'Activation',
+    'Elu': 'ELU',
+    'Selu': 'Activation',
+    'PRelu': 'PReLU',
+    'Softmax': 'Softmax',
+    'Softsign': 'Activation',
+    'Softplus': 'Activation',
+    'Clip': 'Clip',
+}
+# ---------
+
 
 @onnx_handler(*activation_layers)
 def parse_activation_layer(reader, node, inputs_map, input_shapes, graph, config):
-    
+
     layer = {}
-    
+
     layer['name'] = node.name
     layer['class_name'] = activation_map[node.op_type]
     layer['activation'] = node.op_type.lower()
     layer['inputs'] = get_onnx_input_name(node, graph)
-    
+
     if layer['class_name'] != 'Activation':
-        
+
         if layer['class_name'] == 'Softmax':
             layer['activation'] = 'softmax'
 
         elif layer['class_name'] in ['ELU', 'LeakyReLU', 'ThresholdedReLU']:
             layer['activation'] = layer['class_name']
             layer['activ_param'] = get_onnx_attribute(node, 'alpha', 0.01)
-        
+
         elif layer['class_name'] == 'Clip':
-            
+
             clip_min_node = [x for x in graph.initializer if x.name in node.input]
-            clip_min =  clip_min_node[0].float_data[0]
+            clip_min = clip_min_node[0].float_data[0]
 
-            #Check if it's relu or not
-            if clip_min == 0.:
+            # Check if it's relu or not
+            if clip_min == 0.0:
                 layer['class_name'] = 'Activation'
                 layer['activation'] = 'ReLU'
             else:
                 raise Exception('Clip with min != 0 is not supported yet!')
-        
+
         else:
             layer['activation'] = layer['class_name']
             layer['class_name'] = 'Activation'
-       
+
     return layer, [shape for shape in input_shapes[0]]
-    
+
+
 @onnx_handler('BatchNormalization')
 def parse_batchnorm_layer(reader, node, inputs_map, input_shapes, graph, config):
-    
+
     layer = {}
-   
+
     layer['class_name'] = 'BatchNormalization'
     layer['data_format'] = 'channels_first'
     layer['name'] = node.name
     layer['inputs'] = get_onnx_input_name(node, graph)
-    
-    #Other attributes
+
+    # Other attributes
     layer['epsilon'] = get_onnx_attribute(node, 'epsilon')
     layer['momentum'] = get_onnx_attribute(node, 'momentum')
-            
+
     reader.add_input(layer['name'], node.input)
-    
+
     in_size = 1
     for dim in input_shapes[0][1:]:
         in_size *= dim
-        
+
     layer['n_in'] = layer['n_out'] = in_size
-    
+
     if len(input_shapes[0]) == 2:
         layer['n_filt'] = -1
     elif len(input_shapes[0]) > 2:
-        layer['n_filt']= input_shapes[0][1] #Always channel first for onnx
-    
-    return layer, [shape for shape in input_shapes[0]]
+        layer['n_filt'] = input_shapes[0][1]  # Always channel first for onnx
+
+    return layer, [shape for shape in input_shapes[0]]
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/onnx/pooling.py` & `hls4ml-0.7.0rc1/hls4ml/converters/onnx/pooling.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,112 +1,123 @@
-import math
-from hls4ml.converters.onnx_to_hls import onnx_handler, get_onnx_attribute, compute_pads_1d, compute_pads_2d, get_onnx_input_name
+from hls4ml.converters.onnx_to_hls import (
+    compute_pads_1d,
+    compute_pads_2d,
+    get_onnx_attribute,
+    get_onnx_input_name,
+    onnx_handler,
+)
 from hls4ml.converters.utils import compute_padding_1d, compute_padding_2d
 
 pool_operations = ['AveragePool', 'MaxPool']
+
+
 @onnx_handler(*pool_operations)
 def parse_pool_layer(reader, node, inputs_map, input_shapes, graph, config):
-    
+
     layer = {}
     layer['name'] = node.name
     layer['inputs'] = get_onnx_input_name(node, graph)
     layer['class_name'] = node.op_type
-    layer['data_format'] = 'channels_first' #Default ONNX
+    layer['data_format'] = 'channels_first'  # Default ONNX
 
     info = layer['class_name'].replace('Pool', '')
     strides = get_onnx_attribute(node, 'strides')
     kernel_shape = get_onnx_attribute(node, 'kernel_shape')
-    
-    if len(input_shapes[0]) == 3: # 1D
+
+    if len(input_shapes[0]) == 3:  # 1D
         layer['class_name'] = info + 'Pooling1D'
 
         layer['n_filt'] = input_shapes[0][1]
         layer['n_in'] = input_shapes[0][2]
 
         layer['pool_width'] = kernel_shape[0]
         layer['stride_width'] = strides[0]
-        
-        #Padding
+
+        # Padding
         pads = compute_pads_1d(node, layer)
         layer['pad_left'] = pads[0]
         layer['pad_right'] = pads[1]
 
-        if all(x == 0 for x in pads): # No padding, i.e., 'VALID' padding
+        if all(x == 0 for x in pads):  # No padding, i.e., 'VALID' padding
             layer['padding'] = 'valid'
         else:
             layer['padding'] = 'same'
-            
-        (layer['n_out'],_,_) = compute_padding_1d(layer['padding'],
-                                                      layer['n_in'],
-                                                      layer['stride_width'],
-                                                      layer['pool_width'])
+
+        (layer['n_out'], _, _) = compute_padding_1d(
+            layer['padding'], layer['n_in'], layer['stride_width'], layer['pool_width']
+        )
 
         output_shape = [input_shapes[0][0], layer['n_filt'], layer['n_out']]
-    
-    elif len(input_shapes[0]) == 4: # 2D
+
+    elif len(input_shapes[0]) == 4:  # 2D
         layer['class_name'] = info + 'Pooling2D'
 
         layer['n_filt'] = input_shapes[0][1]
         layer['in_height'] = input_shapes[0][2]
         layer['in_width'] = input_shapes[0][3]
 
         layer['stride_height'] = strides[0]
         layer['stride_width'] = strides[1]
         layer['pool_height'] = layer['filt_height'] = kernel_shape[0]
         layer['pool_width'] = layer['filt_width'] = kernel_shape[1]
-        
+
         pads = compute_pads_2d(node, layer)
         layer['pad_top'] = pads[0]
         layer['pad_bottom'] = pads[2]
         layer['pad_left'] = pads[1]
         layer['pad_right'] = pads[3]
 
-        if all(x == 0 for x in pads): # No padding, i.e., 'VALID' padding in Keras/Tensorflow
+        if all(x == 0 for x in pads):  # No padding, i.e., 'VALID' padding in Keras/Tensorflow
             layer['padding'] = 'valid'
-        else: #Only 'valid' and 'same' padding are available in Keras
+        else:  # Only 'valid' and 'same' padding are available in Keras
             layer['padding'] = 'same'
-            
-        (layer['out_height'], layer['out_width'],_,_,_,_) = compute_padding_2d(layer['padding'],
-                                                                               layer['in_height'],
-                                                                               layer['in_width'],
-                                                                               layer['stride_height'],
-                                                                               layer['stride_width'],
-                                                                               layer['filt_height'],
-                                                                               layer['filt_width'])
-        
+
+        (layer['out_height'], layer['out_width'], _, _, _, _) = compute_padding_2d(
+            layer['padding'],
+            layer['in_height'],
+            layer['in_width'],
+            layer['stride_height'],
+            layer['stride_width'],
+            layer['filt_height'],
+            layer['filt_width'],
+        )
+
         output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_height'], layer['out_width']]
-    
+
     return layer, output_shape
 
+
 global_pooling_layers = ['GlobalMaxPool', 'GlobalAveragePool']
+
+
 @onnx_handler(*global_pooling_layers)
 def parse_global_pooling_layer(reader, node, inputs_map, input_shapes, graph, config):
 
     layer = {}
     layer['name'] = node.name
     layer['inputs'] = get_onnx_input_name(node, graph)
     layer['class_name'] = node.op_type
     layer['data_format'] = 'channels_first'
 
-    #Sonme default parameters for global pooling
+    # Sonme default parameters for global pooling
     layer['n_out'] = 1
     layer['pad_left'] = layer['pad_right'] = 0
     layer['stride'] = 0
 
     info = layer['class_name'].replace('Pool', '')
 
-    if len(input_shapes[0]) == 3: # 1D
+    if len(input_shapes[0]) == 3:  # 1D
         layer['class_name'] = info + 'Pooling1D'
 
         layer['n_in'] = input_shapes[0][2]
         layer['n_filt'] = input_shapes[0][1]
-    
+
     elif len(input_shapes[0]) == 4:
         layer['class_name'] = info + 'Pooling2D'
 
         layer['n_filt'] = input_shapes[0][1]
         layer['in_height'] = input_shapes[0][2]
         layer['in_width'] = input_shapes[0][3]
-    
-    output_shape = [input_shapes[0][0], layer['n_filt']] + [1]*(len(input_shapes[0]) - 2)
 
-    return layer, output_shape
+    output_shape = [input_shapes[0][0], layer['n_filt']] + [1] * (len(input_shapes[0]) - 2)
+
+    return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/onnx/reshape.py` & `hls4ml-0.7.0rc1/hls4ml/converters/onnx/reshape.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,38 +1,41 @@
-from hls4ml.converters.onnx_to_hls import onnx_handler, get_onnx_input_name, get_onnx_attribute
 import numpy as np
 
+from hls4ml.converters.onnx_to_hls import get_onnx_input_name, onnx_handler
+
+
 @onnx_handler('Transpose')
 def parse_transpose_layer(reader, node, inputs_map, input_shapes, graph, config):
-    
+
     layer = {}
     layer['name'] = node.name
     layer['class_name'] = 'Transpose'
     layer['inputs'] = get_onnx_input_name(node, graph)
-    
-    perm = [list(i.ints) for i in node.attribute][0] #This will get something like [[a,b,c]][0] = [a,b,c]    
-    layer['perm'] = [x - 1 for x in perm[1:]] #Ignore the batch dimension in ONNX, and adjust the perm indexing
-    
+
+    perm = [list(i.ints) for i in node.attribute][0]  # This will get something like [[a,b,c]][0] = [a,b,c]
+    layer['perm'] = [x - 1 for x in perm[1:]]  # Ignore the batch dimension in ONNX, and adjust the perm indexing
+
     output_shape = [input_shapes[0][i] for i in perm]
-    
+
     return layer, output_shape
 
+
 @onnx_handler('Reshape')
 def parse_reshape_layer(reader, node, inputs_map, input_shapes, graph, config):
 
     layer = {}
     layer['name'] = node.name
     layer['class_name'] = 'Reshape'
     layer['inputs'] = get_onnx_input_name(node, graph)
 
     target_shape = list([x for x in graph.initializer if x.name == node.input[1]][0].int64_data)[1:]
 
-    if -1 in target_shape: #Need to infer shape for -1
+    if -1 in target_shape:  # Need to infer shape for -1
         print("WARNING: Inferring -1 shape ... ")
         dummy_x = np.ones(input_shapes[0][1:])
         dummy_y = np.reshape(dummy_x, target_shape)
         target_shape = list(dummy_y.shape)
-    
+
     layer['target_shape'] = target_shape
     output_shape = input_shapes[0][:1] + layer['target_shape']
-    
-    return layer, output_shape
+
+    return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/onnx_to_hls.py` & `hls4ml-0.7.0rc1/hls4ml/converters/onnx_to_hls.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,319 +1,329 @@
-from __future__ import print_function
-from sys import path_importer_cache
 import numpy as np
-import math
-from onnx import ModelProto, GraphProto, NodeProto, TensorProto
-from onnx import  helper, numpy_helper, shape_inference
+import onnx
+from onnx import helper, numpy_helper, shape_inference
 
-from hls4ml.model import HLSModel
+from hls4ml.model import ModelGraph
 
 MAXMULT = 4096
 
+
 class ONNXDataReader:
     """
     ONNX data reader to be used for extracting relevant information during conversion.
     """
+
     def __init__(self, model):
         self.model = model
         self.input_map = {}
         self.index_map = {
             # Dense
-            'kernel' : 1,
-            'bias'   : 2,
+            'kernel': 1,
+            'bias': 2,
             # BatchNormalization
-            'gamma'  : 1,
-            'beta'   : 2,
-            'moving_mean'   : 3,
-            'moving_variance' : 4,
+            'gamma': 1,
+            'beta': 2,
+            'moving_mean': 3,
+            'moving_variance': 4,
         }
 
     def get_weights_data(self, layer_name, var_name):
         """Extract weights data from ONNX model.
-        
+
         Parameters
         ----------
         layer_name : string
             layer's name in the ONNX model
         var_name : string
             variable to be extracted
 
         Returns
         -------
         data : numpy array
-            extracted weights data 
-        
+            extracted weights data
+
         """
-        #Get the node associated with the layer name
-        node = next((node for node in self.model.graph.node if node.name == layer_name))
-        
+        # Get the node associated with the layer name
+        node = next(node for node in self.model.graph.node if node.name == layer_name)
+
         inputs = self.input_map[layer_name]
         inp_idx = self.index_map[var_name]
-        
+
         if inp_idx >= len(inputs['inputs']):
             # Check if the layer is an AddBias layer
             if (node.op_type == 'Add') and (var_name == 'bias'):
                 inp_idx = 1
             else:
                 # Input not found, likely a bias tensor is not available
                 return None
 
         tensor = next((x for x in self.model.graph.initializer if x.name == inputs['inputs'][inp_idx]), None)
-        
+
         if tensor is not None:
 
             data = numpy_helper.to_array(tensor)
 
             if inputs['transpose']:
                 if inputs['perm'] is not None and len(data.shape) == len(inputs['perm']):
                     data = data.transpose(inputs['perm'])
                 else:
                     data = data.transpose()
-            
-            #Check for transB in Gemm
+
+            # Check for transB in Gemm
             if node.op_type == 'Gemm':
                 if not get_onnx_attribute(node, 'transB'):
                     data = data.transpose()
 
         return data
-    
+
     def add_input(self, layer_name, inputs, transpose=True, perm=None):
-        self.input_map[layer_name] = { 'inputs': inputs, 'transpose': transpose, 'perm': perm }
-    
-####----------------------Helpers---------------------######
+        self.input_map[layer_name] = {'inputs': inputs, 'transpose': transpose, 'perm': perm}
+
+
+# ----------------------Helpers--------------------- #
 def sanitize_layer_name(layer):
     new_name = layer['name']
     if new_name[0].isdigit():
         new_name = layer['class_name'].lower() + new_name
-    
+
     layer['name'] = new_name
 
+
 def replace_char_inconsitency(name):
     """
     Replace some inconsistent characters that cause issues when writing into HLS.
     """
-    return name.replace('.','_')
+    return name.replace('.', '_')
+
 
 def get_onnx_attribute(operation, name, default=None):
     attr = next((x for x in operation.attribute if x.name == name), None)
     if attr is None:
         value = default
     else:
         value = helper.get_attribute_value(attr)
         if isinstance(value, bytes):
             value = value.decode()
     return value
 
+
 def get_input_shape(model, operation, input_idx=0):
     value_info_idx = next((i for i, x in enumerate(model.graph.value_info) if x.name == operation.input[input_idx]), 0)
     return [d.dim_value for d in model.graph.value_info[value_info_idx].type.tensor_type.shape.dim]
 
+
 def compute_pads_1d(operation, layer):
     auto_pad = get_onnx_attribute(operation, 'auto_pad', 'NOTSET')
     if auto_pad != 'NOTSET':
-        if (layer['in_width'] % layer['stride_width'] == 0):
+        if layer['in_width'] % layer['stride_width'] == 0:
             pad_along_width = max(layer['filt_width'] - layer['stride_width'], 0)
         else:
             pad_along_width = max(layer['filt_width'] - (layer['in_width'] % layer['stride_width']), 0)
 
         pads = [pad_along_width // 2, pad_along_width - (pad_along_width // 2)]
 
         if auto_pad == 'SAME_UPPER':
             pads = sorted(pads)
         elif auto_pad == 'SAME_LOWER':
             pads = sorted(pads, reverse=True)
-        else: # 'VALID' padding
+        else:  # 'VALID' padding
             pads = [0, 0]
     else:
         pads = get_onnx_attribute(operation, 'pads', [0, 0])
-    
+
     return pads
 
+
 def compute_pads_2d(operation, layer):
     auto_pad = get_onnx_attribute(operation, 'auto_pad', 'NOTSET')
     if auto_pad != 'NOTSET':
-        #Height
-        if (layer['in_height'] % layer['stride_height'] == 0):
+        # Height
+        if layer['in_height'] % layer['stride_height'] == 0:
             pad_along_height = max(layer['filt_height'] - layer['stride_height'], 0)
         else:
             pad_along_height = max(layer['filt_height'] - (layer['in_height'] % layer['stride_height']), 0)
         pad_height = [pad_along_height // 2, pad_along_height - pad_along_height // 2]
 
-        #Width
-        if (layer['in_width'] % layer['stride_width'] == 0):
+        # Width
+        if layer['in_width'] % layer['stride_width'] == 0:
             pad_along_width = max(layer['filt_width'] - layer['stride_width'], 0)
         else:
             pad_along_width = max(layer['filt_width'] - (layer['in_width'] % layer['stride_width']), 0)
         pad_width = [pad_along_width // 2, pad_along_width - pad_along_width // 2]
 
         if auto_pad == 'SAME_UPPER':
             pads = [min(pad_height), min(pad_width), max(pad_height), max(pad_width)]
         elif auto_pad == 'SAME_LOWER':
             pads = [max(pad_height), max(pad_width), min(pad_height), min(pad_width)]
-        else: # 'VALID' padding
+        else:  # 'VALID' padding
             pads = [0, 0, 0, 0]
     else:
         pads = get_onnx_attribute(operation, 'pads', [0, 0, 0, 0])
-    
+
     return pads
 
-####----------------------Layer handling---------------------######
+
+# ----------------------Layer handling--------------------- #
 layer_handlers = {}
 
+
 def register_onnx_layer_handler(layer_name, handler_func):
     if layer_name in layer_handlers:
-        raise Exception('Layer {} already registered'.format(layer_name))
+        raise Exception(f'Layer {layer_name} already registered')
     else:
         layer_handlers[layer_name] = handler_func
 
+
 def get_supported_onnx_layers():
     return list(layer_handlers.keys())
 
+
 def onnx_handler(*args):
     def decorator(function):
         function.handles = [arg for arg in args]
         return function
+
     return decorator
 
-#--->> A set of functions to address the naming convetion in ONNx's graph
+
+# --->> A set of functions to address the naming convetion in ONNx's graph
 def get_onnx_input_name(node, graph):
     """
     In ONNX, when calling node.input, it returns the node input's index in the graph instead of the input's name.
-    However, the input's name is used for indexing in HLSModel's graph. This function return the input node's name instead.
+    However, the input's name is used for indexing in ModelGraph's graph. This function return the input node's name instead.
     """
-    
+
     in_node = [in_node for in_node in graph.node if (in_node.output[0] in node.input)]
 
     if in_node:
         if in_node[0].op_type != 'Flatten':
             input_node_name = [x.name for x in in_node]
-        else: #IF it's a flatten
+        else:  # IF it's a flatten
             input_node_name = [x.name for x in graph.node if (x.output[0] in in_node[0].input)]
 
         return input_node_name
-        
-    else: #If there is no input name it's actually the first layer
+
+    else:  # If there is no input name it's actually the first layer
         return [replace_char_inconsitency(node.input[0])]
 
+
 def get_out_layer_name(graph):
     """
     Get the output layer's name for the model.
     graph.output only returns the output's node index
     """
     output_index_list = [x.name for x in graph.output]
     return [node.name for node in graph.node if node.output[0] in output_index_list]
 
 
 def onnx_to_hls(config):
-    """ Convert onnx model to hls model from configuration.
-    
+    """Convert onnx model to hls model from configuration.
+
     Parameters
     ----------
     config: dict
         onnx configuration from yaml file or passed through API.
-        
+
     Returns
     -------
-    hls_model : hls4ml model object
-        
+    ModelGraph : hls4ml model object
+
     """
 
-    #This is a list of dictionaries to hold all the layer info we need to generate HLS
+    # This is a list of dictionaries to hold all the layer info we need to generate HLS
     layer_list = []
 
-    #Extract model architecture
+    # Extract model architecture
     print('Interpreting Model ...')
-    
-    model =  onnx.load(config['OnnxModel']) if isinstance(config['OnnxModel'], str) else config['OnnxModel']
-          
+
+    model = onnx.load(config['OnnxModel']) if isinstance(config['OnnxModel'], str) else config['OnnxModel']
+
     model = shape_inference.infer_shapes(model)
     graph = model.graph
-    
+
     reader = ONNXDataReader(model)
-    
-    #Obtain list of input/ouput layers
+
+    # Obtain list of input/ouput layers
     all_inputs = [x.name for x in model.graph.input]
     all_initializers = [x.name for x in model.graph.initializer]
     input_layers = [x for x in all_inputs if x not in all_initializers]
     output_layers = get_out_layer_name(graph)
-    
+
     print("Output layers: ", output_layers)
-    
+
     for i, inp in enumerate(input_layers):
         input_layer = {}
         input_layer['name'] = replace_char_inconsitency(inp)
         input_layer['class_name'] = 'InputLayer'
         inp_shape = next((x.type.tensor_type.shape.dim for x in model.graph.input if x.name == inp), None)
         input_layer['input_shape'] = [x.dim_value for x in inp_shape]
-        
+
         if len(input_layer['input_shape']) > 1:
-            input_layer['input_shape'][0] = None #Firt dim is batch
+            input_layer['input_shape'][0] = None  # Firt dim is batch
 
-        #Clean the layer name for specific models
+        # Clean the layer name for specific models
         sanitize_layer_name(input_layer)
         input_layers[i] = input_layer['name']
 
         layer_list.append(input_layer)
 
     # Defined supported layers and check for unsupported layer type
     skip_layers = ['Dropout', 'Identity', 'Flatten']
-    
-    #Map inputs of skipped layers
+
+    # Map inputs of skipped layers
     inputs_map = {}
-    
+
     supported_layers = get_supported_onnx_layers() + skip_layers
-    
+
     # Get input shape
     current_shape = [input_layer['input_shape']]
     print('Input shape:', current_shape[0])
-    
-    #Loop through layers
+
+    # Loop through layers
     layer_counter = 0
-    
-    #Output shape tracking
-    output_shapes = {}
+
+    # Output shape tracking
     output_shape = None
 
     print('Topology:')
     for node in graph.node:
-        
+
         if node.op_type not in supported_layers:
-            raise Exception('ERROR: Unsupported operation type: {}'.format(node.op_type))
-        
-        #If not the first layer then input shape is taken from last layer's output
+            raise Exception(f'ERROR: Unsupported operation type: {node.op_type}')
+
+        # If not the first layer then input shape is taken from last layer's output
         if layer_counter != 0:
             current_shape = [output_shape]
-            
+
         if node.op_type in skip_layers:
             if node.op_type == 'Flatten':
                 output_shape = [current_shape[0][0], np.prod(current_shape[0][1:])]
-            
+
             else:
-                #Currently supported skipped layers have only one input and output
-                #Skipped layers can follow each other (e.g., Dropout -> Flatten)
-                
-                #Mapping inputs
+                # Currently supported skipped layers have only one input and output
+                # Skipped layers can follow each other (e.g., Dropout -> Flatten)
+
+                # Mapping inputs
                 input_name = inputs_map.get(node.input[0], node.input[0])
                 output_name = node.output[0]
                 inputs_map[output_name] = input_name
-                
+
                 output_shape = current_shape[0]
-            continue 
-        
+            continue
+
         if node.op_type in supported_layers:
             layer_counter = layer_counter + 1
-        
-        #Process the layer
+
+        # Process the layer
         layer, output_shape = layer_handlers[node.op_type](reader, node, inputs_map, current_shape, graph, config)
-        
+
         sanitize_layer_name(layer)
         print('Layer name: {}, layer type: {}, current shape: {}'.format(layer['name'], layer['class_name'], current_shape))
         layer_list.append(layer)
 
-
     #################
-    ## Generate HLS
+    # Generate HLS
     #################
 
     print('Creating HLS model')
-    hls_model = HLSModel(config, reader, layer_list, input_layers, output_layers)
+    hls_model = ModelGraph(config, reader, layer_list, input_layers, output_layers)
     return hls_model
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/pytorch/convolution.py` & `hls4ml-0.7.0rc1/hls4ml/converters/pytorch/convolution.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,86 +1,85 @@
-import math
 from hls4ml.converters.pytorch_to_hls import pytorch_handler
-from hls4ml.converters.utils import *
+from hls4ml.converters.utils import compute_padding_1d, compute_padding_2d, parse_data_format
+
 
 @pytorch_handler('Conv1d')
 def parse_conv1d_layer(pytorch_layer, layer_name, input_shapes, data_reader, config):
-    assert('Conv1d' in pytorch_layer.__class__.__name__)
-    
+    assert 'Conv1d' in pytorch_layer.__class__.__name__
+
     layer = {}
-    
+
     layer['name'] = layer_name
     layer['class_name'] = 'Conv1D'
-    layer['data_format'] = 'channels_first' #Pytorch default (can't change)
-    
-    #Input info
-    (
-        layer['in_width'],
-        layer['n_chan']
-    ) = parse_data_format(input_shapes[0], 'channels_first') #Keras's default is channels_last
-    
-    #Additional parameters
+    layer['data_format'] = 'channels_first'  # Pytorch default (can't change)
+
+    # Input info
+    (layer['in_width'], layer['n_chan']) = parse_data_format(
+        input_shapes[0], 'channels_first'
+    )  # Keras's default is channels_last
+
+    # Additional parameters
     layer['n_filt'] = pytorch_layer.out_channels
-    layer['filt_width'] = pytorch_layer.kernel_size[0] 
+    layer['filt_width'] = pytorch_layer.kernel_size[0]
     layer['stride_width'] = pytorch_layer.stride[0]
     layer['pad_left'] = layer['pad_right'] = pytorch_layer.padding[0]
     layer['dilation'] = pytorch_layer.dilation[0]
-    
-    if pytorch_layer.padding[0] == 0: # No padding, i.e., 'VALID' padding in Keras/Tensorflow
+
+    if pytorch_layer.padding[0] == 0:  # No padding, i.e., 'VALID' padding in Keras/Tensorflow
         layer['padding'] = 'valid'
-    else: #Only 'valid' and 'same' padding are available in Keras
+    else:  # Only 'valid' and 'same' padding are available in Keras
         layer['padding'] = 'same'
-    
-    #Ouput info
-    (layer['out_width'],_,_) = compute_padding_1d(layer['padding'],
-                                                  layer['in_width'],
-                                                  layer['stride_width'],
-                                                  layer['filt_width'])
-    
-    output_shape=[input_shapes[0][0], layer['n_filt'], layer['out_width']] #Channel first as default
-    
+
+    # Ouput info
+    (layer['out_width'], _, _) = compute_padding_1d(
+        layer['padding'], layer['in_width'], layer['stride_width'], layer['filt_width']
+    )
+
+    output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_width']]  # Channel first as default
+
     return layer, output_shape
 
+
 @pytorch_handler('Conv2d')
 def parse_conv2d_layer(pytorch_layer, layer_name, input_shapes, data_reader, config):
-    assert('Conv2d' in pytorch_layer.__class__.__name__)
-    
+    assert 'Conv2d' in pytorch_layer.__class__.__name__
+
     layer = {}
-    
+
     layer['name'] = layer_name
     layer['class_name'] = 'Conv2D'
-    layer['data_format'] = 'channels_first' #Pytorch default (can't change)
-    
-    #Input info
-    (
-        layer['in_height'],
-        layer['in_width'],
-        layer['n_chan']
-    ) = parse_data_format(input_shapes[0], 'channels_first') #Keras's default is channels_last
-    
-    #Additional parameters
+    layer['data_format'] = 'channels_first'  # Pytorch default (can't change)
+
+    # Input info
+    (layer['in_height'], layer['in_width'], layer['n_chan']) = parse_data_format(
+        input_shapes[0], 'channels_first'
+    )  # Keras's default is channels_last
+
+    # Additional parameters
     layer['n_filt'] = pytorch_layer.out_channels
     layer['filt_height'] = pytorch_layer.kernel_size[0]
     layer['filt_width'] = pytorch_layer.kernel_size[1]
     layer['stride_height'] = pytorch_layer.stride[0]
     layer['stride_width'] = pytorch_layer.stride[1]
     layer['dilation'] = pytorch_layer.dilation[0]
     layer['pad_top'] = layer['pad_bottom'] = pytorch_layer.padding[0]
     layer['pad_left'] = layer['pad_right'] = pytorch_layer.padding[1]
-    
-    if all(x == 0 for x in pytorch_layer.padding): # No padding, i.e., 'VALID' padding in Keras/Tensorflow
+
+    if all(x == 0 for x in pytorch_layer.padding):  # No padding, i.e., 'VALID' padding in Keras/Tensorflow
         layer['padding'] = 'valid'
-    else: #Only 'valid' and 'same' padding are available in Keras
+    else:  # Only 'valid' and 'same' padding are available in Keras
         layer['padding'] = 'same'
-    
-    #Ouput info
-    (layer['out_height'], layer['out_width'],_,_,_,_) = compute_padding_2d(layer['padding'],
-                                                                           layer['in_height'],
-                                                                           layer['in_width'],
-                                                                           layer['stride_height'],
-                                                                           layer['stride_width'],
-                                                                           layer['filt_height'],
-                                                                           layer['filt_width'])
-    
+
+    # Ouput info
+    (layer['out_height'], layer['out_width'], _, _, _, _) = compute_padding_2d(
+        layer['padding'],
+        layer['in_height'],
+        layer['in_width'],
+        layer['stride_height'],
+        layer['stride_width'],
+        layer['filt_height'],
+        layer['filt_width'],
+    )
+
     output_shape = [input_shapes[0][0], layer['n_filt'], layer['out_height'], layer['out_width']]
-    
-    return layer, output_shape
+
+    return layer, output_shape
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/pytorch/core.py` & `hls4ml-0.7.0rc1/hls4ml/converters/pytorch/core.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,75 +1,81 @@
-import numpy as np
-
 from hls4ml.converters.pytorch_to_hls import pytorch_handler
 
+
 # TODO: propagate use_bias info properly
-# https://github.com/fastmachinelearning/hls4ml/issues/409 
+# https://github.com/fastmachinelearning/hls4ml/issues/409
 @pytorch_handler('Linear')
 def parse_linear_layer(pytorch_layer, layer_name, input_shapes, data_reader, config):
-    assert('Linear' in pytorch_layer.__class__.__name__)
-    
+    assert 'Linear' in pytorch_layer.__class__.__name__
+
     layer = {}
-   
+
     layer['class_name'] = 'Dense'
     layer['name'] = layer_name
-    
+
     layer['n_in'] = pytorch_layer.in_features
     layer['n_out'] = pytorch_layer.out_features
-    
-    #Handling whether bias is used or not
-    assert not pytorch_layer.bias is None, "PyTorch Linear with bias=False not yet supported"
-    if pytorch_layer.bias is None:    
+
+    # Handling whether bias is used or not
+    assert pytorch_layer.bias is not None, "PyTorch Linear with bias=False not yet supported"
+    if pytorch_layer.bias is None:
         layer['use_bias'] = False
     else:
         layer['use_bias'] = True
-        
+
     output_shape = [input_shapes[0][0], layer['n_out']]
-    
+
     return layer, output_shape
 
+
 # TODO: propagate parametrized activation parameters
-# https://github.com/fastmachinelearning/hls4ml/issues/409 
+# https://github.com/fastmachinelearning/hls4ml/issues/409
 # activation_layers = ['LeakyReLU', 'ThresholdedReLU', 'ELU', 'PReLU', 'Softmax', 'ReLU']
 activation_layers = ['Softmax', 'ReLU']
+
+
 @pytorch_handler(*activation_layers)
 def parse_activation_layer(pytorch_layer, layer_name, input_shapes, data_reader, config):
-    
+
     layer = {}
-    
-    layer['class_name'] =  pytorch_layer.__class__.__name__
+
+    layer['class_name'] = pytorch_layer.__class__.__name__
     layer['activation'] = layer['class_name']
     layer['name'] = layer_name
-    
+
     if layer['class_name'] == 'ReLU':
         layer['class_name'] = 'Activation'
-    
-    output_shape=input_shapes[0]
-    
+
+    output_shape = input_shapes[0]
+
     return layer, output_shape
 
+
 batchnorm_layers = ['BatchNorm2d', 'BatchNorm1d']
+
+
 @pytorch_handler(*batchnorm_layers)
 def parse_batchnorm_layer(pytorch_layer, layer_name, input_shapes, data_reader, config):
-    assert('BatchNorm' in pytorch_layer.__class__.__name__)
-    
+    assert 'BatchNorm' in pytorch_layer.__class__.__name__
+
     layer = {}
-   
+
     layer['class_name'] = 'BatchNormalization'
     layer['data_format'] = 'channels_first'
     layer['name'] = layer_name
-    
-    #batchnorm para
+
+    # batchnorm para
     layer['epsilon'] = pytorch_layer.eps
-    
+    layer['use_gamma'] = layer['use_beta'] = pytorch_layer.affine
+
     in_size = 1
     for dim in input_shapes[0][1:]:
         in_size *= dim
-        
+
     layer['n_in'] = layer['n_out'] = in_size
-    
+
     if len(input_shapes[0]) == 2:
         layer['n_filt'] = -1
     elif len(input_shapes[0]) > 2:
-        layer['n_filt']=input_shapes[0][1] #Always channel first for Pytorch
+        layer['n_filt'] = input_shapes[0][1]  # Always channel first for Pytorch
 
-    return layer, [shape for shape in input_shapes[0]]
+    return layer, [shape for shape in input_shapes[0]]
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/pytorch_to_hls.py` & `hls4ml-0.7.0rc1/hls4ml/converters/pytorch_to_hls.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,209 +1,206 @@
-from __future__ import print_function
 import numpy as np
-import os
-import yaml
-import sys
 import torch
-import pickle
-import re
 
-from hls4ml.model import HLSModel
+from hls4ml.model import ModelGraph
 
-class PyTorchModelReader(object):
+
+class PyTorchModelReader:
     """
     Pytorch data reader to be used for extracting relevant information during conversion.
     """
+
     def __init__(self, config):
         self.torch_model = config['PytorchModel']
         self.state_dict = self.torch_model.state_dict()
         self.input_shape = config['InputShape']
-    
+
     def get_weights_data(self, layer_name, var_name):
         """Get weights data from layers.
-        
+
         The hls layer classes are based on Keras's default parameters.
         Thus, this function will also need to account for some differences
         between Keras and Pytorch terminology.
-        
+
         Parameters
         ----------
         layer_name : string
             layer's name in the ONNX model
         var_name : string
             variable to be extracted
 
         Returns
         -------
         data : numpy array
-            extracted weights data 
-        
+            extracted weights data
+
         """
-        
+
         data = None
-        
-        #Parameter mapping from pytorch to keras
+
+        # Parameter mapping from pytorch to keras
         torch_paramap = {
-        #Conv
-        'kernel': 'weight', 
-        #Batchnorm
-        'gamma': 'weight',
-        'beta': 'bias',
-        'moving_mean':'running_mean',
-        'moving_variance': 'running_var'}
-            
+            # Conv
+            'kernel': 'weight',
+            # Batchnorm
+            'gamma': 'weight',
+            'beta': 'bias',
+            'moving_mean': 'running_mean',
+            'moving_variance': 'running_var',
+        }
+
         if var_name not in list(torch_paramap.keys()) + ['weight', 'bias']:
             raise Exception('Pytorch parameter not yet supported!')
-        
+
         elif var_name in list(torch_paramap.keys()):
             var_name = torch_paramap[var_name]
-            
-        data = self.state_dict[layer_name + '.' + var_name].numpy().transpose() #Look at transpose when systhesis produce lousy results. Might need to remove it.
-        
+
+        data = (
+            self.state_dict[layer_name + '.' + var_name].numpy().transpose()
+        )  # Look at transpose when systhesis produce lousy results. Might need to remove it.
+
         return data
-    
-class PyTorchFileReader(PyTorchModelReader): #Inherit get_weights_data method
+
+
+class PyTorchFileReader(PyTorchModelReader):  # Inherit get_weights_data method
     def __init__(self, config):
         self.config = config
 
         if not torch.cuda.is_available():
             self.torch_model = torch.load(config['PytorchModel'], map_location=lambda storage, loc: storage)
         else:
             self.torch_model = torch.load(config['PytorchModel'])
-        
-        #Get input tensor's shape
+
+        # Get input tensor's shape
         self.input_shape = config.get('InputShape')
-        
-        if self.input_shape == None:
+
+        if self.input_shape is None:
             raise Exception('Must specify input shape ("InputShape") in config!')
-        
-        #Convert it to a list
+
+        # Convert it to a list
         self.input_shape = self.input_shape.strip('(,)').split(',')
         self.input_shape = [None if n == 'None' else int(n) for n in self.input_shape]
 
         self.state_dict = self.torch_model.state_dict()
-        
-        return data
 
-####----------------------Layer handling---------------------######
+
+# ----------------------Layer handling--------------------- #
 layer_handlers = {}
 
+
 def register_pytorch_layer_handler(layer_name, handler_func):
     if layer_name in layer_handlers:
-        raise Exception('Layer {} already registered'.format(layer_name))
+        raise Exception(f'Layer {layer_name} already registered')
     else:
         layer_handlers[layer_name] = handler_func
 
+
 def get_supported_pytorch_layers():
     return list(layer_handlers.keys())
 
+
 def pytorch_handler(*args):
     def decorator(function):
         function.handles = [arg for arg in args]
         return function
+
     return decorator
-####----------------------------------------------------------------
+
+
+# ----------------------------------------------------------------
+
 
 def pytorch_to_hls(config):
-    """ Convert Pytorch model to hls model from configuration.
-    
+    """Convert Pytorch model to hls model from configuration.
+
     Parameters
     ----------
     config: dict
         pytorch configuration from yaml file or passed through API.
-        
+
     Returns
     -------
-    hls_model : hls4ml model object.
-    
+    ModelGraph : hls4ml model object.
+
     Notes
     -----
     Only sequential pytorch models are supported for now.
     """
-    
-    #This is a list of dictionaries to hold all the layer info we need to generate HLS
+
+    # This is a list of dictionaries to hold all the layer info we need to generate HLS
     layer_list = []
 
     print('Interpreting Model ...')
-    
-    reader = PyTorchFileReader(config) if isinstance(config['PytorchModel'],str) else PyTorchModelReader(config)
+
+    reader = PyTorchFileReader(config) if isinstance(config['PytorchModel'], str) else PyTorchModelReader(config)
     input_shapes = [list(reader.input_shape)]
-        
+
     model = reader.torch_model
 
-    #Define layers to skip for conversion to HLS
+    # Define layers to skip for conversion to HLS
     skip_layers = ['Dropout', 'Flatten', 'Sequential']
-    
-    #All supported layers
+
+    # All supported layers
     supported_layers = get_supported_pytorch_layers() + skip_layers
-    
-    #Map inputs of skipped and split (activation) layers
-    inputs_map = {}
-
-    input_layers = None
-    output_layers = None
-    
-    layer_config = None
-    
-    #Output shape tracking
+
+    # Output shape tracking
     output_shapes = {}
     output_shape = None
-    
-    #Loop through layers
+
+    # Loop through layers
     print('Topology:')
     layer_counter = 0
-    
-    #First add input layer
+
+    # First add input layer
     input_layer = {}
     input_layer['name'] = 'input1'
     input_layer['class_name'] = 'InputLayer'
     input_layer['input_shape'] = input_shapes[0][1:]
     layer_list.insert(0, input_layer)
     print("Input Shape: ", input_shapes)
-    
+
     for layer_name, pytorch_layer in model.named_modules():
-        
+
         pytorch_class = pytorch_layer.__class__.__name__
-        
-        #First module is the whole model's class
+
+        # First module is the whole model's class
         if pytorch_class == model.__class__.__name__:
             continue
-        
+
         if pytorch_class not in supported_layers:
-            raise Exception('Unsupported layer {}'.format(pytorch_class))
-                
-        #If not the first layer then input shape is taken from last layer's output
+            raise Exception(f'Unsupported layer {pytorch_class}')
+
+        # If not the first layer then input shape is taken from last layer's output
         if layer_counter != 0:
-            input_shapes = [output_shape] #In case there are multiple inputs
-        
-        #Handle skipped layers
+            input_shapes = [output_shape]  # In case there are multiple inputs
+
+        # Handle skipped layers
         if pytorch_class in skip_layers:
-            if pytorch_class == 'Sequential': #Ignore the mother module's class name
+            if pytorch_class == 'Sequential':  # Ignore the mother module's class name
                 continue
 
             if pytorch_class == 'Flatten':
                 output_shapes[layer_name] = [input_shapes[0][0], np.prod(input_shapes[0][1:])]
             else:
                 output_shapes[layer_name] = input_shapes[0]
-            continue #!!
-        
-        #Increment the layer counter after initial screenings
+            continue  # !!
+
+        # Increment the layer counter after initial screenings
         if pytorch_class in supported_layers:
             layer_counter += 1
-        
-        #Process the layer
+
+        # Process the layer
         layer, output_shape = layer_handlers[pytorch_class](pytorch_layer, layer_name, input_shapes, reader, config)
 
         print('Layer name: {}, layer type: {}, input shape: {}'.format(layer['name'], layer['class_name'], input_shapes))
         layer_list.append(layer)
-        
-        assert(output_shape is not None)
+
+        assert output_shape is not None
         output_shapes[layer['name']] = output_shape
-           
 
     #################
-    ## Generate HLS
+    # Generate HLS
     #################
-    
+
     print('Creating HLS model')
-    hls_model = HLSModel(config, reader, layer_list)
+    hls_model = ModelGraph(config, reader, layer_list)
     return hls_model
```

### Comparing `hls4ml-0.6.0/hls4ml/converters/tf_to_hls.py` & `hls4ml-0.7.0rc1/hls4ml/converters/tf_to_hls.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,43 +1,42 @@
-from __future__ import print_function
-import numpy as np
-import h5py
-import json
 import math
 import os
+
+import numpy as np
 import tensorflow as tf
 from tensorflow.python.framework import tensor_util
 
-from hls4ml.model import HLSModel
+from hls4ml.model import ModelGraph
 
 MAXMULT = 4096
 
+
 class TFDataReader:
     def __init__(self, graph):
         self.graph = graph
         self.const_ops = [c for c in self.graph.get_operations() if c.type == 'Const']
 
     def get_weights_data(self, layer_name, var_name):
         tf_op = self.graph.get_operation_by_name(layer_name)
         data = None
         if tf_op is not None:
-            if tf_op.type == 'MatMul' and var_name == 'kernel': # MatMul is mapped to Dense, but there is no bias
+            if tf_op.type == 'MatMul' and var_name == 'kernel':  # MatMul is mapped to Dense, but there is no bias
                 w_tensor = tf_op.inputs[1]
                 data = self.read_variable_data(w_tensor)
 
             if tf_op.type == 'Conv2D' and var_name == 'kernel':
                 w_tensor = tf_op.inputs[1]
                 data = self.read_variable_data(w_tensor)
 
             if tf_op.type == 'BiasAdd':
                 b_tensor = tf_op.inputs[1]
                 data = self.read_variable_data(b_tensor)
-            
+
             if tf_op.type == 'FusedBatchNorm':
-                bn_weighs_map = { 'gamma': 1, 'beta': 2, 'moving_mean': 3, 'moving_variance': 4 }
+                bn_weighs_map = {'gamma': 1, 'beta': 2, 'moving_mean': 3, 'moving_variance': 4}
                 w_idx = bn_weighs_map[var_name]
                 w_tensor = tf_op.inputs[w_idx]
                 data = self.read_variable_data(w_tensor)
 
         return data
 
     def read_variable_data(self, tensor):
@@ -46,144 +45,143 @@
             tensor = parent_op.inputs[0]
             parent_op = tensor.op
 
         data = tensor_util.MakeNdarray(parent_op.node_def.attr['value'].tensor)
 
         return data
 
+
 def _parse_data_format(data_format):
     if data_format == 'NCHW':
         format_str = 'channels_first'
         c_idx = 1
         h_idx = 2
         w_idx = 3
     else:
         format_str = 'channels_last'
         h_idx = 1
         w_idx = 2
         c_idx = 3
-    
+
     return format_str, c_idx, h_idx, w_idx
 
+
 def _compute_pads_2d(layer, in_height, in_width):
     if layer['padding'] == 'same':
-        #Height
+        # Height
         layer['out_height'] = int(math.ceil(float(in_height) / float(layer['stride_height'])))
-        if (in_height % layer['stride_height'] == 0):
+        if in_height % layer['stride_height'] == 0:
             pad_along_height = max(layer['filt_height'] - layer['stride_height'], 0)
         else:
             pad_along_height = max(layer['filt_height'] - (in_height % layer['stride_height']), 0)
-        layer['pad_top']  = pad_along_height // 2
-        layer['pad_bottom']  = pad_along_height - layer['pad_top']
-        #Width
+        layer['pad_top'] = pad_along_height // 2
+        layer['pad_bottom'] = pad_along_height - layer['pad_top']
+        # Width
         layer['out_width'] = int(math.ceil(float(in_width) / float(layer['stride_width'])))
-        if (in_width % layer['stride_width'] == 0):
+        if in_width % layer['stride_width'] == 0:
             pad_along_width = max(layer['filt_width'] - layer['stride_width'], 0)
         else:
             pad_along_width = max(layer['filt_width'] - (in_width % layer['stride_width']), 0)
-        layer['pad_left']  = pad_along_width // 2
-        layer['pad_right']  = pad_along_width - layer['pad_left']
-    elif layer['padding']=='valid':
+        layer['pad_left'] = pad_along_width // 2
+        layer['pad_right'] = pad_along_width - layer['pad_left']
+    elif layer['padding'] == 'valid':
         layer['out_width'] = int(math.ceil(float(in_width - layer['filt_width'] + 1) / float(layer['stride_width'])))
         layer['out_height'] = int(math.ceil(float(in_height - layer['filt_height'] + 1) / float(layer['stride_height'])))
         layer['pad_top'] = 0
         layer['pad_bottom'] = 0
         layer['pad_left'] = 0
         layer['pad_right'] = 0
     elif layer['padding'] == 'explicit':
-        #paddings = tf_op.get_attr('explicit_paddings')
+        # paddings = tf_op.get_attr('explicit_paddings')
         raise NotImplementedError('Explicit padding is not supported yet.')
 
+
 def _find_graph_outputs(graph):
     input_list = []
     output_list = []
 
     for tf_op in graph.get_operations():
         input_list.extend(tf_op.inputs)
         if tf_op.type != 'FusedBatchNorm':
             output_list.extend(tf_op.outputs)
 
     return [o.name.rsplit(':', 1)[0] for o in list(set(output_list) - set(input_list))]
 
+
 def _parse_tensor_names(tensors):
     if not isinstance(tensors, list):
         tensors = [tensors]
 
     tensor_names = []
     for t in tensors:
         tensor_names.append(t.name.rsplit(':', 1)[0])
 
     return tensor_names
 
+
 def tf_to_hls(yamlConfig):
 
     ######################
-    ##  Do translation
+    #  Do translation
     ######################
 
-    #This is a list of dictionaries to hold all the layer info we need to generate HLS
+    # This is a list of dictionaries to hold all the layer info we need to generate HLS
     layer_list = []
 
     if not os.path.exists(yamlConfig['TensorFlowModel']):
         raise Exception('The specified file does not exist: {}'.format(yamlConfig['TensorFlowModel']))
 
     graph_def = None
     graph = None
 
-    #Extract model architecture from pb
+    # Extract model architecture from pb
     try:
         with tf.io.gfile.GFile(yamlConfig['TensorFlowModel'], "rb") as f:
             graph_def = tf.compat.v1.GraphDef()
             graph_def.ParseFromString(f.read())
     except BaseException as e:
-        raise Exception('Error loading the graph definition: {}'.format(str(e)))
+        raise Exception(f'Error loading the graph definition: {str(e)}')
 
     try:
         assert graph_def is not None
         with tf.Graph().as_default() as graph:
-            tf.import_graph_def(
-                graph_def,
-                input_map=None,
-                return_elements=None,
-                name='',
-                producer_op_list=None
-            )
+            tf.import_graph_def(graph_def, input_map=None, return_elements=None, name='', producer_op_list=None)
     except BaseException as e:
-        raise Exception('Error importing the graph: {}'.format(str(e)))
+        raise Exception(f'Error importing the graph: {str(e)}')
 
-    #Define supported operations
+    # Define supported operations
     array_ops = ['ConcatV2', 'StridedSlice', 'Transpose']
     core_ops = ['Const', 'Identity', 'Placeholder']
     image_ops = ['ResizeNearestNeighbor']
     math_ops = ['Add', 'MatMul', 'Mul', 'Sigmoid']
     nn_ops = ['AvgPool', 'BiasAdd', 'Conv2D', 'Elu', 'FusedBatchNorm', 'MaxPool', 'Relu', 'Selu', 'Softmax']
     supported_ops = array_ops + core_ops + image_ops + math_ops + nn_ops
 
     input_layers = []
     output_layers = _find_graph_outputs(graph)
 
     # Get input shape and check for unsupported layer type
     output_shape = None
     for tf_op in graph.get_operations():
         if tf_op.type not in supported_ops:
-            raise Exception('ERROR: Unsupported layer type: {}'.format(tf_op.type))
+            raise Exception(f'ERROR: Unsupported layer type: {tf_op.type}')
 
     print('Topology:')
     for tf_op in graph.get_operations():
         handled = False
 
         layer = {}
         layer['name'] = tf_op.name
 
         if tf_op.type == 'Placeholder':
-            if len(tf_op.inputs) == 0: # Input
+            if len(tf_op.inputs) == 0:  # Input
                 output_shape = tf_op.outputs[0].shape.as_list()
                 layer['class_name'] = 'InputLayer'
                 layer['input_shape'] = output_shape[1:]
-                #layer['outputs'] = [tf_op.outputs[0].name for o in tf_op.outputs]
+                # layer['outputs'] = [tf_op.outputs[0].name for o in tf_op.outputs]
                 layer['outputs'] = _parse_tensor_names(tf_op.outputs)
                 input_layers.append(layer['name'])
                 handled = True
 
         elif tf_op.type == 'Const' or tf_op.type == 'Identity':
             # Nothing to do here, TFDataReader handles these
             handled = True
@@ -281,15 +279,15 @@
             _compute_pads_2d(layer, in_height, in_width)
 
             handled = True
 
         elif tf_op.type == 'FusedBatchNorm':
             input_shape = tf_op.inputs[0].shape.as_list()
             output_shape = tf_op.outputs[0].shape.as_list()
-            
+
             layer['class_name'] = 'BatchNormalization'
             layer['inputs'] = _parse_tensor_names(tf_op.inputs[0])
             layer['outputs'] = _parse_tensor_names(tf_op.outputs[0])
             layer['data_format'], c_idx, h_idx, w_idx = _parse_data_format(tf_op.get_attr('data_format').decode())
             layer['n_in'] = np.prod(input_shape[1:])
             layer['epsilon'] = tf_op.get_attr('epsilon')
 
@@ -306,29 +304,29 @@
             layer['outputs'] = _parse_tensor_names(tf_op.outputs[0])
             output_shape = tf_op.outputs[0].shape.as_list()
 
             rank = tf_op.get_attr('N')
             if rank != 2:
                 raise Exception('Unsupported number of inputs in Concat operation')
 
-            layer['op'] = layer['class_name'].lower() + '{}d'.format(rank)
-            layer['axis'] = tf_op.inputs[2].op.node_def.attr['value'].tensor.int_val[0] # Urgh!
+            layer['op'] = layer['class_name'].lower() + f'{rank}d'
+            layer['axis'] = tf_op.inputs[2].op.node_def.attr['value'].tensor.int_val[0]  # Urgh!
 
             handled = True
 
         elif tf_op.type in ['Add', 'Mul']:
             layer['class_name'] = 'Merge'
             layer['inputs'] = _parse_tensor_names(list(tf_op.inputs))
             layer['outputs'] = _parse_tensor_names(tf_op.outputs[0])
             output_shape = tf_op.outputs[0].shape.as_list()
-            
+
             layer['op'] = tf_op.type.lower()
             if layer['op'] == 'mul':
                 layer['op'] = 'multiply'
-            
+
             handled = True
 
         elif tf_op.type == 'Transpose':
             layer['class_name'] = 'Transpose'
             layer['inputs'] = _parse_tensor_names(tf_op.inputs[0])
             layer['outputs'] = _parse_tensor_names(tf_op.outputs[0])
             layer['perm'] = tensor_util.MakeNdarray(tf_op.inputs[1].op.node_def.attr['value'].tensor).tolist()
@@ -338,15 +336,15 @@
 
         elif tf_op.type == 'ResizeNearestNeighbor':
             layer['class_name'] = 'Resize'
             layer['algorithm'] = 'nearest'
             layer['inputs'] = _parse_tensor_names(tf_op.inputs[0])
             layer['outputs'] = _parse_tensor_names(tf_op.outputs[0])
 
-            input_shape = tf_op.inputs[0].shape.as_list() # (B, H, W, C)
+            input_shape = tf_op.inputs[0].shape.as_list()  # (B, H, W, C)
             output_shape = tf_op.outputs[0].shape.as_list()
             layer['in_height'] = input_shape[1]
             layer['in_width'] = input_shape[2]
             layer['n_chan'] = input_shape[3]
             layer['out_height'] = output_shape[1]
             layer['out_width'] = output_shape[2]
 
@@ -357,20 +355,20 @@
             half_pixel_centers = tf_op.get_attr('align_corners')
             if half_pixel_centers:
                 raise NotImplementedError('Property "half_pixel_centers=True" is not supported.')
 
             handled = True
 
         if not handled:
-            raise Exception('Unable to parse operation: {} - {}'.format(tf_op.type, tf_op.name))
+            raise Exception(f'Unable to parse operation: {tf_op.type} - {tf_op.name}')
 
         print('Layer name: {}, layer type: {}, current shape: {}'.format(layer['name'], layer['class_name'], output_shape))
         layer_list.append(layer)
 
     #################
-    ## Generate HLS
+    # Generate HLS
     #################
 
     reader = TFDataReader(graph)
     print('Creating HLS model')
-    hls_model = HLSModel(yamlConfig, reader, layer_list, input_layers, output_layers)
+    hls_model = ModelGraph(yamlConfig, reader, layer_list, input_layers, output_layers)
     return hls_model
```

### Comparing `hls4ml-0.6.0/hls4ml/model/hls_model.py` & `hls4ml-0.7.0rc1/hls4ml/model/graph.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,29 +1,31 @@
-from __future__ import print_function
-import six
+import ctypes
 import os
-import sys
 import platform
-import ctypes
-import re
+from collections import OrderedDict
+
 import numpy as np
 import numpy.ctypeslib as npc
-from collections import OrderedDict
 
-from hls4ml.model.hls_layers import *
-from hls4ml.templates import get_backend
-from hls4ml.writer import get_writer
-from hls4ml.model.optimizer import optimize_model, get_available_passes
-from hls4ml.report.vivado_report import parse_vivado_report
+from hls4ml.backends import get_backend
+from hls4ml.model.flow import get_flow
+from hls4ml.model.layers import layer_map
+from hls4ml.model.optimizer import get_available_passes, optimize_model
+
+
+class HLSConfig:
+    """The configuration class as stored in the ModelGraph.
+
+    Args:
+        config (dict):  The configuration dictionary
+    """
 
-class HLSConfig(object):
     def __init__(self, config):
         self.config = config
         self.backend = get_backend(self.config.get('Backend', 'Vivado'))
-        self.writer = get_writer(self.config.get('Backend', 'Vivado'))
 
         self.model_precision = {}
         self.layer_type_precision = {}
         self.layer_name_precision = {}
 
         self.model_rf = None
         self.layer_type_rf = {}
@@ -58,104 +60,104 @@
 
     def get_output_dir(self):
         return self.get_config_value('OutputDir')
 
     def get_layer_config_value(self, layer, key, default=None):
         hls_config = self.config['HLSConfig']
 
-        name_config = hls_config.get('LayerName', {}).get(layer.name.lower(), None)
+        name_config = hls_config.get('LayerName', {}).get(layer.name, None)
         if name_config is not None:
             return name_config.get(key, default)
 
-        type_config = hls_config.get('LayerType', {}).get(layer.__class__.__name__, None)
+        type_config = hls_config.get('LayerType', {}).get(layer.class_name, None)
         if type_config is not None:
             return type_config.get(key, default)
 
         model_config = hls_config.get('Model', None)
         if model_config is not None:
             return model_config.get(key, default)
 
         return default
 
     def get_layer_config(self, layer):
         hls_config = self.config['HLSConfig']
         layer_config = {}
 
-        type_config = hls_config.get('LayerType', {}).get(layer.__class__.__name__, None)
+        type_config = hls_config.get('LayerType', {}).get(layer.class_name, None)
         if type_config is not None:
             layer_config.update(type_config)
 
-        name_config = hls_config.get('LayerName', {}).get(layer.name.lower(), None)
+        name_config = hls_config.get('LayerName', {}).get(layer.name, None)
         if name_config is not None:
             layer_config.update(name_config)
 
         return layer_config
 
     def get_precision(self, layer, var='default'):
         precision = self.layer_name_precision.get(layer.name.lower() + '_' + var)
         type_name = layer.name.lower() + '_' + var + '_t'
         if precision is None:
             precision = self.layer_name_precision.get(layer.name.lower() + '_default')
             type_name = layer.name.lower() + '_default_t'
 
         if precision is None:
-            precision = self.layer_type_precision.get(layer.__class__.__name__.lower() + '_' + var)
-            type_name = layer.__class__.__name__ + '_' + var + '_t'
+            precision = self.layer_type_precision.get(layer.class_name.lower() + '_' + var)
+            type_name = layer.class_name + '_' + var + '_t'
         if precision is None:
-            precision = self.layer_type_precision.get(layer.__class__.__name__.lower() + '_default')
-            type_name = layer.__class__.__name__ + '_default_t'
+            precision = self.layer_type_precision.get(layer.class_name.lower() + '_default')
+            type_name = layer.class_name + '_default_t'
 
         if precision is None:
             precision = self.model_precision.get(var)
             type_name = var + '_default_t'
         if precision is None:
             precision = self.model_precision.get('default')
             type_name = 'model_default_t'
 
         if precision is None:
-            raise Exception('No precision for {}->{} found and no default specified.'.format(layer.name, var))
+            raise Exception(f'No precision for {layer.name}->{var} found and no default specified.')
 
         precision = self.backend.convert_precision_string(precision)
 
         return (precision, type_name)
 
     def get_bram_size(self, layer):
         bf = self.model_bf
         return bf
 
     def get_reuse_factor(self, layer):
         rf = self.layer_name_rf.get(layer.name.lower())
         if rf is None:
-            rf = self.layer_type_rf.get(layer.__class__.__name__.lower())
+            rf = self.layer_type_rf.get(layer.class_name.lower())
         if rf is None:
             rf = self.model_rf
 
         if rf is None:
-            raise Exception('No reuse factor for {} found and no default specified.'.format(layer.name))
+            raise Exception(f'No reuse factor for {layer.name} found and no default specified.')
 
         return rf
 
     def get_target_cycles(self, layer):
         targ_cycles = self.layer_name_targ_cycles.get(layer.name.lower())
         if targ_cycles is None:
             targ_cycles = self.layer_name_targ_cycles.get(layer.__class__.__name__.lower())
         if targ_cycles is None:
             targ_cycles = self.model_targ_cycles
- 
+
         return targ_cycles
 
     def get_strategy(self, layer):
         strategy = self.layer_name_strategy.get(layer.name.lower())
         if strategy is None:
-            strategy = self.layer_type_strategy.get(layer.__class__.__name__.lower())
+            strategy = self.layer_type_strategy.get(layer.class_name.lower())
         if strategy is None:
             strategy = self.model_strategy
 
         return strategy
-    
+
     def get_conv_implementation(self, layer):
         conv_implementation = self.layer_name_conv_implementation.get(layer.name.lower())
         if conv_implementation is None:
             conv_implementation = self.layer_type_conv_implementation.get(layer.__class__.__name__.lower())
         if conv_implementation is None:
             conv_implementation = self.model_conv_implementation
 
@@ -163,47 +165,52 @@
 
     def is_resource_strategy(self, layer):
         return self.get_strategy(layer).lower() == 'resource'
 
     def get_compression(self, layer):
         compression = self.layer_name_compression.get(layer.name.lower())
         if compression is None:
-            compression = self.layer_type_compression.get(layer.__class__.__name__.lower())
+            compression = self.layer_type_compression.get(layer.class_name.lower())
         if compression is None:
             compression = self.model_compression
 
         return compression
 
     def _parse_hls_config(self):
         hls_config = self.config['HLSConfig']
-        
+
+        self.flows = hls_config.get('Flows')
+        if self.flows is None:
+            self.flows = [self.backend.get_default_flow()]
+
+        # TODO this is now effectively broken
         self.optimizers = hls_config.get('Optimizers')
         if 'SkipOptimizers' in hls_config:
             if self.optimizers is not None:
                 raise Exception('Invalid optimizer configuration, please use either "Optimizers" or "SkipOptimizers".')
             skip_optimizers = hls_config.get('SkipOptimizers')
             selected_optimizers = get_available_passes()
             for opt in skip_optimizers:
                 try:
                     selected_optimizers.remove(opt)
                 except ValueError:
-                    pass                
+                    pass
             self.optimizers = selected_optimizers
-        
+
         model_cfg = hls_config.get('Model')
         if model_cfg is not None:
             precision_cfg = model_cfg.get('Precision')
             if precision_cfg is not None:
                 if isinstance(precision_cfg, dict):
                     for var, precision in precision_cfg.items():
                         self.model_precision[var] = precision
                 else:
-                    self.model_precision['default'] = precision_cfg # Default precision for everything
+                    self.model_precision['default'] = precision_cfg  # Default precision for everything
 
-            self.model_bf = model_cfg.get('BramFactor', np.inf) # Weight threshold to be external BRAM
+            self.model_bf = model_cfg.get('BramFactor', np.inf)  # Weight threshold to be external BRAM
             self.model_rf = model_cfg.get('ReuseFactor')
             self.model_targ_cycles = model_cfg.get('TargetCycles')
             self.model_conv_implementation = model_cfg.get('ConvImplementation', 'LineBuffer')
             self.model_strategy = model_cfg.get('Strategy', 'Latency')
             self.model_compression = bool(model_cfg.get('Compression', 0))
 
         layer_type_cfg = hls_config.get('LayerType')
@@ -215,15 +222,15 @@
                         self.layer_type_precision[layer_type.lower() + '_' + var] = precision
                 else:
                     self.layer_type_precision[layer_type.lower() + '_default'] = precision_cfg
 
                 rf = layer_cfg.get('ReuseFactor')
                 if rf is not None:
                     self.layer_type_rf[layer_type.lower()] = rf
-                
+
                 targ_cycles = layer_cfg.get('TargetCycles')
                 if targ_cycles is not None:
                     self.layer_type_targ_cycles[layer_type.lower()] = targ_cycles
 
                 strategy = layer_cfg.get('Strategy')
                 if strategy is not None:
                     self.layer_type_strategy[layer_type.lower()] = strategy
@@ -269,57 +276,97 @@
     def _validate_hls_config(self):
         use_resource = False
         if self.model_strategy.lower() == 'latency' and self.model_compression:
             print('WARNING: Compression enabled while model strategy set to "Latency".')
             use_resource = True
         for layer_type, strategy in self.layer_type_strategy.items():
             if strategy.lower() == 'resource' and self.model_strategy.lower() == 'latency':
-                print('WARNING: Strategy for layer type {} set to "Resource", while model strategy set to "Latency".'.format(layer_type))
+                print(
+                    'WARNING: Strategy for layer type {} set to "Resource", while model strategy set to "Latency".'.format(
+                        layer_type
+                    )
+                )
                 use_resource = True
 
         for layer_name, strategy in self.layer_name_strategy.items():
             if strategy.lower() == 'resource' and self.model_strategy.lower() == 'latency':
-                print('WARNING: Strategy for layer {} set to "Resource", while model strategy set to "Latency".'.format(layer_name))
+                print(
+                    'WARNING: Strategy for layer {} set to "Resource", while model strategy set to "Latency".'.format(
+                        layer_name
+                    )
+                )
                 use_resource = True
 
         for layer_type, compression in self.layer_type_compression.items():
             if compression and self.model_strategy.lower() == 'latency':
-                print('WARNING: Compression enabled for layer type {}, while model strategy set to "Latency".'.format(layer_type))
+                print(
+                    'WARNING: Compression enabled for layer type {}, while model strategy set to "Latency".'.format(
+                        layer_type
+                    )
+                )
                 use_resource = True
 
         for layer_name, compression in self.layer_name_compression.items():
             if compression and self.model_strategy.lower() == 'latency':
-                print('WARNING: Compression enabled for layer {}, while model strategy set to "Latency".'.format(layer_name))
+                print(f'WARNING: Compression enabled for layer {layer_name}, while model strategy set to "Latency".')
                 use_resource = True
 
         if use_resource:
             print('WARNING: Changing model strategy to "Resource"')
             self.model_strategy = 'Resource'
 
-class HLSModel(object):
+
+class ModelGraph:
+    """The ModelGraph represents the network that is being processed by hls4ml.
+
+    Args:
+        config (dict):  The configuration dictionary
+        data_reader:  The data reader from where weights can be extracted
+        layer_list (list(dict)):  The list contains a dictionary for each input layer
+        inputs (list, optional):  The inputs to the model. If None, determined from layer_list
+        outputs (list, optional):  The outputs to the model. If None, determined from layer_list
+    """
+
     def __init__(self, config, data_reader, layer_list, inputs=None, outputs=None):
         self.config = HLSConfig(config)
         self.reader = data_reader
 
-        # If not provided, assumes layer_list[0] is input, and layer_list[-1] is output
-        self.inputs = inputs if inputs is not None else [layer_list[0]['name']]
-        self.outputs = outputs if outputs is not None else [layer_list[-1]['name']]
+        # keep track of the applied flows
+        self._applied_flows = []
+
+        # If not provided, assumes layer_list[0] is the input layer, and layer_list[-1] is output layer
+
+        # Note, these are actually the variable names, which may differ from the layer name
+        input_layers = inputs if inputs is not None else [layer_list[0]['name']]
+        output_layers = outputs if outputs is not None else [layer_list[-1]['name']]
+        self.inputs = self._find_output_variable_names(layer_list, input_layers)
+        if self.inputs != input_layers:
+            raise RuntimeError(
+                "Currently only support the case when input variables and input layer names match\n"
+                + f"Input layers = {input_layers}, input_vars = {self.inputs}"
+            )
+        self.outputs = self._find_output_variable_names(layer_list, output_layers)
 
         self.index = 0
-        self.graph = OrderedDict()
+        self.graph = OrderedDict()  # where the nodes are stored
         self.output_vars = {}
 
-        # External BRAM 
-        self.bram_vars = {}
-
         self._top_function_lib = None
 
         self._make_graph(layer_list)
 
-        self._optimize_model(self.config.optimizers)
+        for flow in self.config.flows:
+            self.apply_flow(flow)
+
+    def _find_output_variable_names(self, layer_list, layer_names):
+        """Given a list of all layers, and a list input/output names, find the names of the their outputs that will be used
+        as the name of the output variables."""
+        inout_nodes = [node for node in layer_list if node['name'] in layer_names]
+        all_node_output_names = [node['outputs'] if 'outputs' in node else [node['name']] for node in inout_nodes]
+        return [output for node_output_names in all_node_output_names for output in node_output_names]  # to flatten
 
     def _make_graph(self, layer_list):
         for layer in layer_list:
             kind = layer['class_name']
             name = layer['name']
             inputs = layer.get('inputs', [])
             outputs = layer.get('outputs', [])
@@ -328,156 +375,224 @@
             elif len(inputs) == 0:
                 inputs = [next(reversed(self.graph), 'input')]
             if len(outputs) == 0:
                 outputs = [name]
 
             self.graph[name] = self.make_node(kind, name, layer, inputs, outputs)
 
-    def _optimize_model(self, optimizers):
-        optimize_model(self, optimizers)
+    def apply_flow(self, flow, reapply='single'):
+        """Applies a flow (a collection of optimizers).
+
+        Args:
+            flow (str): The name of the flow to apply
+            reapply (str, optional): Determines the action to take if the flow and its requirements have already been
+                applied. Possible values are:
+                - 'all': Apply the flow and all its requirements.
+                - 'single': Apply only the given flow, but skip the already applied requirements.
+                - 'none': Skip applying the flow.
+                Defaults to 'single'.
+        """
+
+        def all_applied_flows():
+            applied_flows = {}
+
+            for flow_group in self._applied_flows:
+                applied_flows.update({flow: set() for flow in flow_group.keys()})
+
+            return applied_flows
+
+        assert reapply in ['all', 'single', 'none']
+
+        if reapply == 'all':
+            applied_flows = {}
+        elif reapply == 'single':
+            applied_flows = all_applied_flows()
+            applied_flows.pop(flow, None)
+        else:  # reapply == 'none'
+            applied_flows = all_applied_flows()
+            if flow in applied_flows:
+                return
+
+        self._applied_flows.append(applied_flows)
+        self._apply_sub_flow(flow, applied_flows)
+
+    def _apply_sub_flow(self, flow_name, applied_flows):
+        if flow_name in applied_flows:
+            return
+        flow = get_flow(flow_name)
+
+        for sub_flow in flow.requires:
+            if sub_flow not in applied_flows.keys():
+                self._apply_sub_flow(sub_flow, applied_flows)
+
+        if len(flow.optimizers) > 0:
+            applied_passes = optimize_model(self, flow.optimizers)
+        else:
+            applied_passes = set()
+        applied_flows[flow.name] = applied_passes
 
     def make_node(self, kind, name, attributes, inputs, outputs=None):
-        """ Make a new node not connected to the model graph.
+        """Make a new node not connected to the model graph.
 
         The 'kind' should be a valid layer registered with `register_layer`. If no outputs
-        are specified, a default output named the same as the node will be created. The 
+        are specified, a default output named the same as the node will be created. The
         returned node should be added to the graph with `insert_node` or `replace_node`
         functions.
 
         Args:
-            kind (str): Type of node to add
+            kind (type or str): Type of node to add
             name (str): Name of the node
             attributes (dict): Initial set of attributes required to construct the node (Layer)
             inputs (list): List of inputs to the layer
             outputs (list, optional): The optional list of named outputs of the node
 
         Raises:
             Exception: If an attempt to insert a node with multiple inputs is made or if
                 `before` does not specify a correct node in sequence.
 
         Returns:
             Layer: The node created.
         """
 
-        if kind not in layer_map:
-            raise Exception('Layer {} not found in registry.'.format(kind))
-
-        node = layer_map[kind](self, name, attributes, inputs, outputs)
+        if isinstance(kind, str):
+            if kind not in layer_map:
+                raise Exception(f'Layer {kind} not found in registry.')
+            layer_cls = layer_map[kind]
+        else:
+            if kind not in layer_map.values():
+                raise Exception(f'Layer {kind} not found in registry.')
+            layer_cls = kind
+
+        if self.config.backend is not None:
+            layer_cls = self.config.backend.create_layer_class(layer_cls)
+        node = layer_cls(self, name, attributes, inputs, outputs)
         for o in node.outputs:
             out_var = node.get_output_variable(output_name=o)
             if o in self.outputs:
                 out_var.type.name = 'result_t'
             self.output_vars[o] = out_var
         return node
 
-    def insert_node(self, node, before=None):
-        """ Insert a new node into the model graph.
+    def insert_node(self, node, before=None, input_idx=0):
+        """Insert a new node into the model graph.
 
-        The node to be inserted should be created with `make_node()` function. The optional 
+        The node to be inserted should be created with `make_node()` function. The optional
         parameter `before` can be used to specify the node that follows in case of ambiguities.
 
         Args:
             node (Layer): Node to insert
             before (Layer, optional): The next node in sequence before which a
-                new node should be inserted. 
+                new node should be inserted.
+            input_idx (int, optional): If the next node takes multiple inputs, the input index
         Raises:
             Exception: If an attempt to insert a node with multiple inputs is made or if
                 `before` does not specify a correct node in sequence.
 
         """
         if len(node.inputs) > 1:
             raise Exception('Cannot insert a node with more than one input (for now).')
 
         prev_node = node.get_input_node(node.inputs[0])
-        next_nodes = [x for x in self.graph.values() if x.inputs[0] in prev_node.outputs]
+        next_nodes = []
+        for x in self.graph.values():
+            overlap = [value for value in x.inputs if value in prev_node.outputs]
+            if overlap:
+                next_nodes.append(x)
+
         if before is None:
             next_node = next((x for x in self.graph.values() if x.inputs[0] in prev_node.outputs), None)
         else:
             if before not in next_nodes:
-                raise Exception('Cannot insert a node {} before {} (candidates: {}).'.format(node.name, before.name, ','.join([n.name for n in next_nodes])))
+                raise Exception(
+                    'Cannot insert a node {} before {} (candidates: {}).'.format(
+                        node.name, before.name, ','.join([n.name for n in next_nodes])
+                    )
+                )
             next_node = before
 
         if next_node is not None:
-            next_node.inputs[0] = node.outputs[0]
+            next_node.inputs[input_idx] = node.outputs[0]
 
         new_graph = OrderedDict()
         for k, v in self.graph.items():
             new_graph[k] = v
             if k == prev_node.name:
                 new_graph[node.name] = node
 
         self.graph = new_graph
         self._update_model_outputs()
 
     def remove_node(self, node, rewire=True):
-        """ Remove a node from a graph.
+        """Remove a node from a graph.
 
         By default, this function can connect the outputs of previous node to the input of next one.
         Note that when removing a leaf node `rewire` should be set to `False`.
 
         Args:
             node (Layer): The node to remove
             rewire (bool, optional): If `True`, connects the outputs of the previous node
                 to the inputs of the next node
 
         Raises:
             Exception: If an attempt is made to rewire a leaf node or a node with multiple
-                inputs/outpus.
+                inputs/outputs.
 
         """
         if rewire:
-            if len(node.inputs) > 1 or len(node.outputs) > 1:
+            inputs = [inp for inp in node.inputs if inp]
+            outputs = [outp for outp in node.outputs if outp]
+            if len(inputs) > 1 or len(outputs) > 1:
                 raise Exception('Cannot rewire a node with multiple inputs/outputs')
-            prev_node = self.graph.get(node.inputs[0])
-            next_node = next((x for x in self.graph.values() if node.outputs[0] in x.inputs), None)
+            prev_node = node.get_input_node(node.inputs[0])
+            next_nodes = [x for x in self.graph.values() if node.outputs[0] in x.inputs]
             if prev_node is not None:
-                if next_node is not None:
-                    for i,_ in enumerate(next_node.inputs):
-                        if node.outputs[0] == next_node.inputs[i]:
-                            next_node.inputs[i] = prev_node.outputs[0]
-                            break
+                if len(next_nodes) > 0:
+                    for next_node in next_nodes:
+                        for i, _ in enumerate(next_node.inputs):
+                            if node.outputs[0] == next_node.inputs[i]:
+                                next_node.inputs[i] = prev_node.outputs[0]
+                                break
                 else:
                     if not node.outputs[0] in self.outputs:
                         raise Exception('Cannot rewire a node without child')
             else:
                 raise Exception('Cannot rewire a node without a parent')
-        
+
         del self.output_vars[node.outputs[0]]
         del self.graph[node.name]
         self._update_model_outputs()
 
     def replace_node(self, old_node, new_node):
-        """ Replace an existing node in the graph with a new one.
+        """Replace an existing node in the graph with a new one.
 
         Args:
             old_node (Layer): The node to replace
             new_node (Layer): The new node
 
         """
         prev_node = self.graph.get(old_node.inputs[0])
         next_node = next((x for x in self.graph.values() if x.inputs[0] == old_node.outputs[0]), None)
         if next_node is not None:
             next_node.inputs[0] = new_node.outputs[0]
         if prev_node is not None:
-            if new_node.inputs is None or len(new_node.inputs) == 0: # Check if already rewired
+            if new_node.inputs is None or len(new_node.inputs) == 0:  # Check if already rewired
                 new_node.inputs = [prev_node.outputs[0]]
 
         self.graph = OrderedDict((new_node.name, new_node) if k == old_node.name else (k, v) for k, v in self.graph.items())
         self._update_model_outputs()
 
     def _update_model_outputs(self):
-        ''' Update the model outputs
+        '''Update the model outputs
 
         All node outputs and inputs are found. The model outputs are set to all node outputs
         that are not also node inputs.
         '''
-        node_outputs = np.array([out for node in self.graph.values() for out in node.outputs])
-        node_inputs = np.array([inp for node in self.graph.values() for inp in node.inputs])
-        model_outputs = node_outputs[np.isin(node_outputs, node_inputs, invert=True)]
-        self.outputs = model_outputs.tolist()
+        node_outputs = [out for node in self.graph.values() for out in node.outputs]
+        node_inputs = [inp for node in self.graph.values() for inp in node.inputs]
+        self.outputs = [out for out in node_outputs if out not in node_inputs]
 
     def get_weights_data(self, layer_name, var_name):
         return self.reader.get_weights_data(layer_name, var_name)
 
     def next_layer(self):
         self.index += 1
         return self.index
@@ -487,175 +602,184 @@
 
     def get_input_variables(self):
         variables = []
         for inp in self.inputs:
             variables.append(self.graph[inp].get_output_variable())
         return variables
 
-    def register_bram_variable(self, out_name, variable):
-        self.bram_vars[out_name] = variable
-
-    def get_bram_variables(self):
-        return self.bram_vars.values()
-
     def register_output_variable(self, out_name, variable):
         if out_name in self.outputs:
             variable.type.name = 'result_t'
         self.output_vars[out_name] = variable
 
     def get_output_variables(self):
         variables = []
         for out in self.outputs:
             variables.append(self.output_vars[out])
         return variables
 
     def get_layer_output_variable(self, output_name):
         return self.output_vars.get(output_name, None)
 
+    def get_weight_variables(self):
+        variables = []
+        for layer in self.get_layers():
+            weights = layer.get_weights()
+            variables.extend(weights)
+
+        return variables
+
     def write(self):
-        def make_stamp():
-            from string import hexdigits
-            from random import choice
-            length = 8
-            return ''.join(choice(hexdigits) for m in range(length))
-        
-        self.config.config['Stamp'] = make_stamp()
+        """Write the generated project to disk.
 
-        self.config.writer.write_hls(self)
+        This function converts the model to C++ and writes the generated files in the output
+        directory specified in the `config`.
+        """
+
+        self.config.backend.write(self)
 
     def compile(self):
+        """Compile the generated project and link the library into current environment.
+
+        Users should call this function if they want to use `predict` functionality for simulation.
+        """
         self.write()
 
-        curr_dir = os.getcwd()
-        os.chdir(self.config.get_output_dir())
+        lib_name = self.config.backend.compile(self)
+        if self._top_function_lib is not None:
 
-        try:
-            ret_val = os.system('bash build_lib.sh')
-            if ret_val != 0:
-                raise Exception('Failed to compile project "{}"'.format(self.config.get_project_name()))
-            lib_name = 'firmware/{}-{}.so'.format(self.config.get_project_name(), self.config.get_config_value('Stamp'))
-            if self._top_function_lib is not None:
-
-                if platform.system() == "Linux":
-                    dlclose_func = ctypes.CDLL('libdl.so').dlclose
-                elif platform.system() == "Darwin":
-                    dlclose_func = ctypes.CDLL('libc.dylib').dlclose
-
-                dlclose_func.argtypes = [ctypes.c_void_p]
-                dlclose_func.restype = ctypes.c_int
-                dlclose_func(self._top_function_lib._handle)
-            self._top_function_lib = ctypes.cdll.LoadLibrary(lib_name)
-        finally:
-            os.chdir(curr_dir)
+            if platform.system() == "Linux":
+                libdl_libs = ['libdl.so', 'libdl.so.2']
+                for libdl in libdl_libs:
+                    try:
+                        dlclose_func = ctypes.CDLL(libdl).dlclose
+                        break
+                    except Exception:
+                        continue
+            elif platform.system() == "Darwin":
+                dlclose_func = ctypes.CDLL('libc.dylib').dlclose
+
+            dlclose_func.argtypes = [ctypes.c_void_p]
+            dlclose_func.restype = ctypes.c_int
+            dlclose_func(self._top_function_lib._handle)
+        self._top_function_lib = ctypes.cdll.LoadLibrary(lib_name)
 
     def _get_top_function(self, x):
         if self._top_function_lib is None:
             raise Exception('Model not compiled')
         if len(self.get_input_variables()) == 1:
             xlist = [x]
-        else: 
+        else:
             xlist = x
-        
+        n_outputs = len(self.get_output_variables())
+
         for xi in xlist:
             if not isinstance(xi, np.ndarray):
-                raise Exception('Expected numpy.ndarray, but got {}'.format(type(x)))
+                raise Exception(f'Expected numpy.ndarray, but got {type(x)}')
             if not xi.flags['C_CONTIGUOUS']:
                 raise Exception('Array must be c_contiguous, try using numpy.ascontiguousarray(x)')
 
         x0 = xlist[0]
         if x0.dtype in [np.single, np.float32]:
             top_function = getattr(self._top_function_lib, self.config.get_project_name() + '_float')
             ctype = ctypes.c_float
         elif x0.dtype in [np.double, np.float64, np.float_]:
             top_function = getattr(self._top_function_lib, self.config.get_project_name() + '_double')
             ctype = ctypes.c_double
         else:
-            raise Exception('Invalid type ({}) of numpy array. Supported types are: single, float32, double, float64, float_.'.format(x0.dtype))
-
+            raise Exception(
+                'Invalid type ({}) of numpy array. Supported types are: single, float32, double, float64, float_.'.format(
+                    x0.dtype
+                )
+            )
 
         top_function.restype = None
-        top_function.argtypes = [npc.ndpointer(ctype, flags="C_CONTIGUOUS") for i in range(len(xlist)+1)]
-        top_function.argtypes += [ctypes.POINTER(ctypes.c_ushort) for i in range(len(xlist)+1)]
+        top_function.argtypes = [npc.ndpointer(ctype, flags="C_CONTIGUOUS") for i in range(len(xlist) + n_outputs)]
 
         return top_function, ctype
 
     def _compute_n_samples(self, x):
         if len(self.get_input_variables()) == 1:
             xlist = [x]
         else:
             xlist = x
         n_samples = []
         for i, xi in enumerate(xlist):
             expected_size = self.get_input_variables()[i].size()
             x_size = np.prod(xi.shape)
             n_sample, rem = divmod(x_size, expected_size)
             if rem != 0:
-                raise Exception('Input size mismatch, got {}, expected {}'.format(x_size.shape, self.get_input_variables()[i].shape))
+                raise Exception(f'Input size mismatch, got {x_size.shape}, expected {self.get_input_variables()[i].shape}')
             n_samples.append(n_sample)
 
-        if not all([n_samples[i] == n_samples[i+1] for i in range(len(xlist)-1)]):
+        if not all([n_samples[i] == n_samples[i + 1] for i in range(len(xlist) - 1)]):
             raise Exception('Input size mismatch, not all inputs match')
 
-        return n_sample
+        return int(n_sample)
 
     def predict(self, x):
         top_function, ctype = self._get_top_function(x)
         n_samples = self._compute_n_samples(x)
         n_inputs = len(self.get_input_variables())
+        n_outputs = len(self.get_output_variables())
 
         curr_dir = os.getcwd()
         os.chdir(self.config.get_output_dir() + '/firmware')
 
         output = []
         if n_samples == 1 and n_inputs == 1:
             x = [x]
 
         try:
             for i in range(n_samples):
-                predictions = np.zeros(self.get_output_variables()[0].size(), dtype=ctype)
+                predictions = [np.zeros(yj.size(), dtype=ctype) for yj in self.get_output_variables()]
                 if n_inputs == 1:
-                    top_function(x[i], predictions, ctypes.byref(ctypes.c_ushort()), ctypes.byref(ctypes.c_ushort()))
+                    inp = [np.asarray(x[i])]
                 else:
-                    inp = [xj[i] for xj in x]
-                    argtuple = inp
-                    argtuple += [predictions]
-                    argtuple += [ctypes.byref(ctypes.c_ushort()) for k in range(len(inp)+1)]
-                    argtuple = tuple(argtuple)
-                    top_function(*argtuple)
+                    inp = [np.asarray(xj[i]) for xj in x]
+                argtuple = inp
+                argtuple += predictions
+                argtuple = tuple(argtuple)
+                top_function(*argtuple)
                 output.append(predictions)
 
-
-            #Convert to numpy array
-            output = np.asarray(output)
+            # Convert to list of numpy arrays (one for each output)
+            output = [
+                np.asarray([output[i_sample][i_output] for i_sample in range(n_samples)]) for i_output in range(n_outputs)
+            ]
         finally:
             os.chdir(curr_dir)
 
-        if n_samples == 1:
+        if n_samples == 1 and n_outputs == 1:
+            return output[0][0]
+        elif n_outputs == 1:
             return output[0]
+        elif n_samples == 1:
+            return [output_i[0] for output_i in output]
         else:
             return output
 
     def trace(self, x):
-        print('Recompiling {} with tracing'.format(self.config.get_project_name()))
+        print(f'Recompiling {self.config.get_project_name()} with tracing')
         self.config.trace_output = True
         self.compile()
 
         top_function, ctype = self._get_top_function(x)
         n_samples = self._compute_n_samples(x)
         n_inputs = len(self.get_input_variables())
+        n_outputs = len(self.get_output_variables())
 
         class TraceData(ctypes.Structure):
-            _fields_ = [('name', ctypes.c_char_p),
-                        ('data', ctypes.c_void_p)]
+            _fields_ = [('name', ctypes.c_char_p), ('data', ctypes.c_void_p)]
 
         trace_output = {}
         layer_sizes = {}
         n_traced = 0
         for layer in self.get_layers():
-            if layer.function_cpp() and layer.get_attr('Trace', False):
+            if layer.get_attr('function_cpp', None) and layer.get_attr('trace', False):
                 n_traced += len(layer.get_variables())
                 trace_output[layer.name] = []
                 layer_sizes[layer.name] = layer.get_output_variable().shape
 
         collect_func = self._top_function_lib.collect_trace_output
         collect_func.argtypes = [ctypes.POINTER(TraceData)]
         collect_func.restype = None
@@ -676,67 +800,55 @@
         if n_samples == 1 and n_inputs == 1:
             x = [x]
 
         try:
             alloc_func(ctypes.sizeof(ctype))
 
             for i in range(n_samples):
-                predictions = np.zeros(self.get_output_variables()[0].size(), dtype=ctype)
+                predictions = [np.zeros(yj.size(), dtype=ctype) for yj in self.get_output_variables()]
                 if n_inputs == 1:
-                    top_function(x[i], predictions, ctypes.byref(ctypes.c_ushort()), ctypes.byref(ctypes.c_ushort()))
+                    inp = [np.asarray(x[i])]
                 else:
-                    inp = [xj[i] for xj in x]
-                    argtuple = inp
-                    argtuple += [predictions]
-                    argtuple += [ctypes.byref(ctypes.c_ushort()) for k in range(len(inp)+1)]
-                    argtuple = tuple(argtuple)
-                    top_function(*argtuple)
+                    inp = [np.asarray(xj[i]) for xj in x]
+                argtuple = inp
+                argtuple += predictions
+                argtuple = tuple(argtuple)
+                top_function(*argtuple)
                 output.append(predictions)
                 collect_func(trace_data)
                 for trace in trace_data:
                     layer_name = str(trace.name, 'utf-8')
                     layer_data = ctypes.cast(trace.data, ctypes.POINTER(ctype))
                     np_array = np.ctypeslib.as_array(layer_data, shape=layer_sizes[layer_name])
                     trace_output[layer_name].append(np.copy(np_array))
 
             for key in trace_output.keys():
                 trace_output[key] = np.asarray(trace_output[key])
 
-            #Convert to numpy array
-            output = np.asarray(output)
+            # Convert to list of numpy arrays (one for each output)
+            output = [
+                np.asarray([output[i_sample][i_output] for i_sample in range(n_samples)]) for i_output in range(n_outputs)
+            ]
 
             free_func()
         finally:
             os.chdir(curr_dir)
 
-        if n_samples == 1:
+        if n_samples == 1 and n_outputs == 1:
+            return output[0][0], trace_output
+        elif n_outputs == 1:
             return output[0], trace_output
+        elif n_samples == 1:
+            return [output_i[0] for output_i in output], trace_output
         else:
             return output, trace_output
 
-    def build(self, reset=False, csim=True, synth=True, cosim=False, validation=False, export=False, vsynth=False):
-        if 'linux' in sys.platform:
-            backend = self.config.get_config_value('Backend', 'Vivado')
-            if backend in ['Vivado', 'VivadoAccelerator']:
-                found = os.system('command -v vivado_hls > /dev/null')
-                if found != 0:
-                    raise Exception('Vivado HLS installation not found. Make sure "vivado_hls" is on PATH.')
-
-            elif backend == 'Intel':
-                raise NotImplementedError
-            elif backend == 'Mentor':
-                raise NotImplementedError
-            else:
-                raise Exception('Backend values can be [Vivado, Intel, Mentor]')
+    def build(self, **kwargs):
+        """Builds the generated project using HLS compiler.
 
+        Please see the `build()` function of backends for a list of possible arguments.
+        """
         if not os.path.exists(self.config.get_output_dir()):
             # Assume the project wasn't written before
             self.write()
 
-        curr_dir = os.getcwd()
-        os.chdir(self.config.get_output_dir())
-        os.system('vivado_hls -f build_prj.tcl "reset={reset} csim={csim} synth={synth} cosim={cosim} validation={validation} export={export} vsynth={vsynth}"'
-            .format(reset=reset, csim=csim, synth=synth, cosim=cosim, validation=validation, export=export, vsynth=vsynth))
-        os.chdir(curr_dir)
-
-        return parse_vivado_report(self.config.get_output_dir())
-
+        return self.config.backend.build(self, **kwargs)
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/bn_fuse.py` & `hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/bn_fuse.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,31 +1,36 @@
+from hls4ml.model.layers import BatchNormalization, Conv1D, Conv2D, Dense
 from hls4ml.model.optimizer import OptimizerPass
 
+
 class FuseBatchNormalization(OptimizerPass):
     def match(self, node):
-        is_match = node.__class__.__name__ == 'BatchNormalization' and \
-            node.get_input_node().__class__.__name__ in ['Dense', 'Conv1D', 'Conv2D'] and \
-            node.get_input_node().get_attr('weight_quantizer') is None and \
-            node.get_input_node().get_attr('bias_quantizer') is None
+        is_match = (
+            isinstance(node, BatchNormalization)
+            and isinstance(node.get_input_node(), (Dense, Conv1D, Conv2D))
+            and node.get_input_node().get_attr('weight_quantizer') is None
+            and node.get_input_node().get_attr('bias_quantizer') is None
+        )
         return is_match
 
     def transform(self, model, node):
         # Fuse weight and bias of Dense/Conv1D/Conv2D layer with BN values
         parent_node = node.get_input_node()
+        parent_map = parent_node.get_output_use_map()
+        node_map = node.get_output_use_map()
+        if len(parent_map[parent_node.name]) > 1 or len(node_map[node.name]) > 1:
+            return False
 
         parent_weight = parent_node.weights['weight']
         parent_bias = parent_node.weights['bias']
 
         bn_scale = node.weights['scale']
         bn_bias = node.weights['bias']
 
-        if parent_node.get_attr('strategy') != 'resource':
-            fused_weight = bn_scale.data * parent_weight.data
-        else:
-            fused_weight = (bn_scale.data * parent_weight.data.T).T
+        fused_weight = bn_scale.data * parent_weight.data
         fused_bias = bn_scale.data * parent_bias.data + bn_bias.data
 
         model.remove_node(node, rewire=True)
         parent_weight.data = fused_weight
         parent_bias.data = fused_bias
         if not parent_node.get_attr('use_bias', True):
             parent_bias.update_precision(bn_bias.type.precision)
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/bn_quant.py` & `hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/bn_quant.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,176 +1,168 @@
 import numpy as np
-import re
 
+from hls4ml.backends.fpga.fpga_layers import BatchNormalizationQuantizedTanh
+from hls4ml.backends.template import FunctionCallTemplate, LayerConfigTemplate
+from hls4ml.model.layers import BatchNormalization, register_layer
 from hls4ml.model.optimizer import OptimizerPass
-from hls4ml.model.hls_model import Layer, IntegerPrecisionType, XnorPrecisionType, register_layer
-from hls4ml.model.hls_layers import BatchNormalization
-from hls4ml.templates import templates
-
-class BatchNormalizationQuantizedTanh(Layer):
-    ''' Merged Batch Normalization and quantized (binary or ternary) Tanh layer.
-        The mean, variance, beta, gamma parameters are folded into the threshold(s) at which the
-        sign of the input flips after the quantized (binary or ternary) Tanh activation.
-    '''
-
-    def initialize(self):
-        inp = self.get_input_variable()
-        shape = inp.shape
-        dims = inp.dim_names
-        if self.get_attr('quantize') == 2:
-            self.add_output_variable(shape, dims, precision=XnorPrecisionType())
-        elif self.get_attr('quantize') == 3:
-            self.add_output_variable(shape, dims, precision=IntegerPrecisionType(width=2))
-        else:
-            raise Exception('Unsupported quantize attribute for BatchNormalizationQuantizedTanh: {}'.format(self.get_attr('quantize')))
-
-    def function_cpp(self):
-        params = self._default_function_params()
-        if self.get_attr('quantize') == 2:
-            params['quantize'] = 'binary'
-            params['threshold'] = self.get_weights('threshold').name
-        elif self.get_attr('quantize') == 3:
-            params['quantize'] = 'ternary'
-            params['threshold'] = self.get_weights('threshold_hi').name + ', ' + self.get_weights('threshold_lo').name
-
-        return [self._function_template.format(**params)]
-
-    def config_cpp(self):
-        params = self._default_config_params()
-        params['n_in'] = self.get_input_variable().size_cpp()
-
-        return self._config_template.format(**params)
-
-    def set_thresholds(self, scale, bias, ternary_threshold=0.5):
-        inp = self.get_input_variable()
-        shape = inp.shape
-        dims = inp.dim_names
-        precision = self.model.config.backend.convert_precision_string(inp.type.precision)
-        W, I, F = precision.width, precision.integer, precision.fractional
-        threshold = - bias / scale
-        if self.get_attr('quantize') == 2:
-            self.add_output_variable(shape, dims, precision=XnorPrecisionType())
-            threshold = np.floor(threshold * 2**F) / 2**F
-            self.add_weights_variable(name='threshold', var_name='t{index}', data=threshold, type_name='threshold{index}_t', precision=inp.type.precision)
-        elif self.get_attr('quantize') == 3:
-            self.add_output_variable(shape, dims, precision=IntegerPrecisionType(width=2))
-            threshold_hi = ternary_threshold / scale + threshold
-            threshold_lo = -ternary_threshold / scale + threshold
-            threshold_hi = np.floor(threshold_hi * 2**F) / 2**F
-            threshold_lo = np.floor(threshold_lo * 2**F) / 2**F
-            self.add_weights_variable(name='threshold_hi', var_name='th{index}', data=threshold_hi, type_name='threshold_hi_{index}_t', precision=inp.type.precision)
-            self.add_weights_variable(name='threshold_lo', var_name='tl{index}', data=threshold_lo, type_name='threshold_lo_{index}_t', precision=inp.type.precision)
+from hls4ml.model.types import IntegerPrecisionType, NamedType, XnorPrecisionType
 
 batchnorm_quantized_tanh_config_template = """struct config{index} : nnet::batchnorm_quantized_tanh_config {{
     static const unsigned n_in = {n_in};
     static const unsigned n_filt = {n_filt};
     static const unsigned io_type = nnet::{iotype};
     static const unsigned reuse_factor = {reuse};
 }};\n"""
 
-batchnorm_quantized_tanh_function_template = 'nnet::normalize_{quantize}_tanh<{input_t}, {config}>({input}, {output}, {threshold});'
+batchnorm_quantized_tanh_function_template = (
+    'nnet::normalize_{quantize}_tanh<{input_t}, {config}>({input}, {output}, {threshold});'
+)
+
+bn_include_list = ['nnet_utils/nnet_batchnorm.h', 'nnet_utils/nnet_batchnorm_stream.h']
+
+
+class BatchNormalizationQuantizedTanhConfigTemplate(LayerConfigTemplate):
+    def __init__(self):
+        super().__init__(BatchNormalizationQuantizedTanh)
+        self.template = batchnorm_quantized_tanh_config_template
+
+    def format(self, node):
+        params = self._default_config_params(node)
+        params['n_in'] = node.get_input_variable().size_cpp()
+
+        return self.template.format(**params)
 
-# Register the layer types to the layer map
-register_layer('BatchNormalizationQuantizedTanh', BatchNormalizationQuantizedTanh)
 
-from hls4ml.templates.vivado_template import batchnorm_include_list
+class BatchNormalizationQuantizedTanhFunctionTemplate(FunctionCallTemplate):
+    def __init__(self):
+        super().__init__(BatchNormalizationQuantizedTanh, include_header=bn_include_list)
+        self.template = batchnorm_quantized_tanh_function_template
+
+    def format(self, node):
+        params = self._default_function_params(node)
+        if node.get_attr('quantize') == 2:
+            params['quantize'] = 'binary'
+            params['threshold'] = node.get_weights('threshold').name
+        elif node.get_attr('quantize') == 3:
+            params['quantize'] = 'ternary'
+            params['threshold'] = node.get_weights('threshold_hi').name + ', ' + node.get_weights('threshold_lo').name
+
+        return self.template.format(**params)
+
+
+def register_bn_quant(backend):
+    # Register the layer types to the layer map
+    register_layer('BatchNormalizationQuantizedTanh', BatchNormalizationQuantizedTanh)
+
+    # Register the optimization passes
+    backend.register_pass('merge_batch_norm_quantized_tanh', MergeBatchNormAndQuantizedTanh)
+    backend.register_pass('quantize_dense_output', QuantizeDenseOutput)
+
+    # Register template passes
+    backend.register_template(BatchNormalizationQuantizedTanhConfigTemplate)
+    backend.register_template(BatchNormalizationQuantizedTanhFunctionTemplate)
 
-# Register the templates for config and function
-for backend in ['Vivado', 'VivadoAccelerator']:
-    templates.get_backend(backend).register_templates('BatchNormalizationQuantizedTanh', batchnorm_quantized_tanh_function_template, batchnorm_quantized_tanh_config_template, batchnorm_include_list)
 
 class MergeBatchNormAndQuantizedTanh(OptimizerPass):
     def match(self, node):
-        is_match = (node.__class__.__name__ == 'Activation'
+        is_match = (
+            node.class_name == 'Activation'
             and node.get_attr('activation') in ['binary', 'binary_tanh', 'ternary', 'ternary_tanh']
-            or node.__class__.__name__ == 'TernaryTanh')
+            or node.class_name == 'TernaryTanh'
+        )
         is_match = is_match and isinstance(node.get_input_node(), BatchNormalization)
         return is_match
 
     def transform(self, model, node):
         bn_layer = node.get_input_node()
         # Make a new layer with the new attributes
         quantize = 0
         if 'binary' in node.get_attr('activation'):
             quantize = 2
         if 'ternary' in node.get_attr('activation'):
             quantize = 3
         attrs = {
-            'name' : bn_layer.get_attr('name'),
-            'original_name' : bn_layer.get_attr('name'),
-            'class_name' : 'BatchNormalizationQuantizedTanh',
-            'n_in' : bn_layer.get_attr('n_in'),
-            'n_out' : bn_layer.get_attr('n_in'),
-            'n_filt' : bn_layer.get_attr('n_filt'),
-            'quantize' : quantize,
-            'Trace' : bn_layer.get_attr('Trace')
+            'name': bn_layer.get_attr('name'),
+            'original_name': bn_layer.get_attr('name'),
+            'class_name': 'BatchNormalizationQuantizedTanh',
+            'n_in': bn_layer.get_attr('n_in'),
+            'n_out': bn_layer.get_attr('n_in'),
+            'n_filt': bn_layer.get_attr('n_filt'),
+            'quantize': quantize,
+            'trace': bn_layer.get_attr('trace'),
         }
-        bnbt_layer = model.make_node('BatchNormalizationQuantizedTanh', 'bnbt_' + bn_layer.name, attrs, bn_layer.inputs)
-        bnbt_layer.set_thresholds(bn_layer.get_weights('scale').data, bn_layer.get_weights('bias').data, node.get_attr('threshold',0.5))
+        bnbt_layer = model.make_node(BatchNormalizationQuantizedTanh, 'bnbt_' + bn_layer.name, attrs, bn_layer.inputs)
+        bnbt_layer.set_thresholds(
+            bn_layer.get_weights('scale').data, bn_layer.get_weights('bias').data, node.get_attr('threshold', 0.5)
+        )
         # Remove the BatchNormalization layer
         model.remove_node(bn_layer, rewire=True)
         # Replace the old Activation layer with this one
         model.replace_node(node, bnbt_layer)
 
         return True
 
+
 class QuantizeDenseOutput(OptimizerPass):
     def match(self, node):
-        is_match = node.__class__.__name__ == 'Dense'
-        is_match = is_match and node.get_input_node().__class__.__name__ == 'BatchNormalizationQuantizedTanh'
+        is_dense = node.class_name == 'Dense'
+        input_node = node.get_input_node()
+        is_input_bnqt = input_node is not None and input_node.class_name == 'BatchNormalizationQuantizedTanh'
         quantizer = node.get_attr('weight_quantizer')
-        is_match = is_match and (quantizer.__class__.__name__ == 'BinaryQuantizer' or quantizer.__class__.__name__ == 'TernaryQuantizer')
-        return is_match
+        is_binary_ternary = quantizer is not None and (
+            quantizer.__class__.__name__ == 'BinaryQuantizer' or quantizer.__class__.__name__ == 'TernaryQuantizer'
+        )
+        return is_dense and is_input_bnqt and is_binary_ternary
 
     def transform(self, model, node):
         # Compute the required precision and update the variables
         # Number of bits for output is log2 of number of input nodes
         # Since this is the number of uint<1>'s which are summed
         nbits = int(np.ceil(np.log2(node.attributes['n_in'])) + 2)
         out_type = IntegerPrecisionType(width=nbits)
-        node.set_attr('accum_t', out_type)
+        accum_t = NamedType(f'layer{node.index}_accum_t', out_type)
+        node.set_attr('accum_t', accum_t)
         out_var = node.get_output_variable()
         out_var.type.precision = out_type
 
         quantized_data = None
         quantized_precision = None
         quantizer = node.get_attr('weight_quantizer')
         if quantizer.__class__.__name__ == 'BinaryQuantizer':
             quantized_precision = XnorPrecisionType()
         elif quantizer.__class__.__name__ == 'TernaryQuantizer':
             quantized_precision = IntegerPrecisionType(width=2)
         else:
-            print('WARNING: Unknown quantizer - {}. Bailing out'.format(quantizer.__class__.__name__))
+            print(f'WARNING: Unknown quantizer - {quantizer.__class__.__name__}. Bailing out')
             return False
         quantizer.bits = quantized_precision.width
         quantizer.hls_type = quantized_precision
         quantized_data = quantizer(node.weights['weight'].data)
 
         weights = node.weights['weight']
         weights.data = quantized_data
-        weights.type.name = 'weight{index}_t'.format(index=node.index)
+        weights.type.name = f'weight{node.index}_t'
         weights.update_precision(quantized_precision)
 
         bias = node.weights['bias']
         bias.data = np.zeros(shape=(node.get_attr('n_out')))
-        bias.type.name = 'bias{index}_t'.format(index=node.index)
+        bias.type.name = f'bias{node.index}_t'
         bias.nzeros = 0
         bias.update_precision(quantized_precision)
 
         # If followed by the BatchNormalizationBinaryTanh, update its input
         # Also requantise the weights
         bd_out_nodes = node.get_output_nodes()
         for out_node in bd_out_nodes:
-            if out_node.__class__.__name__ == 'BatchNormalizationQuantizedTanh':
+            if isinstance(out_node, BatchNormalizationQuantizedTanh):
                 var_names = []
                 if quantizer.__class__.__name__ == 'BinaryQuantizer':
                     var_names.append('threshold')
                 elif quantizer.__class__.__name__ == 'TernaryQuantizer':
                     var_names.append('threshold_hi')
                     var_names.append('threshold_lo')
                 for var_name in var_names:
                     threshold_var = out_node.weights[var_name]
                     threshold_var.update_precision(out_type)
                     threshold_var.data = np.floor(threshold_var.data)
 
         return False
-
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/clone.py` & `hls4ml-0.7.0rc1/hls4ml/backends/fpga/passes/clone.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,76 +1,92 @@
 import numpy as np
 
+from hls4ml.backends.template import FunctionCallTemplate
+from hls4ml.model.layers import Layer, register_layer
 from hls4ml.model.optimizer import OptimizerPass
 
-from hls4ml.model.hls_model import Layer, register_layer
-from hls4ml.templates import templates
 
 class Clone(Layer):
-    ''' Inserted after the layer whose output is used more than once.'''
+    '''Inserted after the layer whose output is used more than once.'''
 
     def initialize(self):
         inp = self.get_input_variable()
-        self.add_output_variable(inp.shape, inp.dim_names, out_name=self.outputs[0], var_name='layer{index}_cpy1')
-        self.add_output_variable(inp.shape, inp.dim_names, out_name=self.outputs[1], var_name='layer{index}_cpy2')
+        for i, out_name in enumerate(self.outputs):
+            self.add_output_variable(inp.shape, inp.dim_names, out_name=out_name, var_name='layer{index}_cpy' + str(i + 1))
 
-    def function_cpp(self):
-        params = self._default_function_params()
-        params['size'] = self.get_attr('size')
-        params['output1'] = self.variables[self.outputs[0]].name
-        params['output2'] = self.variables[self.outputs[1]].name
-        return [self._function_template.format(**params)]
 
-    def config_cpp(self):
-        return None
-
-clone_function_template = 'nnet::clone_stream<{input_t}, {output_t}, {size}>({input}, {output1}, {output2});'
 clone_include_list = ['nnet_utils/nnet_stream.h']
 
-# Register the layer types to the layer map
-register_layer('Clone', Clone)
 
-# Register the templates for config and function
-for backend in ['Vivado', 'VivadoAccelerator']:
-    templates.get_backend(backend).register_templates('Clone', clone_function_template, None, clone_include_list)
+class CloneFunctionTemplate(FunctionCallTemplate):
+    def __init__(self):
+        super().__init__(Clone, include_header=clone_include_list)
+        self.template = None  # to be filled once number of clones known
+
+    def format(self, node):
+        params = self._default_function_params(node)
+        for i, _output in enumerate(node.outputs):
+            params['output' + str(i + 1)] = node.variables[node.outputs[i]].name
+
+        if self.template is None:
+            self.template = (
+                'nnet::clone_stream<{input_t}, {output_t}, {size}>({input}, '
+                + ', '.join(['{output' + str(i + 1) + '}' for i in range(len(node.outputs))])
+                + ');'
+            )
+
+        return self.template.format(**params)
+
+
+def register_clone(backend):
+    # Register the layer types to the layer map
+    register_layer('Clone', Clone)
+
+    # Register the optimization passes
+    backend.register_pass('clone_output', CloneOutput)
+
+    # Register template passes
+    backend.register_template(CloneFunctionTemplate)
 
 
 class CloneOutput(OptimizerPass):
-    ''' Clones streams that are used multiple times '''
+    '''Clones streams that are used multiple times'''
+
     def match(self, node):
         # We may have already inserted the Clone layer
-        if node.__class__.__name__ == 'Clone':
+        if isinstance(node, Clone):
             return False
 
         return True
 
     def transform(self, model, node):
-        if model.config.backend.name not in ['Vivado', 'VivadoAccelerator'] or \
-            model.config.get_config_value('IOType') != 'io_stream':
+        if model.config.get_config_value('IOType') != 'io_stream':
             return False
 
-        output_map = {}
-        for output in node.outputs:
-            output_map[output] = []
-            for layer in model.get_layers():
-                for inp in layer.inputs:
-                    if output == inp:
-                        output_map[output].append(layer)
+        output_map = node.get_output_use_map()
 
         transformed = False
         for output in node.outputs:
             if len(output_map[output]) > 1:
-                if len(output_map[output]) > 2:
-                    print('WARN: Cannot clone output {} of {} ({})'.format(output, node.__class__.__name__, node.name))
+                if len(output_map[output]) > 3:
+                    print(
+                        'WARNING: Cloning output {} of {} ({}) more than 3 times not currently supported'.format(
+                            output, node.__class__.__name__, node.name
+                        )
+                    )
                     return False
                 out_var = node.get_output_variable(output)
                 for i, layer in enumerate(output_map[output], 1):
-                    attrs = {
-                        'size' : np.prod(out_var.shape)
-                    }
+                    attrs = {'size': np.prod(out_var.shape)}
                     idx = layer.inputs.index(output)
                     layer.inputs[idx] = output + '_cpy' + str(i)
-                clone_layer = model.make_node('Clone', 'clone_' + node.name, attrs, [output], [output + '_cpy1', output + '_cpy2'])
+                clone_layer = model.make_node(
+                    Clone,
+                    'clone_' + node.name,
+                    attrs,
+                    [output],
+                    [output + '_cpy' + str(i + 1) for i in range(len(output_map[output]))],
+                )
                 model.insert_node(clone_layer)
                 transformed = True
-        
+
         return transformed
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/conv_same_pad.py` & `hls4ml-0.7.0rc1/hls4ml/backends/vivado/passes/conv_same_pad.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,34 +1,43 @@
+from hls4ml.model.layers import Conv1D, Conv2D, SeparableConv1D, SeparableConv2D
 from hls4ml.model.optimizer import OptimizerPass
 
+
 class InsertZeroPaddingBeforeConv1D(OptimizerPass):
+    name = 'insert_zero_padding_before_conv1d'
+
     def match(self, node):
-        is_match = 'Conv1D' in node.__class__.__name__ and \
-            node.get_attr('padding') == 'same' and \
-            node.get_attr('filt_width') != 1
+        is_match = (
+            isinstance(node, (Conv1D, SeparableConv1D))
+            and ((node.get_attr('padding') == 'same') or (node.get_attr('padding') == 'causal'))
+            and node.get_attr('filt_width') != 1
+        )
         return is_match
 
     def transform(self, model, node):
-        if model.config.backend.name not in ['Vivado', 'VivadoAccelerator'] or \
-            model.config.get_config_value('IOType') != 'io_stream':
+        if model.config.get_config_value('IOType') != 'io_stream':
             return False
-        
+
         # Get the padding parameters from Conv1D layer
         pad_left = node.get_attr('pad_left')
         pad_right = node.get_attr('pad_right')
 
+        # Check if no padding needs to be done
+        if pad_left == pad_right == 0:
+            return False
+
         out_width = pad_left + node.get_attr('in_width') + pad_right
 
         attrs = {
             'pad_left': pad_left,
             'pad_right': pad_right,
             'in_width': node.get_attr('in_width'),
             'out_width': out_width,
             'n_chan': node.get_attr('n_chan'),
-            'data_format': node.get_attr('data_format', 'channels_last')
+            'data_format': node.get_attr('data_format', 'channels_last'),
         }
 
         # Switch Conv1D layer padding to 'valid'
         node.set_attr('padding', 'valid')
         node.set_attr('pad_left', 0)
         node.set_attr('pad_right', 0)
         node.set_attr('in_width', out_width)
@@ -36,46 +45,55 @@
         # Insert new ZeroPadding1D node above Conv1D
         padding_layer = model.make_node('ZeroPadding1D', 'zp1d_' + node.name, attrs, node.inputs.copy())
         padding_layer.get_output_variable().type.precision = node.get_input_variable().type.precision
         model.insert_node(padding_layer)
 
         return True
 
+
 class InsertZeroPaddingBeforeConv2D(OptimizerPass):
+    name = 'insert_zero_padding_before_conv2d'
+
     def match(self, node):
-        is_match = 'Conv2D' in node.__class__.__name__ and \
-            node.get_attr('padding') == 'same' and \
-            node.get_attr('filt_height') != 1 and node.get_attr('filt_width') != 1
+        is_match = (
+            isinstance(node, (Conv2D, SeparableConv2D))
+            and node.get_attr('padding') == 'same'
+            and node.get_attr('filt_height') != 1
+            and node.get_attr('filt_width') != 1
+        )
         return is_match
 
     def transform(self, model, node):
-        if model.config.backend.name not in ['Vivado', 'VivadoAccelerator'] or \
-            model.config.get_config_value('IOType') != 'io_stream':
+        if model.config.get_config_value('IOType') != 'io_stream':
             return False
-        
+
         # Get the padding parameters from Conv2D layer
         pad_top = node.get_attr('pad_top')
         pad_bottom = node.get_attr('pad_bottom')
         pad_left = node.get_attr('pad_left')
         pad_right = node.get_attr('pad_right')
 
+        # Check if no padding neeeds to be done
+        if pad_top == pad_bottom == pad_left == pad_right == 0:
+            return False
+
         out_height = pad_top + node.get_attr('in_height') + pad_bottom
         out_width = pad_left + node.get_attr('in_width') + pad_right
 
         attrs = {
             'pad_top': pad_top,
             'pad_bottom': pad_bottom,
             'pad_left': pad_left,
             'pad_right': pad_right,
             'in_height': node.get_attr('in_height'),
             'in_width': node.get_attr('in_width'),
             'out_height': out_height,
             'out_width': out_width,
             'n_chan': node.get_attr('n_chan'),
-            'data_format': node.get_attr('data_format', 'channels_last')
+            'data_format': node.get_attr('data_format', 'channels_last'),
         }
 
         # Switch Conv2D layer padding to 'valid'
         node.set_attr('padding', 'valid')
         node.set_attr('pad_top', 0)
         node.set_attr('pad_bottom', 0)
         node.set_attr('pad_left', 0)
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/fuse_biasadd.py` & `hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/fuse_biasadd.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,19 +1,16 @@
+from hls4ml.model.layers import BiasAdd, Conv1D, Conv2D, Dense
 from hls4ml.model.optimizer import OptimizerPass
 
-from hls4ml.model import hls_model
 
 class FuseBiasAdd(OptimizerPass):
-    ''' Fuses BiasAdd into Dense/Conv2D layer (common in TF models). '''
+    '''Fuses BiasAdd into Dense/Conv2D layer (common in TF models).'''
+
     def match(self, node):
-        is_match = node.__class__.__name__ == 'BiasAdd' and \
-            (node.get_input_node().__class__.__name__ == 'Dense' or
-            node.get_input_node().__class__.__name__ == 'Conv2D' or
-            node.get_input_node().__class__.__name__ == 'Conv1D')
-        return is_match
+        return isinstance(node, BiasAdd) and isinstance(node.get_input_node(), (Dense, Conv1D, Conv2D))
 
     def transform(self, model, node):
         # Fuse BiasAdd into Dense layer
         dense_layer = node.get_input_node()
         dense_layer.get_weights('bias').data = node.get_weights('bias').data
 
         model.remove_node(node, rewire=True)
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/multi_dense.py` & `hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/multi_dense.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,55 +1,66 @@
-from hls4ml.model.optimizer import OptimizerPass
-from hls4ml.model.hls_model import Dense
 import numpy as np
 
+from hls4ml.model.layers import Dense
+from hls4ml.model.optimizer import OptimizerPass
+
+
 class ReplaceMultidimensionalDenseWithConv(OptimizerPass):
     def match(self, node):
-        return node.__class__.__name__ == 'Dense' and \
-            len(node.get_input_variable().shape) > 1
+        return (
+            isinstance(node, Dense)
+            and len(node.get_input_variable().shape) - sum(d == 1 for d in node.get_input_variable().shape) > 1
+        )
+        # The above sum checks for the number of dimensions in the Dense with size 1
+        # The subtraction allows the check to only count the number of dimensions with non-1 size
+        # For example, this prevents matching for a Dense layer with shape (1,N)
 
     def transform(self, model, node):
-        dim = len(node.get_input_variable().shape) - 1        
+        dim = len(node.get_input_variable().shape) - 1
         input_shape = node.get_input_variable().shape
 
         pointwise_attrs = {
             'data_format': 'channels_last',
             'padding': 'valid',
             'n_chan': input_shape[-1],
             'n_filt': node.get_attr('n_out'),
         }
 
         if dim == 1:
-            pointwise_attrs.update({
-                'in_width': input_shape[0],
-                'out_width': input_shape[0],
-                'filt_width': 1,
-                'stride_width': 1,
-                'pad_left': 0,
-                'pad_right': 0,
-            })
+            pointwise_attrs.update(
+                {
+                    'in_width': input_shape[0],
+                    'out_width': input_shape[0],
+                    'filt_width': 1,
+                    'stride_width': 1,
+                    'pad_left': 0,
+                    'pad_right': 0,
+                }
+            )
         elif dim == 2:
-            pointwise_attrs.update({
-                'in_height': input_shape[0],
-                'in_width': input_shape[1],
-                'out_height': input_shape[0],
-                'out_width': input_shape[1],
-                'filt_height': 1,
-                'filt_width': 1,
-                'stride_height': 1,
-                'stride_width': 1,
-                'pad_top': 0,
-                'pad_bottom': 0,
-                'pad_left': 0,
-                'pad_right': 0,
-            })
+            pointwise_attrs.update(
+                {
+                    'in_height': input_shape[0],
+                    'in_width': input_shape[1],
+                    'out_height': input_shape[0],
+                    'out_width': input_shape[1],
+                    'filt_height': 1,
+                    'filt_width': 1,
+                    'stride_height': 1,
+                    'stride_width': 1,
+                    'pad_top': 0,
+                    'pad_bottom': 0,
+                    'pad_left': 0,
+                    'pad_right': 0,
+                }
+            )
         else:
             raise Exception('Cannot replace Dense over {dim}D tensor with Conv{dim}D.'.format(dim=dim))
 
         class_name = 'PointwiseConv' + str(dim) + 'D'
         pw_node = model.make_node(class_name, node.name, pointwise_attrs, node.inputs.copy())
-        if len(node.weights['weight'].data.shape) == 2: # This can happen if we assign weights of Dense layer to 1x1 Conv2D
-            pw_node.weights['weight'].data = np.expand_dims(node.weights['weight'].data, axis=(0,1))
+        if len(node.weights['weight'].data.shape) == 2:  # This can happen if we assign weights of Dense layer to 1x1 Conv2D
+            pw_node.weights['weight'].data = np.expand_dims(node.weights['weight'].data, axis=tuple(range(dim)))
         pw_node.weights['bias'].data = node.weights['bias'].data
         model.replace_node(node, pw_node)
-        
+
         return True
```

### Comparing `hls4ml-0.6.0/hls4ml/model/optimizer/passes/qkeras.py` & `hls4ml-0.7.0rc1/hls4ml/model/optimizer/passes/qkeras.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,71 +1,69 @@
-from hls4ml.model.optimizer import OptimizerPass
-from hls4ml.model.hls_layers import BatchNormalization
-from hls4ml.model.hls_model import IntegerPrecisionType, FixedPrecisionType, ExponentPrecisionType, register_layer
-from hls4ml.templates import templates
-import tensorflow as tf
 import numpy as np
-from qkeras import get_quantizer
+import tensorflow as tf
+
+from hls4ml.model.layers import BatchNormalization, register_layer
+from hls4ml.model.optimizer import ConfigurableOptimizerPass, OptimizerPass, register_pass
+from hls4ml.model.types import FixedPrecisionType, IntegerPrecisionType, NamedType, QKerasPO2Quantizer
 
-class QKerasPO2Quantizer(object):
-    def __init__(self, config):
-        self.bits = config['config']['bits']
-        self.quantizer_fn = get_quantizer(config)
-        self.hls_type = ExponentPrecisionType(width=self.bits, signed=True)
-
-    def __call__(self, data):
-        '''
-        Weights are rounded to nearest power of 2
-        '''
-        x = tf.convert_to_tensor(data)
-        y = self.quantizer_fn(x)
-        if hasattr(y, 'numpy'):
-            y = y.numpy()
-        return y
 
-class OutputRoundingSaturationMode(OptimizerPass):
+class OutputRoundingSaturationMode(ConfigurableOptimizerPass):
     '''
     Set the Rounding and Saturation mode of the output (and accumulator, if applicable)
     of the layers specific in layer list.
     The layer list is empty by default.
     To specify which layer to apply this pass to, perform e.g.:
-    hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Dense', 'Activation', 'BatchNormalization']
+    hls4ml.model.optimizer.get_optimizer('output_rounding_saturation_mode').configure(layers=['Dense', 'Activation'])
     The Rounding and Saturation modes are 'None' by default (so use the compiler defaults)
     To set which mode to use:
-    hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND_CONV'
-    hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'
+    hls4ml.model.optimizer.get_optimizer('output_rounding_saturation_mode').configure(rounding_mode='AP_RND_CONV')
+    hls4ml.model.optimizer.get_optimizer('output_rounding_saturation_mode').configure(saturation_mode='AP_SAT')
     '''
 
-    layers = [] 
-    rounding_mode = None 
-    saturation_mode = None 
-    saturation_bits = None
+    def __init__(self):
+        self.layers = []
+        self.rounding_mode = None
+        self.saturation_mode = None
+        self.saturation_bits = None
 
     def match(self, node):
-        layer_match = node.__class__.__name__ in self.layers or node.name in self.layers
+        layer_match = node.class_name in self.layers or node.name in self.layers
         t = str(node.get_output_variable().type.precision)
         # check that the type doesn't already contain the rounding mode
         rs_match = False
         if self.rounding_mode is not None:
             rs_match = rs_match or not (self.rounding_mode in t)
         if self.saturation_mode is not None:
             rs_match = rs_match or not (self.saturation_mode in t)
         return layer_match and rs_match
 
     def transform(self, model, node):
-        oldtype = node.get_output_variable().type.precision
-        if isinstance(oldtype, IntegerPrecisionType):
-            newtype = IntegerPrecisionType(oldtype.width, oldtype.signed)
-        elif isinstance(oldtype, FixedPrecisionType):
-            newtype = FixedPrecisionType(oldtype.width, oldtype.integer, oldtype.signed, self.rounding_mode, self.saturation_mode, self.saturation_bits)
-        else: # in case the precision is a string
-            newtype = self.precision_string_modify(oldtype)
-        node.get_output_variable().type.precision = newtype
+        old_precision = node.get_output_variable().type.precision
+        if isinstance(old_precision, IntegerPrecisionType):
+            new_precision = IntegerPrecisionType(old_precision.width, old_precision.signed)
+        elif isinstance(old_precision, FixedPrecisionType):
+            new_precision = FixedPrecisionType(
+                old_precision.width,
+                old_precision.integer,
+                old_precision.signed,
+                self.rounding_mode,
+                self.saturation_mode,
+                self.saturation_bits,
+            )
+        else:  # in case the precision is a string
+            new_precision = self.precision_string_modify(old_precision)
+
+        out_var = node.get_output_variable()
+        out_t = NamedType(out_var.type.name, new_precision)
+        out_var.type = out_t
+        node.attributes['result_t'] = out_t
+
         if node.get_attr('accum_t') is not None:
-            node.set_attr('accum_t', newtype)
+            accum_t = NamedType(f'layer{node.index}_accum_t', new_precision)
+            node.set_attr('accum_t', accum_t)
         return False
 
     def precision_string_modify(self, pstr):
         # For when the type is a string not an Type
         mode = ''
         if self.rounding_mode is not None:
             mode += ',' + self.rounding_mode
@@ -73,45 +71,60 @@
             mode += ',' + self.saturation_mode
         if self.saturation_bits is not None:
             mode += ',' + str(self.saturation_bits)
         mode += '>'
         pstr = pstr.replace('>', mode)
         return pstr
 
+
 class ApplyAlpha(BatchNormalization):
-    ''' A custom layer to scale the output of a QDense layer which used 'alpha != 1'
-        Inference computation uses BatchNormalization methods'''
+    '''A custom layer to scale the output of a QDense layer which used 'alpha != 1'
+    Inference computation uses BatchNormalization methods'''
 
     def initialize(self):
         inp = self.get_input_variable()
         shape = inp.shape
         dims = inp.dim_names
         self.add_output_variable(shape, dims)
 
+        scale = self.get_attr('scale_data')
+        scale_quantizer = self.get_attr('scale_quantizer')
+        bias = self.get_attr('bias_data')
+        bias_quantizer = self.get_attr('bias_quantizer')
+
+        self.add_weights(scale, quantizer=scale_quantizer)
+        self.add_bias(bias, quantizer=bias_quantizer)
+
     def add_weights(self, scale, quantizer=None):
         self.add_weights_variable(name='scale', var_name='s{index}', data=scale, quantizer=quantizer)
 
     def add_bias(self, bias, quantizer=None):
         self.add_weights_variable(name='bias', var_name='b{index}', data=bias, quantizer=quantizer)
 
-# register the layer and its templates
-register_layer('ApplyAlpha', ApplyAlpha)
-# TODO ideally: for backend in backends
-for backend in ['Vivado', 'VivadoAccelerator']:
-    temps = templates.get_backend(backend)
-    temps.register_templates('ApplyAlpha', temps.get_function_template('BatchNormalization'), temps.get_config_template('BatchNormalization'), temps.get_include_list('BatchNormalization'))
+
+def register_qkeras():
+    # Register the layer types to the layer map
+    register_layer('ApplyAlpha', ApplyAlpha)
+
+    # Register the optimization passes
+    register_pass('output_rounding_saturation_mode', OutputRoundingSaturationMode)
+    register_pass('qkeras_factorize_alpha', QKerasFactorizeAlpha)
+    register_pass('extract_ternary_threshold', ExtractTernaryThreshold)
+    register_pass('fuse_consecutive_batch_normalization', FuseConsecutiveBatchNormalization)
+
 
 class QKerasFactorizeAlpha(OptimizerPass):
     '''OptimizerPass for extracting alpha "scale" from QKeras quantized layer.
-       The weights of the Q{Dense, Conv} layer are scaled to the common data type,
-       and an 'ApplyAlpha' layer is inserted to reapply the scale.
+    The weights of the Q{Dense, Conv} layer are scaled to the common data type,
+    and an 'ApplyAlpha' layer is inserted to reapply the scale.
     '''
+
     def match(self, node):
-        q_layer = node.__class__.__name__ in ["Dense", "Conv1D", "Conv2D", "Conv2DBatchnorm"]
-        has_w_quant = node.get_attr('weight_quantizer') is not None 
+        q_layer = node.class_name in ['Dense', 'Conv1D', 'Conv2D', 'Conv2DBatchnorm']
+        has_w_quant = node.get_attr('weight_quantizer') is not None
         has_b_quant = node.get_attr('bias_quantizer') is not None
         has_w_alpha, has_b_alpha = False, False
         if has_w_quant:
             if hasattr(node.get_attr('weight_quantizer'), 'alpha'):
                 w_alpha = node.get_attr('weight_quantizer').alpha
                 has_w_alpha = w_alpha != 1 and w_alpha is not None
             else:
@@ -124,95 +137,98 @@
                 has_b_alpha = False
         is_match = q_layer and ((has_w_quant and has_w_alpha) or (has_b_quant and has_b_alpha))
         return is_match
 
     def transform(self, model, node):
         # The quantizer has to be applied to set the scale attribute
         # This must be applied to the _unquantized_ weights to obtain the correct scale
-        quantizer = node.weights['weight'].quantizer.quantizer_fn # get QKeras quantizer
-        weights = node.weights['weight'].data_unquantized # get weights
+        quantizer = node.weights['weight'].quantizer.quantizer_fn  # get QKeras quantizer
+        weights = node.weights['weight'].data_unquantized  # get weights
         qweights = quantizer(tf.convert_to_tensor(weights))
         if isinstance(quantizer.scale, (int, float)):
             scale = np.ones(shape=node.get_output_variable().shape[-1]) * quantizer.scale
         else:
             scale = quantizer.scale.numpy()
-        unscale = 1. / scale
-
-        new_weights = unscale * qweights # use the quantized weights for safety
+        unscale = 1.0 / scale
 
+        new_weights = unscale * qweights  # use the quantized weights for safety
 
         qcfg = quantizer.get_config()
         alpha = qcfg['alpha']
         # Set the alpha to 1 to avoid hitting this pass again
         qcfg['alpha'] = 1
         node.weights['weight'].quantizer.quantizer_fn = quantizer.from_config(qcfg)
 
         # update the weights also applying the hls4ml quantizer
         # this is only needed for the binary layers which encode -1 as 0
         quantized_new_weights = node.weights['weight'].quantizer(new_weights.numpy())
-        if node.model.config.is_resource_strategy(node):
-            ndim = len(weights.shape) - 1
-            perm = [ndim] + [i for i in range(ndim)]
-            quantized_new_weights = np.transpose(quantized_new_weights, axes=perm)
         node.weights['weight'].data = quantized_new_weights
 
         # Move the biases from the Dense layer to the ApplyAlpha layer
         bias = node.weights['bias'].data
         bias_quantizer = None
         if hasattr(node.weights['bias'], 'quantizer'):
             bias_quantizer = node.weights['bias'].quantizer
         node.weights['bias'].data = np.zeros(bias.shape)
 
-        has_w_quant = node.get_attr('weight_quantizer') is not None 
+        has_w_quant = node.get_attr('weight_quantizer') is not None
         has_b_quant = node.get_attr('bias_quantizer') is not None
-        if has_w_quant: 
+        if has_w_quant:
             node.attributes['weight_quantizer'].alpha = 1
         if has_b_quant:
             node.attributes['bias_quantizer'].alpha = 1
 
         # insert a Batch Normalization layer to apply the alpha scale
         if alpha == 'auto_po2':
-            scale_bits = np.abs(np.log2(scale)).max().astype('int') + 1
-            scale_t = ExponentPrecisionType(width=scale_bits, signed=True)
-            scale_q = QKerasPO2Quantizer({'class_name' : 'quantized_po2', 'config': {'bits': scale_bits}})
+            scale_bits = np.maximum(np.abs(np.log2(scale)).max().astype('int') + 1, 2)
+            scale_quantizer = QKerasPO2Quantizer({'class_name': 'quantized_po2', 'config': {'bits': scale_bits}})
+        else:
+            scale_quantizer = None
+
+        if 'Dense' in node.class_name:
+            n_in = node.get_attr('n_out')
+        elif 'Conv' in node.class_name:
+            n_in = node.get_attr('out_width') * node.get_attr('out_height', 1) * node.get_attr('n_filt')
         else:
-            scale_t = FixedPrecisionType() # TODO: automate this
-            scale_q = None
+            n_in = node.get_attr('n_out')
 
         attrs = {
-            'name' : node.get_attr('name') + '_alpha',
-            'class_name' : 'Alpha',
-            'inputs' : node.outputs,
-            'n_in' : node.get_attr('n_out'),
-            'n_filt' : node.get_attr('n_filt', -1),
-            'reuse_factor' : node.get_attr('reuse_factor'),
-            'bias_t' : node.weights['bias'].type, 
-            'scale_t' : scale_t,
-            'Trace' : node.get_attr('Trace', False) 
+            'name': node.get_attr('name') + '_alpha',
+            'class_name': 'Alpha',
+            'inputs': node.outputs,
+            'n_in': n_in,
+            'n_filt': node.get_attr('n_filt', -1),
+            'reuse_factor': node.get_attr('reuse_factor'),
+            'scale_data': scale,
+            'scale_quantizer': scale_quantizer,
+            'bias_data': bias,
+            'bias_quantizer': bias_quantizer,
+            'trace': node.get_attr('trace', False),
         }
-        alpha_layer = model.make_node('ApplyAlpha', node.name + '_alpha', attrs, node.outputs)
-
-        alpha_layer.add_weights(scale, quantizer=scale_q)
-        alpha_layer.add_bias(bias, quantizer=bias_quantizer)
+        alpha_layer = model.make_node(ApplyAlpha, node.name + '_alpha', attrs, node.outputs)
         model.insert_node(alpha_layer)
         return True
 
+
 class FuseConsecutiveBatchNormalization(OptimizerPass):
     '''OptimizerPass to merge consecutive BatchNormalization layers.
-       These may exist in a model after QKerasFactorizeAlpha layer.
-       Scale and Bias of each layer are combined into scale and bias of a single layer.
+    These may exist in a model after QKerasFactorizeAlpha layer.
+    Scale and Bias of each layer are combined into scale and bias of a single layer.
     '''
 
     def match(self, node):
-        return isinstance(node, BatchNormalization) and \
-               isinstance(node.get_input_node(), BatchNormalization)
+        return isinstance(node, BatchNormalization) and isinstance(node.get_input_node(), BatchNormalization)
 
     def transform(self, model, node):
         bn0 = node.get_input_node()
         bn1 = node
+        bn0_map = bn0.get_output_use_map()
+        bn1_map = bn1.get_output_use_map()
+        if len(bn0_map[bn0.name]) > 1 or len(bn1_map[bn1.name]) > 1:
+            return False
 
         s0 = bn0.weights['scale'].data
         b0 = bn0.weights['bias'].data
         s1 = bn1.weights['scale'].data
         b1 = bn1.weights['bias'].data
 
         s2 = s0 * s1
@@ -220,43 +236,41 @@
 
         bn0.weights['scale'].data = s2
         bn0.weights['bias'].data = b2
 
         model.remove_node(node, rewire=True)
         return True
 
+
 class ExtractTernaryThreshold(OptimizerPass):
-    ''' The input value (threshold) at which the output of a a ternary activation
+    '''The input value (threshold) at which the output of a a ternary activation
     changes is configurable. This pass extracts that threshold point, inserting
     a BatchNormalization layer to execute the scaling. That BatchNormalization
     layer is then expected to be fused into a BatchNormalizationQuantizedTanh
     layer configured with the correct threshold.
     '''
 
     def match(self, node):
-        return node.__class__.__name__ == 'TernaryTanh' and node.get_attr('threshold', None) != 0.5
+        return node.class_name == 'TernaryTanh' and node.get_attr('threshold', None) != 0.5
 
     def transform(self, model, node):
         shape = node.get_input_variable().shape
         scale = np.full(shape, 0.5 / node.get_attr('threshold', 0.5))
         bias = np.zeros_like(scale)
         node.set_attr('threshold', 0.5)
 
         attrs = {
-            'name' : node.get_attr('name') + '_scale',
-            'class_name' : 'Alpha',
-            'inputs' : node.get_input_node().outputs,
-            'outputs' : node.inputs,
-            'n_filt' : node.get_attr('n_filt', -1),
-            'reuse_factor' : node.get_attr('reuse_factor'),
-            # These should just be placeholders
-            'bias_t' : IntegerPrecisionType(1),
-            'scale_t' : FixedPrecisionType(16,6),
-            'Trace' : node.get_attr('Trace', False)
+            'name': node.get_attr('name') + '_scale',
+            'class_name': 'Alpha',
+            'inputs': node.get_input_node().outputs,
+            'outputs': node.inputs,
+            'n_in': node.get_attr('n_in'),
+            'n_filt': node.get_attr('n_filt', -1),
+            'reuse_factor': node.get_attr('reuse_factor'),
+            'scale_data': scale,
+            'bias_data': bias,
+            'trace': node.get_attr('trace', False),
         }
 
-        layer = model.make_node('ApplyAlpha', node.name + '_scale', attrs, node.inputs.copy())
-        layer.add_weights(scale)
-        layer.add_bias(bias)
+        layer = model.make_node(ApplyAlpha, node.name + '_scale', attrs, node.inputs.copy())
         model.insert_node(layer, before=node)
         return True
-
```

### Comparing `hls4ml-0.6.0/hls4ml/model/profiling.py` & `hls4ml-0.7.0rc1/hls4ml/model/profiling.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,30 +1,31 @@
-from hls4ml.model.hls_model import HLSModel
-from hls4ml.model.hls_layers import IntegerPrecisionType, FixedPrecisionType
+import json
+import os
+import shutil
+import uuid
+from collections import defaultdict
+
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas
 import seaborn as sb
-import uuid
-import os
-import shutil
-import json
-from collections import defaultdict
 
-from hls4ml.model.hls_model import HLSModel
+from hls4ml.model.graph import ModelGraph
 
 try:
-    from tensorflow import keras
     import qkeras
+    from tensorflow import keras
+
     __tf_profiling_enabled__ = True
 except ImportError:
     __tf_profiling_enabled__ = False
 
 try:
     import torch
+
     __torch_profiling_enabled__ = True
 except ImportError:
     __torch_profiling_enabled__ = False
 
 
 def get_unoptimized_hlsmodel(model):
     from hls4ml.converters import convert_from_config
@@ -44,67 +45,65 @@
     new_config['OutputDir'] = new_output_dir
 
     return convert_from_config(new_config), new_output_dir
 
 
 def array_to_summary(x, fmt='boxplot'):
     if fmt == 'boxplot':
-        y = {'med' : np.median(x),
-             'q1' : np.percentile(x, 25),
-             'q3' : np.percentile(x, 75),
-             'whislo' : min(x),
-             'whishi' : max(x)
-        }
+        y = {'med': np.median(x), 'q1': np.percentile(x, 25), 'q3': np.percentile(x, 75), 'whislo': min(x), 'whishi': max(x)}
     elif fmt == 'histogram':
         # Power of 2 bins covering data range
         high = np.ceil(np.log2(max(x))) + 1
         low = np.floor(np.log2(min(x))) - 1
         bits = np.arange(low, high, 1)
-        bins = 2 ** bits
+        bins = 2**bits
         h, b = np.histogram(x, bins=bins)
-        h = h * 1. / float(sum(h)) # normalize
-        y = {'h' : h,
-             'b' : np.log2(b)}
+        h = h * 1.0 / float(sum(h))  # normalize
+        y = {'h': h, 'b': np.log2(b)}
     return y
 
+
 def boxplot(data, fmt='longform'):
     if fmt == 'longform':
-        f = plt.figure() #figsize=(3, 3))
+        f = plt.figure()  # figsize=(3, 3))
         hue = 'layer' if 'layer' in data.keys() else None
         vp = sb.boxplot(x='x', y='weight', hue=hue, data=data[data['x'] > 0], showfliers=False)
         vp.set_yticklabels(vp.get_yticklabels(), rotation=45, ha='right')
         if hue is not None:
             vp.get_legend().remove()
         vp.set_xscale('log', base=2)
         return f
     elif fmt == 'summary':
         from matplotlib.patches import Rectangle
+
         medianprops = dict(linestyle='-', color='k')
         f, ax = plt.subplots(1, 1)
         data.reverse()
         colors = sb.color_palette("Blues", len(data))
         bp = ax.bxp(data, showfliers=False, vert=False, medianprops=medianprops)
         # add colored boxes
         for line, color in zip(bp['boxes'], colors):
             x = line.get_xdata()
             xl, xh = min(x), max(x)
             y = line.get_ydata()
             yl, yh = min(y), max(y)
-            rect = Rectangle((xl, yl), (xh-xl), (yh-yl), fill=True, color=color)
+            rect = Rectangle((xl, yl), (xh - xl), (yh - yl), fill=True, color=color)
             ax.add_patch(rect)
         ax.set_yticklabels([d['weight'] for d in data])
         ax.set_xscale('log', base=2)
         plt.xlabel('x')
         return f
     else:
         return None
 
+
 def histogram(data, fmt='longform'):
     f = plt.figure()
     from matplotlib.ticker import MaxNLocator
+
     n = len(data) if fmt == 'summary' else len(data['weight'].unique())
     colors = sb.color_palette("husl", n)
     if fmt == 'longform':
         for i, weight in enumerate(data['weight'].unique()):
             y = array_to_summary(data[data['weight'] == weight]['x'], fmt='histogram')
             plt.bar(y['b'][:-1], y['h'], width=1, fill=False, label=weight, edgecolor=colors[i])
     elif fmt == 'summary':
@@ -113,126 +112,135 @@
 
     plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
     plt.xlabel('log2(x)')
     plt.ylabel('frequency')
     plt.legend()
     return f
 
-plots = {'boxplot' : boxplot,
-         'histogram' : histogram}
+
+plots = {'boxplot': boxplot, 'histogram': histogram}
+
 
 def types_boxplot(data, fmt='longform'):
-    from matplotlib.patches import PathPatch
-    from matplotlib.patches import Rectangle
+    from matplotlib.patches import PathPatch, Rectangle
+
     ax = plt.gca()
-    f = plt.gcf()
+    _ = plt.gcf()
     # Scale the data
-    data['low'] = 2.**data['low']
-    data['high'] = 2.**data['high']
+    data['low'] = 2.0 ** data['low']
+    data['high'] = 2.0 ** data['high']
 
     # Plot the custom precisions
     ticks = np.array([tick.get_text() for tick in plt.yticks()[1]])
     # Get the coordinates of the boxes to place the markers
     if fmt == 'longform':
         # seaborn adjusts the box positions slightly in groups
         boxes = [c.get_extents().inverse_transformed(ax.transData) for c in ax.get_children() if isinstance(c, PathPatch)]
         ys = [(box.y0 + box.y1) / 2 for box in boxes]
         ys = [(y, y) for y in ys]
     elif fmt == 'summary':
         ys = [(y, y) for y in plt.yticks()[0]]
-    for irow, row in data[data['layer'] != 'model'].iterrows():
+    for _irow, row in data[data['layer'] != 'model'].iterrows():
         if row['layer'] in ticks:
-            iy = np.argwhere(ticks == row['layer'])[0][0] # Determine which layer in the plot
-            rectangle = Rectangle((row['low'], ys[iy][0]-0.4), row['high']-row['low'], 0.8, fill=True, color='grey', alpha=0.2)
+            iy = np.argwhere(ticks == row['layer'])[0][0]  # Determine which layer in the plot
+            rectangle = Rectangle(
+                (row['low'], ys[iy][0] - 0.4), row['high'] - row['low'], 0.8, fill=True, color='grey', alpha=0.2
+            )
             ax.add_patch(rectangle)
 
+
 def types_histogram(data, fmt='longform'):
     ax = plt.gca()
     layers = np.array(ax.get_legend_handles_labels()[1])
     colors = sb.color_palette("husl", len(layers))
     ylim = ax.get_ylim()
-    for irow, row in data[data['layer'] != 'model'].iterrows():
+    for _irow, row in data[data['layer'] != 'model'].iterrows():
         if row['layer'] in layers:
             col = colors[np.argwhere(layers == row['layer'])[0][0]]
             plt.plot((row['low'], row['low']), ylim, '--', color=col)
             plt.plot((row['high'], row['high']), ylim, '--', color=col)
 
-types_plots = {'boxplot' : types_boxplot,
-               'histogram' : types_histogram}
+
+types_plots = {'boxplot': types_boxplot, 'histogram': types_histogram}
+
 
 def ap_fixed_WIFS(dtype):
-    from hls4ml.templates.vivado_template import VivadoBackend
-    dtype = VivadoBackend.convert_precision_string(None, dtype) 
+    from hls4ml.backends import VivadoBackend
+
+    dtype = VivadoBackend.convert_precision_string(dtype)
     W, I, F, S = dtype.width, dtype.integer, dtype.fractional, dtype.signed
     return W, I, F, S
 
+
 def types_hlsmodel(model):
     suffix = ['w', 'b']
-    data = {'layer' : [], 'low' : [], 'high' : []}
+    data = {'layer': [], 'low': [], 'high': []}
     # Plot the default precision
     default_precision = model.config.model_precision['default']
     W, I, F, S = ap_fixed_WIFS(default_precision)
     data['layer'].append('model')
     data['low'].append(-F)
-    data['high'].append(I-1 if S else I)
+    data['high'].append(I - 1 if S else I)
 
     for layer in model.get_layers():
         for iw, weight in enumerate(layer.get_weights()):
-            wname = '{}/{}'.format(layer.name, suffix[iw])
+            wname = f'{layer.name}/{suffix[iw]}'
             T = weight.type
             if T.name != 'model':
                 W, I, F, S = ap_fixed_WIFS(T.precision)
                 data['layer'].append(wname)
                 data['low'].append(-F)
-                data['high'].append(I-1 if S else I)
+                data['high'].append(I - 1 if S else I)
     data = pandas.DataFrame(data)
     return data
 
+
 def activation_types_hlsmodel(model):
-    data = {'layer' : [], 'low' : [], 'high' : []}
+    data = {'layer': [], 'low': [], 'high': []}
     # Get the default precision
     default_precision = model.config.model_precision['default']
     W, I, F, S = ap_fixed_WIFS(default_precision)
     data['layer'].append('model')
     data['low'].append(-F)
-    data['high'].append(I-1 if S else I)
+    data['high'].append(I - 1 if S else I)
     for layer in model.get_layers():
         T = layer.get_output_variable().type.precision
         W, I, F, S = ap_fixed_WIFS(T)
         data['layer'].append(layer.name)
         data['low'].append(-F)
-        data['high'].append(I-1 if S else I)
+        data['high'].append(I - 1 if S else I)
     data = pandas.DataFrame(data)
     return data
 
+
 def weights_hlsmodel(model, fmt='longform', plot='boxplot'):
     suffix = ['w', 'b']
     if fmt == 'longform':
-        data = {'x' : [], 'layer' : [], 'weight' : []}
+        data = {'x': [], 'layer': [], 'weight': []}
     elif fmt == 'summary':
         data = []
 
     for layer in model.get_layers():
         name = layer.name
         for iw, weight in enumerate(layer.get_weights()):
-            l = '{}/{}'.format(name, suffix[iw])
+            label = f'{name}/{suffix[iw]}'
             w = weight.data.flatten()
             w = abs(w[w != 0])
             n = len(w)
             if n == 0:
                 print(f'Weights for {name} are only zeros, ignoring.')
                 break
             if fmt == 'longform':
                 data['x'].extend(w.tolist())
-                data['layer'].extend([name for i in range(len(w))])
-                data['weight'].extend([l for i in range(len(w))])
+                data['layer'].extend([name] * len(w))
+                data['weight'].extend([label] * len(w))
             elif fmt == 'summary':
                 data.append(array_to_summary(w, fmt=plot))
                 data[-1]['layer'] = name
-                data[-1]['weight'] = l
+                data[-1]['weight'] = label
 
     if fmt == 'longform':
         data = pandas.DataFrame(data)
     return data
 
 
 def _keras_batchnorm(layer):
@@ -250,34 +258,32 @@
     return [scale, bias], ['s', 'b']
 
 
 def _keras_layer(layer):
     return layer.get_weights(), ['w', 'b']
 
 
-keras_process_layer_map = defaultdict(lambda: _keras_layer,
-                                      {
-                                          'BatchNormalization': _keras_batchnorm,
-                                          'QBatchNormalization': _keras_batchnorm
-                                      })
+keras_process_layer_map = defaultdict(
+    lambda: _keras_layer, {'BatchNormalization': _keras_batchnorm, 'QBatchNormalization': _keras_batchnorm}
+)
 
 
 def activations_hlsmodel(model, X, fmt='summary', plot='boxplot'):
     if fmt == 'longform':
-        raise NotImplemented
+        raise NotImplementedError
     elif fmt == 'summary':
         data = []
 
     _, trace = model.trace(np.ascontiguousarray(X))
 
     if len(trace) == 0:
-        raise RuntimeError("HLSModel must have tracing on for at least 1 layer (this can be set in its config)")
+        raise RuntimeError("ModelGraph must have tracing on for at least 1 layer (this can be set in its config)")
 
     for layer in trace.keys():
-        print("   {}".format(layer))
+        print(f"   {layer}")
 
         if fmt == 'summary':
             y = trace[layer].flatten()
             y = abs(y[y != 0])
 
             if len(y) == 0:
                 print(f'Activations for {layer} are only zeros, ignoring.')
@@ -287,55 +293,56 @@
             data[-1]['weight'] = layer
 
     return data
 
 
 def weights_keras(model, fmt='longform', plot='boxplot'):
     if fmt == 'longform':
-        data = {'x' : [], 'layer' : [], 'weight' : []}
+        data = {'x': [], 'layer': [], 'weight': []}
     elif fmt == 'summary':
         data = []
     for layer in model.layers:
         name = layer.name
         weights, suffix = keras_process_layer_map[type(layer).__name__](layer)
 
         for i, w in enumerate(weights):
-            l = '{}/{}'.format(name, suffix[i])
+            label = f'{name}/{suffix[i]}'
             w = w.flatten()
             w = abs(w[w != 0])
             n = len(w)
             if n == 0:
                 print(f'Weights for {name} are only zeros, ignoring.')
                 break
             if fmt == 'longform':
                 data['x'].extend(w.tolist())
-                data['layer'].extend([name for j in range(n)])
-                data['weight'].extend([l for j in range(n)])
+                data['layer'].extend([name] * n)
+                data['weight'].extend([label] * n)
             elif fmt == 'summary':
                 data.append(array_to_summary(w, fmt=plot))
                 data[-1]['layer'] = name
-                data[-1]['weight'] = l
+                data[-1]['weight'] = label
 
     if fmt == 'longform':
         data = pandas.DataFrame(data)
     return data
 
+
 def activations_keras(model, X, fmt='longform', plot='boxplot'):
     # test layer by layer on data
     if fmt == 'longform':
         # return long form pandas dataframe for
         # seaborn boxplot
-        data = {'x' : [], 'weight' : []}
+        data = {'x': [], 'weight': []}
     elif fmt == 'summary':
         # return summary statistics for matplotlib.axes.Axes.bxp
         # or histogram bin edges and heights
         data = []
 
     for layer in model.layers:
-        print("   {}".format(layer.name))
+        print(f"   {layer.name}")
         if not isinstance(layer, keras.layers.InputLayer):
             y = _get_output(layer, X, model.input).flatten()
             y = abs(y[y != 0])
             if len(y) == 0:
                 print(f'Activations for {layer.name} are only zeros, ignoring.')
                 continue
             if fmt == 'longform':
@@ -357,30 +364,30 @@
     elif fmt == 'summary':
         data = []
     for layer in model.children():
         if isinstance(layer, torch.nn.Linear):
             name = layer.__class__.__name__
             weights = list(layer.parameters())
             for i, w in enumerate(weights):
-                l = '{}/{}'.format(name, suffix[i])
+                label = f'{name}/{suffix[i]}'
                 w = weights[i].detach().numpy()
                 w = w.flatten()
                 w = abs(w[w != 0])
                 n = len(w)
                 if n == 0:
                     print(f'Weights for {name} are only zeros, ignoring.')
                     break
                 if fmt == 'longform':
                     data['x'].extend(w.tolist())
-                    data['layer'].extend([name for _ in range(n)])
-                    data['weight'].extend([l for _ in range(n)])
+                    data['layer'].extend([name] * n)
+                    data['weight'].extend([label] * n)
                 elif fmt == 'summary':
                     data.append(array_to_summary(w, fmt=plot))
                     data[-1]['layer'] = name
-                    data[-1]['weight'] = l
+                    data[-1]['weight'] = label
 
     if fmt == 'longform':
         data = pandas.DataFrame(data)
     return data
 
 
 def activations_torch(model, X, fmt='longform', plot='boxplot'):
@@ -392,15 +399,15 @@
 
     partial_model = torch.nn.Sequential
     layers = []
     for layer in model.children():
         lname = layer.__class__.__name__
         layers.append(layer)
         pm = partial_model(*layers)
-        print("   {}".format(lname))
+        print(f"   {lname}")
         y = pm(X).flatten().detach().numpy()
         y = abs(y[y != 0])
         if len(y) == 0:
             print(f'Activations for {lname} are only zeros, ignoring.')
             continue
         if fmt == 'longform':
             data['x'].extend(y.tolist())
@@ -418,36 +425,36 @@
     """
     Perform numerical profiling of a model
 
     Parameters
     ----------
     model : keras or pytorch model
         The model to profile
-    hls_model : HLSModel
-        The HLSModel to profile
+    hls_model : ModelGraph
+        The ModelGraph to profile
     X : array-like, optional
         Test data on which to evaluate the model to profile activations
         Must be formatted suitably for the ``model.predict(X)`` method
     plot : str, optional
         The type of plot to produce.
         Options are: 'boxplot' (default), 'violinplot', 'histogram',
         'FacetGrid'
 
     Returns
     -------
     tuple
         The quadruple of produced figures. First weights and biases
         for the pre- and post-optimization models respectively,
         then activations for the pre- and post-optimization models
-        respectively. (Optimizations are applied to an HLSModel by hls4ml,
-        a post-optimization HLSModel is a final model)
+        respectively. (Optimizations are applied to an ModelGraph by hls4ml,
+        a post-optimization ModelGraph is a final model)
     """
     wp, wph, ap, aph = None, None, None, None
 
-    hls_model_present = hls_model is not None and isinstance(hls_model, HLSModel)
+    hls_model_present = hls_model is not None and isinstance(hls_model, ModelGraph)
     model_present = model is not None
 
     if hls_model_present:
         before = " (before optimization)"
         after = " (final / after optimization)"
         hls_model_unoptimized, tmp_output_dir = get_unoptimized_hlsmodel(hls_model)
     else:
@@ -459,21 +466,19 @@
     data = None
 
     if hls_model_present:
         data = weights_hlsmodel(hls_model_unoptimized, fmt='summary', plot=plot)
     elif model_present:
         if __tf_profiling_enabled__ and isinstance(model, keras.Model):
             data = weights_keras(model, fmt='summary', plot=plot)
-        elif __torch_profiling_enabled__ and \
-                isinstance(model, torch.nn.Sequential):
+        elif __torch_profiling_enabled__ and isinstance(model, torch.nn.Sequential):
             data = weights_torch(model, fmt='summary', plot=plot)
 
     if data is None:
-        print("Only keras, PyTorch (Sequential) and HLSModel models " +
-              "can currently be profiled")
+        print("Only keras, PyTorch (Sequential) and ModelGraph models " + "can currently be profiled")
 
         if hls_model_present and os.path.exists(tmp_output_dir):
             shutil.rmtree(tmp_output_dir)
 
         return wp, wph, ap, aph
 
     wp = plots[plot](data, fmt='summary')  # weight plot
@@ -499,16 +504,15 @@
         plt.tight_layout()
 
     if X is not None:
         print("Profiling activations" + before)
         data = None
         if __tf_profiling_enabled__ and isinstance(model, keras.Model):
             data = activations_keras(model, X, fmt='summary', plot=plot)
-        elif __torch_profiling_enabled__ and \
-                isinstance(model, torch.nn.Sequential):
+        elif __torch_profiling_enabled__ and isinstance(model, torch.nn.Sequential):
             data = activations_torch(model, X, fmt='summary', plot=plot)
 
         if data is not None:
             ap = plots[plot](data, fmt='summary')  # activation plot
             if hls_model_present and plot in types_plots:
                 t_data = activation_types_hlsmodel(hls_model_unoptimized)
                 types_plots[plot](t_data, fmt='summary')
@@ -528,29 +532,31 @@
 
     if hls_model_present and os.path.exists(tmp_output_dir):
         shutil.rmtree(tmp_output_dir)
 
     return wp, wph, ap, aph
 
 
-########COMPARE OUTPUT IMPLEMENTATION########
+#########
+# COMPARE OUTPUT IMPLEMENTATION
+#########
 def _is_ignored_layer(layer):
     """Some layers need to be ingored during inference"""
-    if isinstance(layer, (keras.layers.InputLayer,
-                        keras.layers.Dropout)):
+    if isinstance(layer, (keras.layers.InputLayer, keras.layers.Dropout)):
         return True
     return False
 
+
 def _get_output(layer, X, model_input):
     """Get output of partial model"""
-    partial_model = keras.models.Model(inputs=model_input,
-                                       outputs=layer.output)
+    partial_model = keras.models.Model(inputs=model_input, outputs=layer.output)
     y = partial_model.predict(X)
     return y
 
+
 def get_ymodel_keras(keras_model, X):
     """
     Calculate each layer's ouput and put them into a dictionary
 
     Parameters
     ----------
     keras_model :
@@ -560,65 +566,71 @@
         Must be formatted suitably for the ``model.predict(X)`` method.
 
     Returns
     -------
     dictionary
         A dictionary in the form {"layer_name": ouput array of layer}
     """
-    
+
     ymodel = {}
-    
+
     for layer in keras_model.layers:
-        print("Processing {} in Keras model...".format(layer.name))
+        print(f"Processing {layer.name} in Keras model...")
         if not _is_ignored_layer(layer):
-            #If the layer has activation integrated then separate them
-            #Note that if the layer is a standalone activation layer then skip this
-            if hasattr(layer, 'activation') and not (isinstance(layer,keras.layers.Activation) or isinstance(layer, qkeras.qlayers.QActivation)):
+            # If the layer has activation integrated then separate them
+            # Note that if the layer is a standalone activation layer then skip this
+            if hasattr(layer, 'activation') and not (
+                isinstance(layer, keras.layers.Activation) or isinstance(layer, qkeras.qlayers.QActivation)
+            ):
                 if layer.activation:
-                    
+
                     if layer.activation.__class__.__name__ == "linear":
                         ymodel[layer.name] = _get_output(layer, X, keras_model.input)
-                    
+
                     else:
                         temp_activation = layer.activation
                         layer.activation = None
-                        #Get output for layer without activation
+                        # Get output for layer without activation
                         ymodel[layer.name] = _get_output(layer, X, keras_model.input)
 
-                        #Add the activation back 
+                        # Add the activation back
                         layer.activation = temp_activation
-                        #Get ouput for activation
-                        ymodel[layer.name + "_{}".format(temp_activation.__class__.__name__)] =  _get_output(layer, X, keras_model.input)
+                        # Get ouput for activation
+                        ymodel[layer.name + f"_{temp_activation.__class__.__name__}"] = _get_output(
+                            layer, X, keras_model.input
+                        )
                 else:
                     ymodel[layer.name] = _get_output(layer, X, keras_model.input)
-            else:    
+            else:
                 ymodel[layer.name] = _get_output(layer, X, keras_model.input)
     print("Done taking outputs for Keras model.")
     return ymodel
 
+
 def _norm_diff(ymodel, ysim):
     """Calculate the square root of the sum of the squares of the differences"""
     diff = {}
-    
+
     for key in list(ysim.keys()):
-        diff[key] = np.linalg.norm(ysim[key]-ymodel[key])
-    
-    #---Bar Plot---
+        diff[key] = np.linalg.norm(ysim[key] - ymodel[key])
+
+    # ---Bar Plot---
     f, ax = plt.subplots()
-    plt.bar(list(diff.keys()),list(diff.values()))
+    plt.bar(list(diff.keys()), list(diff.values()))
     plt.title("layer-by-layer output differences")
     ax.set_ylabel('Norm of difference vector')
     plt.xticks(rotation=90)
     plt.tight_layout()
     return f
 
+
 def _dist_diff(ymodel, ysim):
     """
     Calculate the normalized distribution of the differences of the elements
-    of the output vectors. 
+    of the output vectors.
     If difference >= original value then the normalized difference will be set to 1,
     meaning "very difference".
     If difference < original value then the normalized difference would be difference/original.
     """
 
     diff = {}
 
@@ -627,69 +639,70 @@
         flattened_ymodel = np.array(ymodel[key]).flatten()
 
         diff[key] = np.absolute(flattened_ymodel - flattened_ysim) / np.linalg.norm(flattened_ymodel - flattened_ysim)
         diff_vector = np.absolute(flattened_ymodel - flattened_ysim)
         abs_ymodel = np.absolute(flattened_ymodel)
 
         normalized_diff = np.zeros(diff_vector.shape)
-        normalized_diff[(diff_vector >= abs_ymodel) & (abs_ymodel>0) & (diff_vector>0)] = 1
+        normalized_diff[(diff_vector >= abs_ymodel) & (abs_ymodel > 0) & (diff_vector > 0)] = 1
 
-        #Fill out the rest
+        # Fill out the rest
         index = diff_vector < abs_ymodel
         normalized_diff[index] = diff_vector[index] / abs_ymodel[index]
 
         diff[key] = normalized_diff
 
-    #---Box Plot---
+    # ---Box Plot---
     f, ax = plt.subplots()
-    pos = np.array(range(len(list(diff.values())))) + 1            
+    pos = np.array(range(len(list(diff.values())))) + 1
     ax.boxplot(list(diff.values()), sym='k+', positions=pos)
 
-    #--formatting
+    # --formatting
     plt.title("Layer-by-layer distribution of output differences")
     ax.set_xticklabels(list(diff.keys()))
     ax.set_ylabel('Normalized difference')
     ax.set_ylabel('Percent difference.')
     plt.xticks(rotation=90)
     plt.tight_layout()
 
     return f
 
-def compare(keras_model, hls_model, X, plot_type = "dist_diff"):
+
+def compare(keras_model, hls_model, X, plot_type="dist_diff"):
     """
     Compare each layer's output in keras and hls model. Note that the hls_model should not be compiled before using this.
 
     Parameters
     ----------
-    keras_model : 
+    keras_model :
         original keras model
     hls_model :
-        converted HLSModel, with "Trace:True" in the configuration file.
-    X : array-like 
-        Input for the model. 
+        converted ModelGraph, with "Trace:True" in the configuration file.
+    X : array-like
+        Input for the model.
     plot_type : string
         different methods to visualize the y_model and y_sim differences.
         Possible options include:
-        
-        - 'norm_diff' : square root of the sum of the squares of the differences 
-          between each output vectors 
+
+        - 'norm_diff' : square root of the sum of the squares of the differences
+          between each output vectors
         - 'dist_diff' : The normalized distribution of the differences of the elements
           between two output vectors
-        
+
     Returns
     -------
     matplotlib figure
         plot object of the histogram depicting the difference in each layer's output
     """
-    
-    #Take in output from both models
-    #Note that each y is a dictionary with structure {"layer_name": flattened ouput array}
+
+    # Take in output from both models
+    # Note that each y is a dictionary with structure {"layer_name": flattened ouput array}
     ymodel = get_ymodel_keras(keras_model, X)
     _, ysim = hls_model.trace(X)
-    
+
     print("Plotting difference...")
     f = plt.figure()
     if plot_type == "norm_diff":
         f = _norm_diff(ymodel, ysim)
     elif plot_type == "dist_diff":
         f = _dist_diff(ymodel, ysim)
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_common.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_common.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_decl.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_decl.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed_base.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed_base.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed_ref.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed_ref.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_fixed_special.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_fixed_special.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int_base.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int_base.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int_ref.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int_ref.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_int_special.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_int_special.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/ap_shift_reg.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/ap_shift_reg.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/etc/ap_private.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/etc/ap_private.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/hls_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/hls_stream.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/ap_types/utils/x_hls_utils.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/ap_types/utils/x_hls_utils.h`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/build_lib.sh` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/build_lib.sh`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -10,8 +10,8 @@
 INCFLAGS="-Ifirmware/ap_types/"
 PROJECT=myproject
 LIB_STAMP=mystamp
 
 ${CC} ${CFLAGS} ${INCFLAGS} -c firmware/${PROJECT}.cpp -o ${PROJECT}.o
 ${CC} ${CFLAGS} ${INCFLAGS} -c ${PROJECT}_bridge.cpp -o ${PROJECT}_bridge.o
 ${CC} ${CFLAGS} ${INCFLAGS} -shared ${PROJECT}.o ${PROJECT}_bridge.o -o firmware/${PROJECT}-${LIB_STAMP}.so
-rm -f *.o
+rm -f *.o
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/myproject_bridge.cpp` & `hls4ml-0.7.0rc1/hls4ml/templates/quartus/myproject_bridge.cpp`

 * *Files 11% similar despite different names*

```diff
@@ -2,35 +2,34 @@
 #define MYPROJECT_BRIDGE_H_
 
 #include "firmware/myproject.h"
 #include "firmware/nnet_utils/nnet_helpers.h"
 #include <algorithm>
 #include <map>
 
-//hls-fpga-machine-learning insert bram
-
+// hls-fpga-machine-learning insert bram
 
 namespace nnet {
-    bool trace_enabled = false;
-    std::map<std::string, void *> *trace_outputs = NULL;
-    size_t trace_type_size = sizeof(double);
-}
+bool trace_enabled = false;
+std::map<std::string, void *> *trace_outputs = NULL;
+size_t trace_type_size = sizeof(double);
+} // namespace nnet
 
 extern "C" {
 
 struct trace_data {
     const char *name;
     void *data;
 };
 
 void allocate_trace_storage(size_t element_size) {
     nnet::trace_enabled = true;
     nnet::trace_outputs = new std::map<std::string, void *>;
     nnet::trace_type_size = element_size;
-    //hls-fpga-machine-learning insert trace_outputs
+    // hls-fpga-machine-learning insert trace_outputs
 }
 
 void free_trace_storage() {
     for (std::map<std::string, void *>::iterator i = nnet::trace_outputs->begin(); i != nnet::trace_outputs->end(); i++) {
         void *ptr = i->second;
         free(ptr);
     }
@@ -47,22 +46,21 @@
         c_trace_outputs[ii].data = i->second;
         ii++;
     }
 }
 
 // Wrapper of top level function for Python bridge
 void myproject_float(
-    //hls-fpga-machine-learning insert header #float
+    // hls-fpga-machine-learning insert header #float
 ) {
-    
-    //hls-fpga-machine-learning insert wrapper #float
+
+    // hls-fpga-machine-learning insert wrapper #float
 }
 
 void myproject_double(
-    //hls-fpga-machine-learning insert header #double
+    // hls-fpga-machine-learning insert header #double
 ) {
-    //hls-fpga-machine-learning insert wrapper #double
+    // hls-fpga-machine-learning insert wrapper #double
 }
-
 }
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/myproject_test.cpp` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/myproject_test.cpp`

 * *Files 23% similar despite different names*

```diff
@@ -1,112 +1,92 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
+#include <algorithm>
 #include <fstream>
 #include <iostream>
-#include <algorithm>
-#include <vector>
 #include <map>
+#include <math.h>
 #include <stdio.h>
 #include <stdlib.h>
-#include <math.h>
+#include <vector>
 
 #include "firmware/myproject.h"
 #include "firmware/nnet_utils/nnet_helpers.h"
 
-//hls-fpga-machine-learning insert bram
+// hls-fpga-machine-learning insert bram
 
 #define CHECKPOINT 5000
 
 namespace nnet {
-    bool trace_enabled = true;
-    std::map<std::string, void *> *trace_outputs = NULL;
-    size_t trace_type_size = sizeof(double);
-}
-
-int main(int argc, char **argv)
-{
-  //load input data from text file
-  std::ifstream fin("tb_data/tb_input_features.dat");
-  //load predictions from text file
-  std::ifstream fpr("tb_data/tb_output_predictions.dat");
+bool trace_enabled = true;
+std::map<std::string, void *> *trace_outputs = NULL;
+size_t trace_type_size = sizeof(double);
+} // namespace nnet
+
+int main(int argc, char **argv) {
+    // load input data from text file
+    std::ifstream fin("tb_data/tb_input_features.dat");
+    // load predictions from text file
+    std::ifstream fpr("tb_data/tb_output_predictions.dat");
 
 #ifdef RTL_SIM
-  std::string RESULTS_LOG = "tb_data/rtl_cosim_results.log";
+    std::string RESULTS_LOG = "tb_data/rtl_cosim_results.log";
 #else
-  std::string RESULTS_LOG = "tb_data/csim_results.log";
+    std::string RESULTS_LOG = "tb_data/csim_results.log";
 #endif
-  std::ofstream fout(RESULTS_LOG);
-
-  std::string iline;
-  std::string pline;
-  int e = 0;
-
-  if (fin.is_open() && fpr.is_open()) {
-    while ( std::getline(fin,iline) && std::getline (fpr,pline) ) {
-      if (e % CHECKPOINT == 0) std::cout << "Processing input " << e << std::endl;
-      char* cstr=const_cast<char*>(iline.c_str());
-      char* current;
-      std::vector<float> in;
-      current=strtok(cstr," ");
-      while(current!=NULL) {
-        in.push_back(atof(current));
-        current=strtok(NULL," ");
-      }
-      cstr=const_cast<char*>(pline.c_str());
-      std::vector<float> pr;
-      current=strtok(cstr," ");
-      while(current!=NULL) {
-        pr.push_back(atof(current));
-        current=strtok(NULL," ");
-      }
-
-      //hls-fpga-machine-learning insert data
-
-      //hls-fpga-machine-learning insert top-level-function
-
-      if (e % CHECKPOINT == 0) {
-        std::cout << "Predictions" << std::endl;
-        //hls-fpga-machine-learning insert predictions
-        std::cout << "Quantized predictions" << std::endl;
-        //hls-fpga-machine-learning insert quantized
-      }
-      e++;
-
-      //hls-fpga-machine-learning insert tb-output
+    std::ofstream fout(RESULTS_LOG);
 
-    }
-    fin.close();
-    fpr.close();
-  } else {
-    std::cout << "INFO: Unable to open input/predictions file, using default input." << std::endl;
-
-    //hls-fpga-machine-learning insert zero
+    std::string iline;
+    std::string pline;
+    int e = 0;
+
+    if (fin.is_open() && fpr.is_open()) {
+        while (std::getline(fin, iline) && std::getline(fpr, pline)) {
+            if (e % CHECKPOINT == 0)
+                std::cout << "Processing input " << e << std::endl;
+            char *cstr = const_cast<char *>(iline.c_str());
+            char *current;
+            std::vector<float> in;
+            current = strtok(cstr, " ");
+            while (current != NULL) {
+                in.push_back(atof(current));
+                current = strtok(NULL, " ");
+            }
+            cstr = const_cast<char *>(pline.c_str());
+            std::vector<float> pr;
+            current = strtok(cstr, " ");
+            while (current != NULL) {
+                pr.push_back(atof(current));
+                current = strtok(NULL, " ");
+            }
+
+            // hls-fpga-machine-learning insert data
+
+            // hls-fpga-machine-learning insert top-level-function
+
+            if (e % CHECKPOINT == 0) {
+                std::cout << "Predictions" << std::endl;
+                // hls-fpga-machine-learning insert predictions
+                std::cout << "Quantized predictions" << std::endl;
+                // hls-fpga-machine-learning insert quantized
+            }
+            e++;
+
+            // hls-fpga-machine-learning insert tb-output
+        }
+        fin.close();
+        fpr.close();
+    } else {
+        std::cout << "INFO: Unable to open input/predictions file, using default input." << std::endl;
 
-    //hls-fpga-machine-learning insert top-level-function
+        // hls-fpga-machine-learning insert zero
 
-    //hls-fpga-machine-learning insert output
+        // hls-fpga-machine-learning insert top-level-function
 
-    //hls-fpga-machine-learning insert tb-output
+        // hls-fpga-machine-learning insert output
 
-  }
+        // hls-fpga-machine-learning insert tb-output
+    }
 
-  fout.close();
-  std::cout << "INFO: Saved inference results to file: " << RESULTS_LOG << std::endl;
+    fout.close();
+    std::cout << "INFO: Saved inference results to file: " << RESULTS_LOG << std::endl;
 
-  return 0;
+    return 0;
 }
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_activation.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_activation.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,230 +1,177 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_ACTIVATION_H_
 #define NNET_ACTIVATION_H_
 
-#include <cmath>
 #include "ap_fixed.h"
 #include "nnet_common.h"
+#include <cmath>
 
 namespace nnet {
 
-struct activ_config
-{
+struct activ_config {
     // IO size
     static const unsigned n_in = 10;
 
     // Internal info
     static const unsigned table_size = 1024;
 
     // Resource reuse info
     static const unsigned io_type = io_parallel;
     static const unsigned reuse_factor = 1;
 
     // Internal data type definitions
-    typedef ap_fixed<18,8> table_t;
+    typedef ap_fixed<18, 8> table_t;
 };
 
 // *************************************************
 //       LINEAR Activation -- See Issue 53
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  linear(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, typename CONFIG_T> void linear(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         res[ii] = data[ii];
     }
 }
 
-
-
 // *************************************************
 //       RELU Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  relu(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, typename CONFIG_T> void relu(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
     data_T datareg;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
-        if (datareg > 0) res[ii] = datareg;
-        else res[ii] = 0;
+        if (datareg > 0)
+            res[ii] = datareg;
+        else
+            res[ii] = 0;
     }
 }
 
-template<class data_T, class res_T, int MAX_INT, typename CONFIG_T>
-void  relu_max(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, int MAX_INT, typename CONFIG_T>
+void relu_max(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
     data_T datareg;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
-        if (datareg < 0) res[ii] = 0;
-        else if (datareg > MAX_INT) res[ii] = MAX_INT;
-        else res[ii] = datareg;
+        if (datareg < 0)
+            res[ii] = 0;
+        else if (datareg > MAX_INT)
+            res[ii] = MAX_INT;
+        else
+            res[ii] = datareg;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  relu6(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T> void relu6(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     relu_max<data_T, res_T, 6, CONFIG_T>(data, res);
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  relu1(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T> void relu1(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     relu_max<data_T, res_T, 1, CONFIG_T>(data, res);
 }
 
 // *************************************************
 //       Sigmoid Activation
 // *************************************************
-inline float sigmoid_fcn_float(float input) {
-    return 1.0 / (1 + std::exp(-input));
-}
+inline float sigmoid_fcn_float(float input) { return 1.0 / (1 + std::exp(-input)); }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_sigmoid_table(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_sigmoid_table(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Default logistic sigmoid function:
     //   result = 1/(1+e^(-x))
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -8 to +8)
-        float in_val = 2*8.0*(ii-float(N_TABLE)/2.0)/float(N_TABLE);
+        float in_val = 2 * 8.0 * (ii - float(N_TABLE) / 2.0) / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = sigmoid_fcn_float(in_val);
-        //std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
         table_out[ii] = real_val;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  sigmoid(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void sigmoid(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t sigmoid_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t sigmoid_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_sigmoid_table<CONFIG_T, CONFIG_T::table_size>(sigmoid_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     // Index into the lookup table based on data
     int data_round;
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
-        data_round = data[ii]*CONFIG_T::table_size/16;
-        index = data_round + 8*CONFIG_T::table_size/16;
-        if (index < 0)   index = 0;
-        if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
-        res[ii] = (res_T) sigmoid_table[index];
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        data_round = data[ii] * CONFIG_T::table_size / 16;
+        index = data_round + 8 * CONFIG_T::table_size / 16;
+        if (index < 0)
+            index = 0;
+        if (index > CONFIG_T::table_size - 1)
+            index = CONFIG_T::table_size - 1;
+        res[ii] = (res_T)sigmoid_table[index];
     }
 }
 
 // *************************************************
 //       Softmax Activation
 // *************************************************
 
-enum class softmax_implementation {latency=0, legacy=1, stable=2};
+enum class softmax_implementation { latency = 0, legacy = 1, stable = 2, argmax = 3 };
 
-inline float exp_fcn_float(float input) {
-    return std::exp(input);
-}
+inline float exp_fcn_float(float input) { return std::exp(input); }
 
-template<class data_T, typename CONFIG_T>
-inline float softmax_real_val_from_idx(unsigned i){
+template <class data_T, typename CONFIG_T> inline float softmax_real_val_from_idx(unsigned i) {
     // Treat the index as the top N bits
     static constexpr int N = ceillog2(CONFIG_T::table_size); // number of address bits for table
     data_T x(0);
-    x(x.width-1, x.width-N) = i;
-    return (float) x;
+    x(x.width - 1, x.width - N) = i;
+    return (float)x;
 }
 
-template<class data_T, typename CONFIG_T>
-inline unsigned softmax_idx_from_real_val(data_T x){
+template <class data_T, typename CONFIG_T> inline unsigned softmax_idx_from_real_val(data_T x) {
     // Slice the top N bits to get an index into the table
     static constexpr int N = ceillog2(CONFIG_T::table_size); // number of address bits for table
-    ap_uint<N> y = x(x.width-1, x.width-N); // slice the top N bits of input
-    return (unsigned) y(N-1, 0);
+    ap_uint<N> y = x(x.width - 1, x.width - N);              // slice the top N bits of input
+    return (unsigned)y(N - 1, 0);
 }
 
-template<class data_T, typename CONFIG_T>
-void init_exp_table(typename CONFIG_T::exp_table_t table_out[CONFIG_T::table_size]){
+template <class data_T, typename CONFIG_T>
+void init_exp_table(typename CONFIG_T::exp_table_t table_out[CONFIG_T::table_size]) {
     // The template data_T is the data type used to address the table
-    for(unsigned i = 0; i < CONFIG_T::table_size; i++){
+    for (unsigned i = 0; i < CONFIG_T::table_size; i++) {
         // Slicing bits for address is going to round towards 0, so take the central value
         float x = softmax_real_val_from_idx<data_T, CONFIG_T>(i);
         typename CONFIG_T::exp_table_t exp_x = exp_fcn_float(x);
         table_out[i] = exp_x;
     }
 }
 
-template<class data_T, typename CONFIG_T>
-void init_invert_table(typename CONFIG_T::inv_table_t table_out[CONFIG_T::table_size]){
+template <class data_T, typename CONFIG_T>
+void init_invert_table(typename CONFIG_T::inv_table_t table_out[CONFIG_T::table_size]) {
     // The template data_T is the data type used to address the table
-    for(unsigned i = 0; i < CONFIG_T::table_size; i++){
+    for (unsigned i = 0; i < CONFIG_T::table_size; i++) {
         float x = softmax_real_val_from_idx<data_T, CONFIG_T>(i);
         typename CONFIG_T::inv_table_t inv_x = 1 / x;
         table_out[i] = inv_x;
     }
 }
 
 template <class data_T, class res_T, typename CONFIG_T>
-void softmax_latency(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]){
+void softmax_latency(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     #pragma HLS pipeline
     // Initialize the lookup tables
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::exp_table_t exp_table[CONFIG_T::table_size];
     typename CONFIG_T::inv_table_t invert_table[CONFIG_T::table_size];
 #else
@@ -241,34 +188,36 @@
         initialized = true;
     }
 
     // Calculate all the e^x's
     typename CONFIG_T::exp_table_t exp_res[CONFIG_T::n_in];
     #pragma HLS array_partition variable=exp_res complete
     typename CONFIG_T::exp_table_t exp_sum(0);
-    for(unsigned i = 0; i < CONFIG_T::n_in; i++){
+    for (unsigned i = 0; i < CONFIG_T::n_in; i++) {
         #pragma HLS unroll
         unsigned x = softmax_idx_from_real_val<data_T, CONFIG_T>(data[i]);
         exp_res[i] = exp_table[x];
     }
 
     // Explicitly sum the results with an adder tree.
     // Rounding & Saturation mode, which improve accuracy, prevent Vivado from expression balancing
     Op_add<typename CONFIG_T::exp_table_t> op_add;
-    exp_sum = reduce<typename CONFIG_T::exp_table_t, CONFIG_T::n_in, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
+    exp_sum =
+        reduce<typename CONFIG_T::exp_table_t, CONFIG_T::n_in, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
 
-    typename CONFIG_T::inv_table_t inv_exp_sum = invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t,CONFIG_T>(exp_sum)];
-    for(unsigned i = 0; i < CONFIG_T::n_in; i++){
+    typename CONFIG_T::inv_table_t inv_exp_sum =
+        invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t, CONFIG_T>(exp_sum)];
+    for (unsigned i = 0; i < CONFIG_T::n_in; i++) {
         #pragma HLS unroll
         res[i] = exp_res[i] * inv_exp_sum;
     }
 }
 
 template <class data_T, class res_T, typename CONFIG_T>
-void softmax_stable(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]){
+void softmax_stable(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     #pragma HLS pipeline
     // Initialize the lookup tables
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::exp_table_t exp_table[CONFIG_T::table_size];
     typename CONFIG_T::inv_table_t invert_table[CONFIG_T::table_size];
 #else
@@ -286,72 +235,71 @@
     }
 
     // Find the max and compute all delta(x_i, x_max)
     Op_max<data_T> op_max;
     data_T x_max = reduce<data_T, CONFIG_T::n_in, Op_max<data_T>>(data, op_max);
 
     // For the diffs, use the same type as the input but force rounding and saturation
-    ap_fixed<data_T::width, data_T::iwidth,AP_RND,AP_SAT> d_xi_xmax[CONFIG_T::n_in];
-    for(unsigned i = 0; i < CONFIG_T::n_in; i++){
+    ap_fixed<data_T::width, data_T::iwidth, AP_RND, AP_SAT> d_xi_xmax[CONFIG_T::n_in];
+    for (unsigned i = 0; i < CONFIG_T::n_in; i++) {
         #pragma HLS unroll
         d_xi_xmax[i] = data[i] - x_max;
     }
 
     // Calculate all the e^x's
     typename CONFIG_T::exp_table_t exp_res[CONFIG_T::n_in];
     #pragma HLS array_partition variable=exp_res complete
     typename CONFIG_T::exp_table_t exp_sum(0);
-    for(unsigned i = 0; i < CONFIG_T::n_in; i++){
+    for (unsigned i = 0; i < CONFIG_T::n_in; i++) {
         #pragma HLS unroll
         unsigned x = softmax_idx_from_real_val<data_T, CONFIG_T>(d_xi_xmax[i]);
         exp_res[i] = exp_table[x];
     }
 
     // Explicitly sum the results with an adder tree.
     // Rounding & Saturation mode, which improve accuracy, prevent Vivado from expression balancing
     Op_add<typename CONFIG_T::exp_table_t> op_add;
-    exp_sum = reduce<typename CONFIG_T::exp_table_t, CONFIG_T::n_in, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
+    exp_sum =
+        reduce<typename CONFIG_T::exp_table_t, CONFIG_T::n_in, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
 
-    typename CONFIG_T::inv_table_t inv_exp_sum = invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t,CONFIG_T>(exp_sum)];
-    for(unsigned i = 0; i < CONFIG_T::n_in; i++){
+    typename CONFIG_T::inv_table_t inv_exp_sum =
+        invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t, CONFIG_T>(exp_sum)];
+    for (unsigned i = 0; i < CONFIG_T::n_in; i++) {
         #pragma HLS unroll
         res[i] = exp_res[i] * inv_exp_sum;
     }
 }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_exp_table_legacy(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_exp_table_legacy(typename CONFIG_T::table_t table_out[N_TABLE]) {
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -8 to +8)
-        float in_val = 2*8.0*(ii-float(N_TABLE)/2.0)/float(N_TABLE);
+        float in_val = 2 * 8.0 * (ii - float(N_TABLE) / 2.0) / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = exp_fcn_float(in_val);
-        //std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
         table_out[ii] = real_val;
     }
 }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_invert_table_legacy(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_invert_table_legacy(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Inversion function:
     //   result = 1/x
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range 0 to +64)
-        float in_val = 64.0*ii/float(N_TABLE);
+        float in_val = 64.0 * ii / float(N_TABLE);
         // Next, compute lookup table function
-        if (in_val > 0.0) table_out[ii] = 1.0/in_val;
-        else table_out[ii] = 0.0;
+        if (in_val > 0.0)
+            table_out[ii] = 1.0 / in_val;
+        else
+            table_out[ii] = 0.0;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  softmax_legacy(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void softmax_legacy(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t exp_table[CONFIG_T::table_size];
     typename CONFIG_T::table_t invert_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
@@ -360,502 +308,470 @@
 #endif
     if (!initialized) {
         init_exp_table_legacy<CONFIG_T, CONFIG_T::table_size>(exp_table);
         init_invert_table_legacy<CONFIG_T, CONFIG_T::table_size>(invert_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        // Note: This is going to be a resource hog to run with pipeline, but hey, whatever
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     // Index into the lookup table based on data for exponentials
-    typename CONFIG_T::table_t exp_res[CONFIG_T::n_in];// different, independent, fixed point precision
-    typename CONFIG_T::table_t exp_diff_res;// different, independent, fixed point precision
+    typename CONFIG_T::table_t exp_res[CONFIG_T::n_in]; // different, independent, fixed point precision
+    typename CONFIG_T::table_t exp_diff_res;            // different, independent, fixed point precision
     data_T data_cache[CONFIG_T::n_in];
     int data_round;
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         data_cache[ii] = data[ii];
         exp_res[ii] = 0;
     }
 
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial) {
-            #pragma HLS PIPELINE
-        }
-        for (int jj=0; jj<CONFIG_T::n_in; jj++) {
-            if (ii==jj) exp_diff_res = 1;
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        for (int jj = 0; jj < CONFIG_T::n_in; jj++) {
+            if (ii == jj)
+                exp_diff_res = 1;
             else {
-                data_round = (data_cache[jj]-data_cache[ii])*CONFIG_T::table_size/16;
-                index = data_round + 8*CONFIG_T::table_size/16;
-                if (index < 0)   index = 0;
-                if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+                data_round = (data_cache[jj] - data_cache[ii]) * CONFIG_T::table_size / 16;
+                index = data_round + 8 * CONFIG_T::table_size / 16;
+                if (index < 0)
+                    index = 0;
+                if (index > CONFIG_T::table_size - 1)
+                    index = CONFIG_T::table_size - 1;
                 exp_diff_res = exp_table[index];
             }
             exp_res[ii] += exp_diff_res;
         }
     }
 
-    //Second loop to invert
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        int exp_res_index = exp_res[ii]*CONFIG_T::table_size/64;
-        if (exp_res_index < 0)   exp_res_index = 0;
-        if (exp_res_index > CONFIG_T::table_size-1) exp_res_index = CONFIG_T::table_size-1;
-        //typename CONFIG_T::table_t exp_res_invert = invert_table[exp_res_index];
-        res[ii] = (res_T) invert_table[exp_res_index];
+    // Second loop to invert
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        int exp_res_index = exp_res[ii] * CONFIG_T::table_size / 64;
+        if (exp_res_index < 0)
+            exp_res_index = 0;
+        if (exp_res_index > CONFIG_T::table_size - 1)
+            exp_res_index = CONFIG_T::table_size - 1;
+        // typename CONFIG_T::table_t exp_res_invert = invert_table[exp_res_index];
+        res[ii] = (res_T)invert_table[exp_res_index];
+    }
+}
+
+template <class data_T, class res_T, typename CONFIG_T>
+void softmax_argmax(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    for (int i = 0; i < CONFIG_T::n_in; i++) {
+        #pragma HLS UNROLL
+        res[i] = (res_T)0;
+    }
+
+    data_T maximum = data[0];
+    int idx = 0;
+
+    for (int i = 1; i < CONFIG_T::n_in; i++) {
+        #pragma HLS PIPELINE
+        if (data[i] > maximum) {
+            maximum = data[i];
+            idx = i;
+        }
     }
 
+    res[idx] = (res_T)1;
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void softmax(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]){
+template <class data_T, class res_T, typename CONFIG_T>
+void softmax(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     #pragma HLS inline
-    switch(CONFIG_T::implementation){
+    switch (CONFIG_T::implementation) {
     case softmax_implementation::latency:
         softmax_latency<data_T, res_T, CONFIG_T>(data, res);
         break;
     case softmax_implementation::stable:
         softmax_stable<data_T, res_T, CONFIG_T>(data, res);
         break;
     case softmax_implementation::legacy:
         softmax_legacy<data_T, res_T, CONFIG_T>(data, res);
         break;
+    case softmax_implementation::argmax:
+        softmax_argmax<data_T, res_T, CONFIG_T>(data, res);
+        break;
     }
 }
 
 // *************************************************
 //       TanH Activation
 // *************************************************
-template<typename CONFIG_T, int N_TABLE>
-void init_tanh_table(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_tanh_table(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Implement tanh lookup
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -4 to +4)
-        float in_val = 2*4.0*(ii-float(N_TABLE)/2.0)/float(N_TABLE);
+        float in_val = 2 * 4.0 * (ii - float(N_TABLE) / 2.0) / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = tanh(in_val);
-        //std::cout << "Tanh:  Lookup table Index: " <<  ii<< " In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Tanh:  Lookup table Index: " <<  ii<< " In Value: " << in_val << " Result: " << real_val <<
+        // std::endl;
         table_out[ii] = real_val;
     }
 }
 
-
-template<class data_T, class res_T, typename CONFIG_T>
-void  tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T> void tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t tanh_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t tanh_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_tanh_table<CONFIG_T, CONFIG_T::table_size>(tanh_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     // Index into the lookup table based on data
     int data_round;
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
-        data_round = data[ii]*CONFIG_T::table_size/8;
-        index = data_round + 4*CONFIG_T::table_size/8;
-        //std::cout << "Input: "  << data[ii] << " Round: " << data_round << " Index: " << index << std::endl;
-        if (index < 0)   index = 0;
-        if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
-        res[ii] = (res_T) tanh_table[index];
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        data_round = data[ii] * CONFIG_T::table_size / 8;
+        index = data_round + 4 * CONFIG_T::table_size / 8;
+        // std::cout << "Input: "  << data[ii] << " Round: " << data_round << " Index: " << index << std::endl;
+        if (index < 0)
+            index = 0;
+        if (index > CONFIG_T::table_size - 1)
+            index = CONFIG_T::table_size - 1;
+        res[ii] = (res_T)tanh_table[index];
     }
 }
 
 // *************************************************
 //       Hard sigmoid Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  hard_sigmoid(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
+template <class data_T, class res_T, typename CONFIG_T>
+void hard_sigmoid(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        auto datareg = CONFIG_T::slope * data[ii] + CONFIG_T::shift;
+        if (datareg > 1)
+            datareg = 1;
+        else if (datareg < 0)
+            datareg = 0;
+        res[ii] = datareg;
+    }
+}
+
+template <class data_T, class res_T, typename CONFIG_T>
+void hard_tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    if (CONFIG_T::io_type == io_parallel) {
         #pragma HLS PIPELINE
     }
 
-    data_T datareg;
-    data_T slope = (data_T) 0.2;
-    data_T shift = (data_T) 0.5;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
-        datareg = slope * data[ii] + shift;
-        if (datareg > 1) datareg = 1;
-        else if (datareg < 0) datareg = 0;
-        res[ii] = datareg;
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        auto sigmoid = CONFIG_T::slope * data[ii] + CONFIG_T::shift;
+        if (sigmoid > 1)
+            sigmoid = 1;
+        else if (sigmoid < 0)
+            sigmoid = 0;
+        res[ii] = 2 * sigmoid - 1;
     }
 }
 
 // *************************************************
 //       Leaky RELU Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  leaky_relu(data_T data[CONFIG_T::n_in], data_T alpha, res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, typename CONFIG_T>
+void leaky_relu(data_T data[CONFIG_T::n_in], data_T alpha, res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
     data_T datareg;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
-        if (datareg > 0) res[ii] = datareg;
-        else res[ii] = alpha * datareg;
+        if (datareg > 0)
+            res[ii] = datareg;
+        else
+            res[ii] = alpha * datareg;
     }
 }
 
 // *************************************************
 //       Thresholded RELU Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  thresholded_relu(data_T data[CONFIG_T::n_in], data_T theta, res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, typename CONFIG_T>
+void thresholded_relu(data_T data[CONFIG_T::n_in], data_T theta, res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
     data_T datareg;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
-        if (datareg > theta) res[ii] = datareg;
-        else res[ii] = 0;
+        if (datareg > theta)
+            res[ii] = datareg;
+        else
+            res[ii] = 0;
     }
 }
 
 // *************************************************
 //       Softplus Activation
 // *************************************************
-inline float softplus_fcn_float(float input) {
-    return std::log(std::exp(input) + 1.);
-}
+inline float softplus_fcn_float(float input) { return std::log(std::exp(input) + 1.); }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_softplus_table(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_softplus_table(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Default softplus function:
     //   result = log(exp(x) + 1)
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -8 to +8)
-        float in_val = 2*8.0*(ii-float(N_TABLE)/2.0)/float(N_TABLE);
+        float in_val = 2 * 8.0 * (ii - float(N_TABLE) / 2.0) / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = softplus_fcn_float(in_val);
-        //std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
         table_out[ii] = real_val;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  softplus(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void softplus(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t softplus_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t softplus_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_softplus_table<CONFIG_T, CONFIG_T::table_size>(softplus_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     // Index into the lookup table based on data
     int data_round;
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
-        data_round = data[ii]*CONFIG_T::table_size/16;
-        index = data_round + 8*CONFIG_T::table_size/16;
-        if (index < 0)   index = 0;
-        if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
-        res[ii] = (res_T) softplus_table[index];
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        data_round = data[ii] * CONFIG_T::table_size / 16;
+        index = data_round + 8 * CONFIG_T::table_size / 16;
+        if (index < 0)
+            index = 0;
+        if (index > CONFIG_T::table_size - 1)
+            index = CONFIG_T::table_size - 1;
+        res[ii] = (res_T)softplus_table[index];
     }
 }
 
 // *************************************************
 //       Softsign Activation
 // *************************************************
-inline float softsign_fcn_float(float input) {
-    return input / (std::abs(input) + 1.);
-}
+inline float softsign_fcn_float(float input) { return input / (std::abs(input) + 1.); }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_softsign_table(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_softsign_table(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Default softsign function:
     //   result = x / (abs(x) + 1)
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -8 to +8)
-        float in_val = 2*8.0*(ii-float(N_TABLE)/2.0)/float(N_TABLE);
+        float in_val = 2 * 8.0 * (ii - float(N_TABLE) / 2.0) / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = softsign_fcn_float(in_val);
-        //std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
         table_out[ii] = real_val;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  softsign(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void softsign(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t softsign_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t softsign_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_softsign_table<CONFIG_T, CONFIG_T::table_size>(softsign_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     // Index into the lookup table based on data
     int data_round;
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
-        data_round = data[ii]*CONFIG_T::table_size/16;
-        index = data_round + 8*CONFIG_T::table_size/16;
-        if (index < 0)   index = 0;
-        if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
-        res[ii] = (res_T) softsign_table[index];
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        data_round = data[ii] * CONFIG_T::table_size / 16;
+        index = data_round + 8 * CONFIG_T::table_size / 16;
+        if (index < 0)
+            index = 0;
+        if (index > CONFIG_T::table_size - 1)
+            index = CONFIG_T::table_size - 1;
+        res[ii] = (res_T)softsign_table[index];
     }
 }
 
 // *************************************************
 //       ELU Activation
 // *************************************************
-inline float elu_fcn_float(float input) {
-    return std::exp(input) - 1.;
-}
+inline float elu_fcn_float(float input) { return std::exp(input) - 1.; }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_elu_table(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_elu_table(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Default ELU function:
     //   result = alpha * (e^(x) - 1)
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -8 to 0)
-        float in_val = -8.0*ii/float(N_TABLE);
+        float in_val = -8.0 * ii / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = elu_fcn_float(in_val);
-        //std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
         table_out[ii] = real_val;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  elu(data_T data[CONFIG_T::n_in], const res_T alpha, res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void elu(data_T data[CONFIG_T::n_in], const res_T alpha, res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t elu_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t elu_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_elu_table<CONFIG_T, CONFIG_T::table_size>(elu_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     data_T datareg;
     // Index into the lookup table based on data
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
         if (datareg >= 0) {
             res[ii] = datareg;
         } else {
-            index = datareg*CONFIG_T::table_size/-8;
-            if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+            index = datareg * CONFIG_T::table_size / -8;
+            if (index > CONFIG_T::table_size - 1)
+                index = CONFIG_T::table_size - 1;
             res[ii] = alpha * elu_table[index];
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  elu(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T> void elu(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     elu<data_T, res_T, CONFIG_T>(data, 1.0, res);
 }
 
 // *************************************************
 //       SELU Activation
 // *************************************************
 inline float selu_fcn_float(float input) {
     return 1.0507009873554804934193349852946 * (1.6732632423543772848170429916717 * (std::exp(input) - 1.));
 }
 
-template<typename CONFIG_T, int N_TABLE>
-void init_selu_table(typename CONFIG_T::table_t table_out[N_TABLE])
-{
+template <typename CONFIG_T, int N_TABLE> void init_selu_table(typename CONFIG_T::table_t table_out[N_TABLE]) {
     // Default SELU function:
     //   result = 1.05 * (1.673 * (e^(x) - 1))
     for (int ii = 0; ii < N_TABLE; ii++) {
         // First, convert from table index to X-value (signed 8-bit, range -8 to 0)
-        float in_val = -8.0*ii/float(N_TABLE);
+        float in_val = -8.0 * ii / float(N_TABLE);
         // Next, compute lookup table function
         typename CONFIG_T::table_t real_val = selu_fcn_float(in_val);
-        //std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
+        // std::cout << "Lookup table In Value: " << in_val << " Result: " << real_val << std::endl;
         table_out[ii] = real_val;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void  selu(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T> void selu(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t selu_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t selu_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_selu_table<CONFIG_T, CONFIG_T::table_size>(selu_table);
         initialized = true;
     }
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+    #pragma HLS PIPELINE
 
     data_T datareg;
     // Index into the lookup table based on data
     int index;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
         if (datareg >= 0) {
             res[ii] = res_T(1.0507009873554804934193349852946) * datareg;
         } else {
-            index = datareg*CONFIG_T::table_size/-8;
-            if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+            index = datareg * CONFIG_T::table_size / -8;
+            if (index > CONFIG_T::table_size - 1)
+                index = CONFIG_T::table_size - 1;
             res[ii] = selu_table[index];
         }
     }
 }
 
 // *************************************************
 //       PReLU Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  prelu(data_T data[CONFIG_T::n_in], data_T alpha[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, typename CONFIG_T>
+void prelu(data_T data[CONFIG_T::n_in], data_T alpha[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
     data_T datareg;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
-        if (datareg > 0) res[ii] = datareg;
-        else res[ii] = alpha[ii] * datareg;
+        if (datareg > 0)
+            res[ii] = datareg;
+        else
+            res[ii] = alpha[ii] * datareg;
     }
 }
 
 // *************************************************
 //       Binary TanH Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  binary_tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
+template <class data_T, class res_T, typename CONFIG_T>
+void binary_tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
     data_T datareg;
     res_T cache;
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial){
-            #pragma HLS PIPELINE
-        }
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
         datareg = data[ii];
-        if( datareg > 0 ) cache = 1;
-        else cache = -1;
+        if (datareg > 0)
+            cache = 1;
+        else
+            cache = -1;
 
-        res[ii] = (res_T) cache;
+        res[ii] = (res_T)cache;
     }
 }
 
 // *************************************************
 //       Ternary TanH Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void  ternary_tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void ternary_tanh(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_in]) {
+    #pragma HLS PIPELINE
 
-    if (CONFIG_T::io_type == io_parallel){
-        #pragma HLS PIPELINE
-    }
-  
-    data_T datareg;   
-    res_T cache; 
-    for (int ii=0; ii<CONFIG_T::n_in; ii++) {
-        if (CONFIG_T::io_type == io_serial) {
-            #pragma HLS PIPELINE
-        }
-        datareg = 2*data[ii];
-        if( datareg > 1 ) cache = 1;
-        else if( datareg > -1 && datareg <= 1) cache=0;
-        else cache = -1;
-  
-        res[ii] = (res_T) cache;
+    data_T datareg;
+    res_T cache;
+    for (int ii = 0; ii < CONFIG_T::n_in; ii++) {
+        datareg = 2 * data[ii];
+        if (datareg > 1)
+            cache = 1;
+        else if (datareg > -1 && datareg <= 1)
+            cache = 0;
+        else
+            cache = -1;
+
+        res[ii] = (res_T)cache;
     }
- 
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_activation_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_activation_stream.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,129 +1,115 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_ACTIVATION_STREAM_H_
 #define NNET_ACTIVATION_STREAM_H_
 
-#include <cmath>
 #include "ap_fixed.h"
 #include "hls_stream.h"
+#include "nnet_activation.h"
 #include "nnet_common.h"
-#include "nnet_types.h"
 #include "nnet_stream.h"
-#include "nnet_activation.h"
+#include "nnet_types.h"
+#include <cmath>
 
 namespace nnet {
 
 // *************************************************
 //       LINEAR Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void linear(hls::stream<data_T> &data, hls::stream<res_T> &res) {
-    LinearActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+template <class data_T, class res_T, typename CONFIG_T> void linear(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+LinearActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        LinearPackLoop: for (int j = 0; j < res_T::size; j++) {
+    LinearPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
             out_data[j] = in_data[j];
         }
 
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       RELU Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void relu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
-    ReLUActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+template <class data_T, class res_T, typename CONFIG_T> void relu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+ReLUActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        ReLUPackLoop: for (int j = 0; j < res_T::size; j++) {
+    ReLUPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            if (in_data[j] > 0) out_data[j] = in_data[j];
-            else out_data[j] = 0;
+            if (in_data[j] > 0)
+                out_data[j] = in_data[j];
+            else
+                out_data[j] = 0;
         }
 
         res.write(out_data);
     }
 }
 
 // *************************************************
 //       Sigmoid Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
-void sigmoid(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void sigmoid(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t sigmoid_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t sigmoid_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_sigmoid_table<CONFIG_T, CONFIG_T::table_size>(sigmoid_table);
         initialized = true;
     }
 
-    SigmoidActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+SigmoidActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        SigmoidPackLoop: for (int j = 0; j < res_T::size; j++) {
+    SigmoidPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            int data_round = in_data[j]*CONFIG_T::table_size/16;
-            int index = data_round + 8*CONFIG_T::table_size/16;
-            if (index < 0)   index = 0;
-            else if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+            int data_round = in_data[j] * CONFIG_T::table_size / 16;
+            int index = data_round + 8 * CONFIG_T::table_size / 16;
+            if (index < 0)
+                index = 0;
+            else if (index > CONFIG_T::table_size - 1)
+                index = CONFIG_T::table_size - 1;
             out_data[j] = sigmoid_table[index];
         }
 
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       Softmax Activation
 // *************************************************
 
 template <class data_T, class res_T, typename CONFIG_T>
-void softmax_latency(hls::stream<data_T> &data, hls::stream<res_T> &res){
+void softmax_latency(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup tables
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::exp_table_t exp_table[CONFIG_T::table_size];
     typename CONFIG_T::inv_table_t invert_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
@@ -142,44 +128,50 @@
     constexpr unsigned multiplier_limit = DIV_ROUNDUP(data_T::size, CONFIG_T::reuse_factor);
     constexpr unsigned ii = data_T::size / multiplier_limit;
 
     // Calculate all the e^x's
     typename CONFIG_T::exp_table_t exp_res[data_T::size];
     #pragma HLS array_partition variable=exp_res complete
     typename CONFIG_T::exp_table_t exp_sum(0);
-    SoftmaxExpLoop: for(unsigned i = 0; i < CONFIG_T::n_in / data_T::size; i++){
+SoftmaxExpLoop:
+    for (unsigned i = 0; i < CONFIG_T::n_in / data_T::size; i++) {
         #pragma HLS PIPELINE II=ii
 
         data_T in_pack = data.read();
-        SoftmaxExpPackLoop: for(unsigned j = 0; j < data_T::size; j++){
+    SoftmaxExpPackLoop:
+        for (unsigned j = 0; j < data_T::size; j++) {
             #pragma HLS UNROLL
             unsigned x = softmax_idx_from_real_val<typename data_T::value_type, CONFIG_T>(in_pack[j]);
             exp_res[j] = exp_table[x];
         }
 
         // Explicitly sum the results with an adder tree.
         // Rounding & Saturation mode, which improve accuracy, prevent Vivado from expression balancing
         Op_add<typename CONFIG_T::exp_table_t> op_add;
-        exp_sum = reduce<typename CONFIG_T::exp_table_t, data_T::size, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
+        exp_sum =
+            reduce<typename CONFIG_T::exp_table_t, data_T::size, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
 
-        typename CONFIG_T::inv_table_t inv_exp_sum = invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t,CONFIG_T>(exp_sum)];
+        typename CONFIG_T::inv_table_t inv_exp_sum =
+            invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t, CONFIG_T>(exp_sum)];
 
         res_T out_pack;
-        #pragma HLS DATA_PACK variable=out_pack
-        SoftmaxInvPackLoop: for(unsigned j = 0; j < res_T::size; j++){
+        PRAGMA_DATA_PACK(out_pack)
+
+    SoftmaxInvPackLoop:
+        for (unsigned j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            #pragma HLS ALLOCATION instances=mul limit=multiplier_limit operation
+            #pragma HLS ALLOCATION operation instances=mul limit=multiplier_limit
             out_pack[j] = exp_res[j] * inv_exp_sum;
         }
         res.write(out_pack);
     }
 }
 
 template <class data_T, class res_T, typename CONFIG_T>
-void softmax_stable(hls::stream<data_T> &data, hls::stream<res_T> &res){
+void softmax_stable(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup tables
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::exp_table_t exp_table[CONFIG_T::table_size];
     typename CONFIG_T::inv_table_t invert_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
@@ -195,64 +187,71 @@
         initialized = true;
     }
 
     constexpr unsigned multiplier_limit = DIV_ROUNDUP(data_T::size, CONFIG_T::reuse_factor);
     constexpr unsigned ii = data_T::size / multiplier_limit;
 
     typename data_T::value_type data_array[data_T::size];
-    #pragma HLS ARRAY_PARTITION variable=data_array complete
-    SoftmaxArrayLoop: for(unsigned i = 0; i < CONFIG_T::n_in / data_T::size; i++){
+#pragma HLS ARRAY_PARTITION variable=data_array complete
+SoftmaxArrayLoop:
+    for (unsigned i = 0; i < CONFIG_T::n_in / data_T::size; i++) {
         #pragma HLS PIPELINE II=ii
 
         data_T in_pack = data.read();
-        SoftmaxArrayPackLoop: for(unsigned j = 0; j < data_T::size; j++){
+    SoftmaxArrayPackLoop:
+        for (unsigned j = 0; j < data_T::size; j++) {
             #pragma HLS UNROLL
             data_array[j] = in_pack[j];
         }
 
         // Find the max and compute all delta(x_i, x_max)
         Op_max<typename data_T::value_type> op_max;
-        typename data_T::value_type x_max = reduce<typename data_T::value_type, data_T::size, Op_max<typename data_T::value_type>>(data_array, op_max);
+        typename data_T::value_type x_max =
+            reduce<typename data_T::value_type, data_T::size, Op_max<typename data_T::value_type>>(data_array, op_max);
 
         // For the diffs, use the same type as the input but force rounding and saturation
-        ap_fixed<data_T::value_type::width, data_T::value_type::iwidth,AP_RND,AP_SAT> d_xi_xmax[data_T::size];
-        for(unsigned j = 0; j < data_T::size; j++){
+        ap_fixed<data_T::value_type::width, data_T::value_type::iwidth, AP_RND, AP_SAT> d_xi_xmax[data_T::size];
+        for (unsigned j = 0; j < data_T::size; j++) {
             #pragma HLS UNROLL
             d_xi_xmax[j] = data_array[j] - x_max;
         }
 
         // Calculate all the e^x's
         typename CONFIG_T::exp_table_t exp_res[data_T::size];
         #pragma HLS ARRAY_PARTITION variable=exp_res complete
         typename CONFIG_T::exp_table_t exp_sum(0);
-        for(unsigned j = 0; j < data_T::size; j++){
+        for (unsigned j = 0; j < data_T::size; j++) {
             #pragma HLS UNROLL
             unsigned x = softmax_idx_from_real_val<typename data_T::value_type, CONFIG_T>(d_xi_xmax[j]);
             exp_res[j] = exp_table[x];
         }
 
         // Explicitly sum the results with an adder tree.
         // Rounding & Saturation mode, which improve accuracy, prevent Vivado from expression balancing
         Op_add<typename CONFIG_T::exp_table_t> op_add;
-        exp_sum = reduce<typename CONFIG_T::exp_table_t, data_T::size, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
+        exp_sum =
+            reduce<typename CONFIG_T::exp_table_t, data_T::size, Op_add<typename CONFIG_T::exp_table_t>>(exp_res, op_add);
 
-        typename CONFIG_T::inv_table_t inv_exp_sum = invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t,CONFIG_T>(exp_sum)];
+        typename CONFIG_T::inv_table_t inv_exp_sum =
+            invert_table[softmax_idx_from_real_val<typename CONFIG_T::exp_table_t, CONFIG_T>(exp_sum)];
 
         res_T out_pack;
-        #pragma HLS DATA_PACK variable=out_pack
-        SoftmaxInvPackLoop: for(unsigned j = 0; j < res_T::size; j++){
+        PRAGMA_DATA_PACK(out_pack)
+
+    SoftmaxInvPackLoop:
+        for (unsigned j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            #pragma HLS ALLOCATION instances=mul limit=multiplier_limit operation
+            #pragma HLS ALLOCATION operation instances=mul limit=multiplier_limit
             out_pack[j] = exp_res[j] * inv_exp_sum;
         }
         res.write(out_pack);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void softmax_legacy(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t exp_table[CONFIG_T::table_size];
     typename CONFIG_T::table_t invert_table[CONFIG_T::table_size];
 #else
@@ -267,377 +266,512 @@
     }
 
     // Index into the lookup table based on data for exponentials
     typename CONFIG_T::table_t exp_res[data_T::size];
     typename CONFIG_T::table_t exp_diff_res;
     typename data_T::value_type data_cache[data_T::size];
 
-    SoftmaxInitLoop: for(unsigned s = 0; s < CONFIG_T::n_in / data_T::size; s++) {
+SoftmaxInitLoop:
+    for (unsigned s = 0; s < CONFIG_T::n_in / data_T::size; s++) {
         #pragma HLS PIPELINE
         data_T in_pack = data.read();
-        SoftmaxInitPackLoop: for(unsigned j = 0; j < data_T::size; j++) {
+    SoftmaxInitPackLoop:
+        for (unsigned j = 0; j < data_T::size; j++) {
             #pragma HLS UNROLL
             data_cache[j] = in_pack[j];
             exp_res[j] = 0;
         }
 
-        SoftmaxExpLoop: for (int i = 0; i < data_T::size; i++) {
-            #pragma HLS UNROLL
-            SoftmaxExpInner: for (int j = 0; j < data_T::size; j++) {
+    SoftmaxExpLoop:
+        for (int i = 0; i < data_T::size; i++) {
+        #pragma HLS UNROLL
+        SoftmaxExpInner:
+            for (int j = 0; j < data_T::size; j++) {
                 #pragma HLS UNROLL
 
                 if (i == j) {
                     exp_diff_res = 1;
                 } else {
                     int data_round = (data_cache[j] - data_cache[i]) * CONFIG_T::table_size / 16;
                     int index = data_round + 8 * CONFIG_T::table_size / 16;
-                    if (index < 0) index = 0;
-                    if (index > CONFIG_T::table_size - 1) index = CONFIG_T::table_size - 1;
+                    if (index < 0)
+                        index = 0;
+                    if (index > CONFIG_T::table_size - 1)
+                        index = CONFIG_T::table_size - 1;
                     exp_diff_res = exp_table[index];
                 }
 
                 exp_res[i] += exp_diff_res;
             }
         }
 
         res_T out_pack;
-        #pragma HLS DATA_PACK variable=out_pack
-        SoftmaxInvPackLoop: for(unsigned j = 0; j < res_T::size; j++) {
+        PRAGMA_DATA_PACK(out_pack)
+
+    SoftmaxInvPackLoop:
+        for (unsigned j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
 
             int exp_res_index = exp_res[j] * CONFIG_T::table_size / 64;
-            if (exp_res_index < 0) exp_res_index = 0;
-            if (exp_res_index > CONFIG_T::table_size - 1) exp_res_index = CONFIG_T::table_size - 1;
+            if (exp_res_index < 0)
+                exp_res_index = 0;
+            if (exp_res_index > CONFIG_T::table_size - 1)
+                exp_res_index = CONFIG_T::table_size - 1;
 
-            out_pack[j] = (typename res_T::value_type) invert_table[exp_res_index];
+            out_pack[j] = (typename res_T::value_type)invert_table[exp_res_index];
         }
         res.write(out_pack);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void softmax(hls::stream<data_T> &data, hls::stream<res_T> &res){
+template <class data_T, class res_T, typename CONFIG_T>
+void softmax_argmax(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+        #pragma HLS PIPELINE
+        data_T in_data = data.read();
+        res_T out_data;
+
+        for (int i = 0; i < res_T::size; i++) {
+            #pragma HLS UNROLL
+            out_data[i] = (typename res_T::value_type)0;
+        }
+
+        typename data_T::value_type maximum = in_data[0];
+        int idx = 0;
+
+        for (int i = 1; i < res_T::size; i++) {
+            #pragma HLS PIPELINE
+            if (in_data[i] > maximum) {
+                maximum = in_data[i];
+                idx = i;
+            }
+        }
+
+        out_data[idx] = (typename res_T::value_type)1;
+        res.write(out_data);
+    }
+}
+
+template <class data_T, class res_T, typename CONFIG_T> void softmax(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::axis == -1);
 
-    switch(CONFIG_T::implementation){
+    switch (CONFIG_T::implementation) {
     case softmax_implementation::latency:
         softmax_latency<data_T, res_T, CONFIG_T>(data, res);
         break;
     case softmax_implementation::stable:
         softmax_stable<data_T, res_T, CONFIG_T>(data, res);
         break;
     case softmax_implementation::legacy:
         softmax_legacy<data_T, res_T, CONFIG_T>(data, res);
         break;
-    }    
+    case softmax_implementation::argmax:
+        softmax_argmax<data_T, res_T, CONFIG_T>(data, res);
+        break;
+    }
 }
 
 // *************************************************
 //       TanH Activation
 // *************************************************
 
-
-template<class data_T, class res_T, typename CONFIG_T>
-void tanh(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void tanh(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t tanh_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t tanh_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_tanh_table<CONFIG_T, CONFIG_T::table_size>(tanh_table);
         initialized = true;
     }
 
-    TanHActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+TanHActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        TanHPackLoop: for (int j = 0; j < res_T::size; j++) {
+    TanHPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            int data_round = in_data[j]*CONFIG_T::table_size/8;
-            int index = data_round + 4*CONFIG_T::table_size/8;
-            if (index < 0)   index = 0;
-            else if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+            int data_round = in_data[j] * CONFIG_T::table_size / 8;
+            int index = data_round + 4 * CONFIG_T::table_size / 8;
+            if (index < 0)
+                index = 0;
+            else if (index > CONFIG_T::table_size - 1)
+                index = CONFIG_T::table_size - 1;
             out_data[j] = tanh_table[index];
         }
 
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       Hard sigmoid Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void hard_sigmoid(hls::stream<data_T> &data, hls::stream<res_T> &res) {
-    typename data_T::value_type slope = (typename data_T::value_type) 0.2;
-    typename data_T::value_type shift = (typename data_T::value_type) 0.5;
 
-    HardSigmoidActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+HardSigmoidActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        HardSigmoidPackLoop: for (int j = 0; j < res_T::size; j++) {
+    HardSigmoidPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            typename data_T::value_type datareg = slope * in_data[j] + shift;
-            if (datareg > 1) datareg = 1;
-            else if (datareg < 0) datareg = 0;
+            auto datareg = CONFIG_T::slope * in_data[j] + CONFIG_T::shift;
+            if (datareg > 1)
+                datareg = 1;
+            else if (datareg < 0)
+                datareg = 0;
             out_data[j] = datareg;
         }
 
         res.write(out_data);
     }
 }
 
+template <class data_T, class res_T, typename CONFIG_T> void hard_tanh(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+
+HardSigmoidActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+        #pragma HLS PIPELINE
+
+        data_T in_data = data.read();
+        res_T out_data;
+        #pragma HLS DATA_PACK variable=out_data
+
+    HardSigmoidPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
+            #pragma HLS UNROLL
+            auto sigmoid = CONFIG_T::slope * in_data[j] + CONFIG_T::shift;
+            if (sigmoid > 1)
+                sigmoid = 1;
+            else if (sigmoid < 0)
+                sigmoid = 0;
+            out_data[j] = 2 * sigmoid - 1;
+        }
+
+        res.write(out_data);
+    }
+}
 
 // *************************************************
 //       Leaky RELU Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void leaky_relu(hls::stream<data_T> &data, typename data_T::value_type alpha, hls::stream<res_T> &res) {
-    LeakyReLUActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+LeakyReLUActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        LeakyReLUPackLoop: for (int j = 0; j < res_T::size; j++) {
+    LeakyReLUPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            if (in_data[j] > 0) out_data[j] = in_data[j];
-            else out_data[j] = alpha * in_data[j];
+            if (in_data[j] > 0)
+                out_data[j] = in_data[j];
+            else
+                out_data[j] = alpha * in_data[j];
         }
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       Thresholded RELU Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void thresholded_relu(hls::stream<data_T> &data, typename data_T::value_type theta, hls::stream<res_T> &res) {
-    ThresholdedReLUActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+ThresholdedReLUActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        ThresholdedReLUPackLoop: for (int j = 0; j < res_T::size; j++) {
+    ThresholdedReLUPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            if (in_data[j] > theta) out_data[j] = in_data[j];
-            else out_data[j] = 0;
+            if (in_data[j] > theta)
+                out_data[j] = in_data[j];
+            else
+                out_data[j] = 0;
         }
 
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       Softplus Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
-void softplus(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void softplus(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t softplus_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t softplus_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_softplus_table<CONFIG_T, CONFIG_T::table_size>(softplus_table);
         initialized = true;
     }
 
-    SoftplusActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+SoftplusActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        SoftplusPackLoop: for (int j = 0; j < res_T::size; j++) {
+    SoftplusPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            int data_round = in_data[j]*CONFIG_T::table_size/16;
-            int index = data_round + 8*CONFIG_T::table_size/16;
-            if (index < 0)   index = 0;
-            else if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+            int data_round = in_data[j] * CONFIG_T::table_size / 16;
+            int index = data_round + 8 * CONFIG_T::table_size / 16;
+            if (index < 0)
+                index = 0;
+            else if (index > CONFIG_T::table_size - 1)
+                index = CONFIG_T::table_size - 1;
             out_data[j] = softplus_table[index];
         }
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       Softsign Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
-void softsign(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void softsign(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t softsign_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t softsign_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_softsign_table<CONFIG_T, CONFIG_T::table_size>(softsign_table);
         initialized = true;
     }
 
-    SoftsignActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+SoftsignActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        SoftsignPackLoop: for (int j = 0; j < res_T::size; j++) {
+    SoftsignPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            int data_round = in_data[j]*CONFIG_T::table_size/16;
-            int index = data_round + 8*CONFIG_T::table_size/16;
-            if (index < 0)   index = 0;
-            else if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+            int data_round = in_data[j] * CONFIG_T::table_size / 16;
+            int index = data_round + 8 * CONFIG_T::table_size / 16;
+            if (index < 0)
+                index = 0;
+            else if (index > CONFIG_T::table_size - 1)
+                index = CONFIG_T::table_size - 1;
             out_data[j] = softsign_table[index];
         }
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       ELU Activation
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void elu(hls::stream<data_T> &data, typename data_T::value_type alpha, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t elu_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t elu_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_elu_table<CONFIG_T, CONFIG_T::table_size>(elu_table);
         initialized = true;
     }
 
-    EluActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+EluActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        EluPackLoop: for (int j = 0; j < res_T::size; j++) {
+    EluPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            
+
             typename data_T::value_type datareg = in_data[j];
             if (datareg >= 0) {
                 out_data[j] = datareg;
             } else {
-                int index = datareg*CONFIG_T::table_size/-8;
-                if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+                int index = datareg * CONFIG_T::table_size / -8;
+                if (index > CONFIG_T::table_size - 1)
+                    index = CONFIG_T::table_size - 1;
                 out_data[j] = alpha * elu_table[index];
             }
         }
         res.write(out_data);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void elu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void elu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     elu<data_T, res_T, CONFIG_T>(data, 1.0, res);
 }
 
 // *************************************************
 //       SELU Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
-void selu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void selu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     // Initialize the lookup table
 #ifdef __HLS_SYN__
     bool initialized = false;
     typename CONFIG_T::table_t selu_table[CONFIG_T::table_size];
 #else
     static bool initialized = false;
     static typename CONFIG_T::table_t selu_table[CONFIG_T::table_size];
 #endif
     if (!initialized) {
         init_selu_table<CONFIG_T, CONFIG_T::table_size>(selu_table);
         initialized = true;
     }
 
-    SeluActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+SeluActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
 
-        SeluPackLoop: for (int j = 0; j < res_T::size; j++) {
+    SeluPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
 
             typename data_T::value_type datareg = in_data[j];
             if (datareg >= 0) {
-                out_data[j] = (typename data_T::value_type) 1.0507009873554804934193349852946 * datareg;
+                out_data[j] = (typename data_T::value_type)1.0507009873554804934193349852946 * datareg;
             } else {
-                int index = datareg*CONFIG_T::table_size/-8;
-                if (index > CONFIG_T::table_size-1) index = CONFIG_T::table_size-1;
+                int index = datareg * CONFIG_T::table_size / -8;
+                if (index > CONFIG_T::table_size - 1)
+                    index = CONFIG_T::table_size - 1;
                 out_data[j] = selu_table[index];
             }
         }
         res.write(out_data);
     }
 }
 
-
 // *************************************************
 //       PReLU Activation
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void prelu(hls::stream<data_T> &data, typename data_T::value_type alpha[CONFIG_T::n_in], hls::stream<res_T> &res) {
-    PReLUActLoop: for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+PReLUActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
         #pragma HLS PIPELINE
 
         data_T in_data = data.read();
         res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
+        PRAGMA_DATA_PACK(out_data)
+
+    PReLUPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
+            #pragma HLS UNROLL
+            if (in_data[j] > 0)
+                out_data[j] = in_data[j];
+            else
+                out_data[j] = alpha[i * res_T::size + j] * in_data[j];
+        }
+        res.write(out_data);
+    }
+}
+
+// *************************************************
+//       Binary TanH Activation
+// *************************************************
+template <class data_T, class res_T, typename CONFIG_T>
+void binary_tanh(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+PReLUActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+        #pragma HLS PIPELINE
+
+        data_T in_data = data.read();
+        res_T out_data;
+        PRAGMA_DATA_PACK(out_data)
 
-        PReLUPackLoop: for (int j = 0; j < res_T::size; j++) {
+    PReLUPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
             #pragma HLS UNROLL
-            if (in_data[j] > 0) out_data[j] = in_data[j];
-            else out_data[j] = alpha[i*res_T::size+j] * in_data[j];
+            if (in_data[j] > 0)
+                out_data[j] = (typename res_T::value_type)1;
+            else
+                out_data[j] = (typename res_T::value_type) - 1;
         }
         res.write(out_data);
     }
 }
 
+// *************************************************
+//       Ternary TanH Activation
+// *************************************************
+template <class data_T, class res_T, typename CONFIG_T>
+void ternary_tanh(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+PReLUActLoop:
+    for (int i = 0; i < CONFIG_T::n_in / res_T::size; i++) {
+        #pragma HLS PIPELINE
+
+        data_T in_data = data.read();
+        res_T out_data;
+        PRAGMA_DATA_PACK(out_data)
 
+    PReLUPackLoop:
+        for (int j = 0; j < res_T::size; j++) {
+            #pragma HLS UNROLL
+            if (in_data[j] > 1)
+                out_data[j] = (typename res_T::value_type)1;
+            else if (in_data[j] <= -1)
+                out_data[j] = (typename res_T::value_type) - 1;
+            else
+                out_data[j] = (typename res_T::value_type)0;
+        }
+        res.write(out_data);
+    }
 }
 
-#endif
+} // namespace nnet
+
+#endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_array.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_image_stream.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,56 +1,66 @@
-#ifndef NNET_ARRAY_H_
-#define NNET_ARRAY_H_
-
-#include <math.h>
-
-namespace nnet {
-
-struct transpose_config {
-    static const unsigned height = 10;
-    static const unsigned width = 10;
-    static const unsigned depth = 10;
-    static constexpr unsigned perm[3] = {2, 0, 1};
-};
-
-template<class data_T, typename CONFIG_T>
-void transpose_2d(
-    data_T data[CONFIG_T::height * CONFIG_T::width],
-    data_T data_t[CONFIG_T::height * CONFIG_T::width]
-) {
-    #pragma HLS PIPELINE
-
-    for (int i = 0; i < CONFIG_T::height; i++) {
-        for (int j = 0; j < CONFIG_T::width; j++) {
-            data_t[j * CONFIG_T::height + i] = data[i * CONFIG_T::width + j];
-        }
-    }
-}
-
-template<class data_T, typename CONFIG_T>
-void transpose_3d(
-    data_T data[CONFIG_T::depth * CONFIG_T::height * CONFIG_T::width],
-    data_T data_t[CONFIG_T::depth * CONFIG_T::height * CONFIG_T::width]
-) {
-    unsigned dims[3] = { CONFIG_T::depth, CONFIG_T::height, CONFIG_T::width };
-    unsigned dims_t[3];
-    dims_t[0] = dims[CONFIG_T::perm[0]];
-    dims_t[1] = dims[CONFIG_T::perm[1]];
-    dims_t[2] = dims[CONFIG_T::perm[2]];
-
-    int idx[3] = {0}, idx_t[3] = {0};
-    for (idx[0] = 0; idx[0] < dims[0]; idx[0]++) {
-        for (idx[1] = 0; idx[1] < dims[1]; idx[1]++) {
-            for (idx[2] = 0; idx[2] < dims[2]; idx[2]++) {
-                idx_t[0] = idx[CONFIG_T::perm[0]];
-                idx_t[1] = idx[CONFIG_T::perm[1]];
-                idx_t[2] = idx[CONFIG_T::perm[2]];
-
-                data_t[idx_t[0] * dims_t[1] * dims_t[2] + idx_t[1] * dims_t[2] + idx_t[2]] = data[idx[0] * dims[1] * dims[2] + idx[1] * dims[2] + idx[2]];
-            }
-        }
-    }
-}
-
-}
-
-#endif
+#ifndef NNET_IMAGE_STREAM_H_
+#define NNET_IMAGE_STREAM_H_
+
+#include "hls_stream.h"
+#include "nnet_common.h"
+
+namespace nnet {
+
+template <class data_T, typename CONFIG_T> void resize_nearest(hls::stream<data_T> &image, hls::stream<data_T> &resized) {
+    assert(CONFIG_T::new_height % CONFIG_T::height == 0);
+    assert(CONFIG_T::new_width % CONFIG_T::width == 0);
+    constexpr unsigned ratio_height = CONFIG_T::new_height / CONFIG_T::height;
+    constexpr unsigned ratio_width = CONFIG_T::new_width / CONFIG_T::width;
+
+ImageHeight:
+    for (unsigned h = 0; h < CONFIG_T::height; h++) {
+        #pragma HLS PIPELINE
+
+        data_T data_in_row[CONFIG_T::width];
+
+    ImageWidth:
+        for (unsigned i = 0; i < CONFIG_T::width; i++) {
+            #pragma HLS UNROLL
+
+            data_T in_data = image.read();
+
+        ImageChan:
+            for (unsigned j = 0; j < CONFIG_T::n_chan; j++) {
+                #pragma HLS UNROLL
+
+                data_in_row[i][j] = in_data[j];
+            }
+        }
+
+    ResizeHeight:
+        for (unsigned i = 0; i < ratio_height; i++) {
+            #pragma HLS UNROLL
+
+        ImageWidth2:
+            for (unsigned l = 0; l < CONFIG_T::width; l++) {
+                #pragma HLS UNROLL
+
+            ResizeWidth:
+                for (unsigned j = 0; j < ratio_width; j++) {
+                    #pragma HLS UNROLL
+
+                    data_T out_data;
+                    PRAGMA_DATA_PACK(out_data)
+
+                ResizeChan:
+                    for (unsigned k = 0; k < CONFIG_T::n_chan; k++) {
+                        #pragma HLS UNROLL
+
+                        out_data[k] = data_in_row[l][k];
+                    }
+
+                    resized.write(out_data);
+                }
+            }
+        }
+    }
+}
+
+} // namespace nnet
+
+#endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_common.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_common.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,100 +1,75 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_COMMON_H_
 #define NNET_COMMON_H_
 
 #include "ap_fixed.h"
 
 // This is a substitute for "ceil(n/(float)d)".
-#define DIV_ROUNDUP(n,d) ((n + d - 1) / d)
-#define MIN(n,d) (n > d ? d : n)
-#define MAX(n,d) (n > d ? n : d)
+#define DIV_ROUNDUP(n, d) ((n + d - 1) / d)
+#define MIN(n, d) (n > d ? d : n)
+#define MAX(n, d) (n > d ? n : d)
+
+#define STRINGIFY(x) #x
+#define EXPAND_STRING(x) STRINGIFY(x)
+
+#ifndef __VITIS_HLS__
+#define DATA_PACK_TXT HLS DATA_PACK variable =
+#define DATA_PACK_PRAGMA(variable) DATA_PACK_TXT variable
+#define PRAGMA_DATA_PACK(variable) _Pragma(EXPAND_STRING(DATA_PACK_PRAGMA(variable)))
+#else
+#define PRAGMA_DATA_PACK(variable)
+#endif
 
 namespace nnet {
 
 // Common type definitions
-enum io_type {io_parallel = 0, io_serial, io_stream};
+enum io_type { io_parallel = 0, io_stream };
 enum strategy { latency, resource };
 
- /* ---
-  * Balanced tree reduce implementation.
-  * For use in scenarios where Vivado cannot expression balance
-  * Reduces an array of inputs to a single value using the template binary operator 'Op',
-  * for example summing all elements with Op_add, or finding the maximum with Op_max
-  * Use only when the input array is fully unrolled. Or, slice out a fully unrolled section
-  * before applying and accumulate the result over the rolled dimension.
-  * --- */
- template<class T, int N, class Op>
- T reduce(const T* x, Op op)
- {
-     static constexpr int leftN = pow2(floorlog2(N - 1)) > 0 ? pow2(floorlog2(N - 1)) : 0;
-     static constexpr int rightN = N - leftN > 0 ? N - leftN : 0;
-     if (N == 1){
-         return x[0];
-     }
-     if (N == 2){
-         return op(x[0],x[1]);
-     }
-     return op(reduce<T,leftN,Op>(x, op), reduce<T,rightN,Op>(x+leftN, op));
- } 
-
- template<class T>
- class Op_add{
- public:
-	 T operator()(T a, T b){
-		 return a + b;
-	 }
- };
-
- template<class T>
- class Op_and{
- public:
-	 T operator()(T a, T b){
-		 return a && b;
-	 }
- };
-
- template<class T>
- class Op_or{
- public:
-	 T operator()(T a, T b){
-		 return a || b;
-	 }
- };
-
- template<class T>
- class Op_max{
- public:
-     T operator()(T a, T b){
-        return a >= b ? a : b;
-     }
- };
-
- template<class T>
- class Op_min{
- public:
-     T operator()(T a, T b){
-        return a <= b ? a : b;
-     }
- };
-
+/* ---
+ * Balanced tree reduce implementation.
+ * For use in scenarios where Vivado cannot expression balance
+ * Reduces an array of inputs to a single value using the template binary operator 'Op',
+ * for example summing all elements with Op_add, or finding the maximum with Op_max
+ * Use only when the input array is fully unrolled. Or, slice out a fully unrolled section
+ * before applying and accumulate the result over the rolled dimension.
+ * --- */
+template <class T, int N, class Op> T reduce(const T *x, Op op) {
+    static constexpr int leftN = pow2(floorlog2(N - 1)) > 0 ? pow2(floorlog2(N - 1)) : 0;
+    static constexpr int rightN = N - leftN > 0 ? N - leftN : 0;
+    if (N == 1) {
+        return x[0];
+    }
+    if (N == 2) {
+        return op(x[0], x[1]);
+    }
+    return op(reduce<T, leftN, Op>(x, op), reduce<T, rightN, Op>(x + leftN, op));
 }
 
+template <class T> class Op_add {
+  public:
+    T operator()(T a, T b) { return a + b; }
+};
+
+template <class T> class Op_and {
+  public:
+    T operator()(T a, T b) { return a && b; }
+};
+
+template <class T> class Op_or {
+  public:
+    T operator()(T a, T b) { return a || b; }
+};
+
+template <class T> class Op_max {
+  public:
+    T operator()(T a, T b) { return a >= b ? a : b; }
+};
+
+template <class T> class Op_min {
+  public:
+    T operator()(T a, T b) { return a <= b ? a : b; }
+};
+
+} // namespace nnet
+
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv1d.h` & `hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_conv2d.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,70 +1,72 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
-#ifndef NNET_CONV1D_H_
-#define NNET_CONV1D_H_
-
-#include "nnet_common.h"
-#include "nnet_conv1d_latency.h"
-#include "nnet_conv1d_resource.h"
-#include <cstdlib>
+#ifndef NNET_CONV2D_H_
+#define NNET_CONV2D_H_
+
+#include "nnet_conv2d_resource.h"
 
 namespace nnet {
 
-struct conv1d_config
-{
-    // Internal data type definitions
-    typedef float bias_t;
-    typedef float weight_t;
-    typedef float accum_t;
+struct conv2d_config {
+    // I/O sizes
+    static const unsigned in_height = 10;
+    static const unsigned in_width = 10;
+    static const unsigned out_height = 10;
+    static const unsigned out_width = 10;
 
-    // Convolutional parameters
+    // Number of channels, filters
+    static const unsigned n_chan = 1;
+    static const unsigned n_filt = 1;
+
+    // Original filter size
+    static const unsigned filt_height = 1;
+    static const unsigned filt_width = 1;
+    static const unsigned kernel_size = filt_height * filt_width;
+
+    // Modified filter size (post-Wionograd transformation, if applied)
+    static const unsigned impl_filt_height = 1;
+    static const unsigned impl_filt_width = 1;
+
+    // Padding, stride, dilation
+    static const unsigned pad_top = 0;
+    static const unsigned pad_bottom = 0;
     static const unsigned pad_left = 0;
     static const unsigned pad_right = 0;
-    static const unsigned in_width = 10;
-    static const unsigned n_chan = 0;
-    static const unsigned filt_width = 1;
-    static const unsigned kernel_size = filt_width;
-    static const unsigned n_filt = 1;
+    static const unsigned stride_height = 1;
     static const unsigned stride_width = 1;
-    static const unsigned dilation = 1;
-    static const unsigned out_width = 10; //(N_IN + PAD_LEFT * PAD_RIGHT - (DILATION * (FILT_WIDTH - 1) + 1)) / STRIDE + 1
+    static const unsigned dilation_height = 1;
+    static const unsigned dilation_width = 1;
 
+    // Run-time configuration
+    static const unsigned n_zeros = 0;
     static const unsigned reuse_factor = 1;
+    static const unsigned parallelisation_factor = 1;
+
+    // TODO: BRAM Storage on Quartus
     static const bool store_weights_in_bram = false;
-    static const unsigned n_zeros = 0; // not used yet
+
+    // Internal data type definitions
+    typedef float bias_t;
+    typedef float weight_t;
+    typedef float accum_t;
 };
 
-template<class data_T, class res_T, typename CONFIG_T>
-void conv_1d_cl(
-    data_T data[CONFIG_T::in_width * CONFIG_T::n_chan],
-    res_T  res[CONFIG_T::out_width * CONFIG_T::n_filt],
-    typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
-    if (CONFIG_T::strategy == nnet::latency) {
-        conv_1d_latency_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-    } else {
-        conv_1d_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-    }
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_2d_cl(data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_chan],
+                res_T res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt],
+                const typename CONFIG_T::weight_t
+                    weights[CONFIG_T::impl_filt_height * CONFIG_T::impl_filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                const typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    conv_2d_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+}
+
+template <class data_T, class res_T, typename CONFIG_T>
+void pointwise_conv_2d_cl(data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_chan],
+                          res_T res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt],
+                          const typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
+                          const typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    assert(CONFIG_T::filt_height == 1 && CONFIG_T::filt_width == 1);
+    pointwise_conv_2d_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
 }
 
-}//end namespace
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv1d_stream.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,96 +1,89 @@
 #ifndef NNET_CONV1D_STREAM_H_
 #define NNET_CONV1D_STREAM_H_
 
+#include "hls_stream.h"
 #include "nnet_common.h"
 #include "nnet_conv_stream.h"
-#include "hls_stream.h"
 
 namespace nnet {
 
-template<class data_T, typename CONFIG_T>
-void compute_scaled_indices_1d(
-    const unsigned w_idx,
-    ap_uint<CONFIG_T::filt_width> *pixel_idx
-) {
+template <class data_T, typename CONFIG_T>
+void compute_scaled_indices_1d(const unsigned w_idx, ap_uint<CONFIG_T::filt_width> *pixel_idx) {
     unsigned wp_idx = w_idx * (data_T::size / CONFIG_T::n_chan);
 
-    ComputeIndex: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
+ComputeIndex:
+    for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
         #pragma HLS UNROLL
-
-        unsigned sw_idx = scale_index<CONFIG_T::filt_width, CONFIG_T::stride_width, CONFIG_T::in_width>(wp_idx + p);
+        unsigned sw_idx =
+            CONFIG_T::template scale_index<CONFIG_T::filt_width, CONFIG_T::stride_width, CONFIG_T::in_width>::scale_index(
+                wp_idx + p);
         pixel_idx[p] = CONFIG_T::pixels[sw_idx];
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void conv_1d_encoded_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_1d_encoded_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                        typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                        typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
 
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::filt_width * CONFIG_T::n_chan];
     const int win_depth = CONFIG_T::out_width;
     for (unsigned i_out = 0; i_out < CONFIG_T::filt_width * CONFIG_T::n_chan; i_out++) {
         #pragma HLS STREAM variable=data_window[i_out] depth=win_depth
     }
 
     #pragma HLS ARRAY_PARTITION variable=CONFIG_T::pixels complete
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
     unsigned outputs_ready = 0;
 
     ap_uint<CONFIG_T::filt_width> pixel_idx[data_T::size / CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=pixel_idx complete
 
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
         #pragma HLS LOOP_FLATTEN
         if (CONFIG_T::strategy == nnet::latency && data_T::size / CONFIG_T::n_chan == 1) {
             #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
         }
         compute_scaled_indices_1d<data_T, CONFIG_T>(i_iw, pixel_idx);
-        compute_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready, weights, biases, pixel_idx);
+        compute_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready, weights,
+                                                        biases, pixel_idx);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void conv_1d_buffer_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_1d_buffer_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                       typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                       typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
 
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
         #pragma HLS LOOP_FLATTEN
         if (CONFIG_T::strategy == nnet::latency) {
             #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
         }
         compute_output_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void conv_1d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
-    #pragma HLS inline region
-    switch(CONFIG_T::implementation){
-        case conv_implementation::linebuffer:
-            conv_1d_buffer_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-            break;
-        case conv_implementation::encoded:
-            conv_1d_encoded_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-            break;
-    }  
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_1d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    #pragma HLS inline recursive
+    switch (CONFIG_T::implementation) {
+    case conv_implementation::linebuffer:
+        conv_1d_buffer_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+        break;
+    case conv_implementation::encoded:
+        conv_1d_encoded_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+        break;
+    }
 }
 
-}
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv2d.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vitis/nnet_utils/nnet_conv2d_stream.h`

 * *Files 25% similar despite different names*

```diff
@@ -1,91 +1,82 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
-#ifndef NNET_CONV2D_H_
-#define NNET_CONV2D_H_
+#ifndef NNET_CONV2D_STREAM_H_
+#define NNET_CONV2D_STREAM_H_
 
+#include "ap_shift_reg.h"
+#include "hls_stream.h"
 #include "nnet_common.h"
-#include "nnet_conv2d_latency.h"
-#include "nnet_conv2d_resource.h"
-#include <cstdlib>
+#include "nnet_conv_stream.h"
 
 namespace nnet {
 
-struct conv2d_config
-{
-    // Internal data type definitions
-    typedef float bias_t;
-    typedef float weight_t;
-    typedef float accum_t;
-
-    // Convolutional parameters
-    static const unsigned pad_top = 0;
-    static const unsigned pad_bottom = 0;
-    static const unsigned pad_left = 0;
-    static const unsigned pad_right = 0;
-    static const unsigned in_height = 10;
-    static const unsigned in_width = 10;
-    static const unsigned n_chan = 1;
-    static const unsigned filt_height = 1;
-    static const unsigned filt_width = 1;
-    static const unsigned kernel_size = filt_height * filt_width;
-    static const unsigned n_filt = 1;
-    static const unsigned stride_height = 1;
-    static const unsigned stride_width = 1;
-    static const unsigned out_height = 10;
-    static const unsigned out_width = 10;
-    static const unsigned dilation_height = 1;
-    static const unsigned dilation_width = 1;
-
-    static const unsigned reuse_factor = 1;
-    static const bool store_weights_in_bram = false;
-    static const unsigned n_zeros = 0; // not used yet
-};
-
-template<class data_T, class res_T, typename CONFIG_T>
-void conv_2d_cf(
-    data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_chan],
-    res_T  res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt],
+// Line Buffer
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_2d_buffer_latency_cl(
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
-    if (CONFIG_T::strategy == nnet::latency) {
-        conv_2d_latency_cf<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-    } else {
-        conv_2d_resource_cf<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
+
+    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1, 1)]
+                                                                                    [CONFIG_T::n_chan];
+    #pragma HLS ARRAY_PARTITION variable = line_buffer complete dim = 2
+
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+            #pragma HLS LOOP_FLATTEN
+            #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
+
+            if (CONFIG_T::filt_height > 1) {
+                compute_output_buffer_2d<data_T, res_T, CONFIG_T>(data.read(), line_buffer, res, weights, biases);
+            } else {
+                compute_output_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
+            }
+        }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
+void conv_2d_buffer_resource_cl(
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
+    typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
+
+    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1, 1)]
+                                                                                    [CONFIG_T::n_chan];
+    #pragma HLS ARRAY_PARTITION variable = line_buffer complete dim = 2
+
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+            #pragma HLS LOOP_FLATTEN
+
+            if (CONFIG_T::filt_height > 1) {
+                compute_output_buffer_2d<data_T, res_T, CONFIG_T>(data.read(), line_buffer, res, weights, biases);
+            } else {
+                compute_output_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
+            }
+        }
+    }
+}
+
+template <class data_T, class res_T, typename CONFIG_T>
 void conv_2d_cl(
-    data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_chan],
-    res_T  res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt],
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    assert(CONFIG_T::implementation == conv_implementation::linebuffer &&
+           "Only \"linebuffer\" implementation is supported in Vitis HLS.");
+
+    #pragma HLS INLINE recursive
     if (CONFIG_T::strategy == nnet::latency) {
-        conv_2d_latency_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+        conv_2d_buffer_latency_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     } else {
-        conv_2d_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+        conv_2d_buffer_resource_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     }
 }
 
-}//end namespace
-
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv2d_stream.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,112 +1,112 @@
 #ifndef NNET_CONV2D_STREAM_H_
 #define NNET_CONV2D_STREAM_H_
 
 #include "ap_shift_reg.h"
+#include "hls_stream.h"
 #include "nnet_common.h"
 #include "nnet_conv_stream.h"
-#include "hls_stream.h"
 
 namespace nnet {
 
-template<class data_T, typename CONFIG_T>
-void compute_scaled_indices_2d(
-    const unsigned h_idx,
-    const unsigned w_idx,
-    ap_uint<CONFIG_T::filt_height * CONFIG_T::filt_width> *pixel_idx
-) {
-    const unsigned sh_idx = scale_index<CONFIG_T::filt_height, CONFIG_T::stride_height, CONFIG_T::in_height>(h_idx);
+template <class data_T, typename CONFIG_T>
+void compute_scaled_indices_2d(const unsigned h_idx, const unsigned w_idx,
+                               ap_uint<CONFIG_T::filt_height * CONFIG_T::filt_width> *pixel_idx) {
+    const unsigned sh_idx = CONFIG_T::template scale_index_height<CONFIG_T::filt_height, CONFIG_T::stride_height,
+                                                                  CONFIG_T::in_height>::scale_index(h_idx);
     unsigned wp_idx = w_idx * (data_T::size / CONFIG_T::n_chan);
 
-    ComputeIndex: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
+ComputeIndex:
+    for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
         #pragma HLS UNROLL
 
-        unsigned sw_idx = scale_index<CONFIG_T::filt_width, CONFIG_T::stride_width, CONFIG_T::in_width>(wp_idx + p);
+        unsigned sw_idx = CONFIG_T::template scale_index_width<CONFIG_T::filt_width, CONFIG_T::stride_width,
+                                                               CONFIG_T::in_width>::scale_index(wp_idx + p);
         pixel_idx[p] = CONFIG_T::pixels[sh_idx * CONFIG_T::min_width + sw_idx];
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void conv_2d_encoded_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::filt_height == CONFIG_T::filt_width);
 
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan];
     const int win_depth = CONFIG_T::filt_height * CONFIG_T::out_width;
     for (unsigned i_out = 0; i_out < CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan; i_out++) {
         #pragma HLS STREAM variable=data_window[i_out] depth=win_depth
     }
 
     #pragma HLS ARRAY_PARTITION variable=CONFIG_T::pixels complete
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
     unsigned outputs_ready = 0;
 
     ap_uint<CONFIG_T::filt_height * CONFIG_T::filt_width> pixel_idx[data_T::size / CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=pixel_idx complete
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
             #pragma HLS LOOP_FLATTEN
             if (CONFIG_T::strategy == nnet::latency && data_T::size / CONFIG_T::n_chan == 1) {
                 #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
             }
             compute_scaled_indices_2d<data_T, CONFIG_T>(i_ih, i_iw, pixel_idx);
-            compute_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready, weights, biases, pixel_idx);
+            compute_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready, weights,
+                                                            biases, pixel_idx);
         }
     }
 }
 
 // Line Buffer
 template <class data_T, class res_T, typename CONFIG_T>
 void conv_2d_buffer_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
 
-    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1,1)][CONFIG_T::n_chan];
+    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1, 1)]
+                                                                                    [CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable = line_buffer complete dim = 2
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
             #pragma HLS LOOP_FLATTEN
-            if(CONFIG_T::strategy == nnet::latency) {
+            if (CONFIG_T::strategy == nnet::latency) {
                 #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
             }
             if (CONFIG_T::filt_height > 1) {
                 compute_output_buffer_2d<data_T, res_T, CONFIG_T>(data.read(), line_buffer, res, weights, biases);
             } else {
                 compute_output_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
             }
         }
     }
 }
 
 template <class data_T, class res_T, typename CONFIG_T>
 void conv_2d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
-    #pragma HLS inline region
-    switch(CONFIG_T::implementation){
-        case conv_implementation::linebuffer:
-            conv_2d_buffer_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-            break;
-        case conv_implementation::encoded:
-            conv_2d_encoded_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-            break;
-    }  
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
+    #pragma HLS inline recursive
+    switch (CONFIG_T::implementation) {
+    case conv_implementation::linebuffer:
+        conv_2d_buffer_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+        break;
+    case conv_implementation::encoded:
+        conv_2d_encoded_cl<data_T, res_T, CONFIG_T>(data, res, weights, biases);
+        break;
+    }
 }
 
-}
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_conv_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_conv_stream.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,106 +1,114 @@
 #ifndef NNET_CONV_STREAM_H_
 #define NNET_CONV_STREAM_H_
 
 #include "ap_shift_reg.h"
-#include "nnet_common.h"
 #include "hls_stream.h"
+#include "nnet_common.h"
 #include "nnet_dense.h"
 
 namespace nnet {
 
-enum class conv_implementation { linebuffer=0, encoded=1};
+enum class conv_implementation { linebuffer = 0, encoded = 1 };
 
 // *************************************************
 //       Encoded Implementation (Vlad's)
 // *************************************************
-template<unsigned K, unsigned S, unsigned W>
-unsigned scale_index_K_gte_S(const unsigned idx) {
+template <unsigned K, unsigned S, unsigned W> unsigned scale_index_K_gte_S(const unsigned idx) {
     #pragma HLS INLINE
 
     if (idx < K - S) {
         return idx;
     }
 
-    constexpr unsigned nW = ((W - K) / S) * S + K; // Nearest W without unused pixels on the right
+    constexpr unsigned nW = ((W - K) / S) * S + K;           // Nearest W without unused pixels on the right
     constexpr unsigned sW = (DIV_ROUNDUP(K, S) - 1) * S + K; // Scaled W that behaves like original W
     if (idx >= nW) {
         return sW;
     }
 
     const unsigned r = nW - idx;
     if (r <= K - S) {
         return sW - r;
     }
 
     return K - S + (idx - (K - S)) % S;
 }
 
-template<unsigned K, unsigned S, unsigned W>
-unsigned scale_index_K_lt_S(const unsigned idx) {
+template <unsigned K, unsigned S, unsigned W> unsigned scale_index_K_lt_S(const unsigned idx) {
     #pragma HLS INLINE
 
     if (idx < S - K) {
         return idx;
     }
 
-    constexpr unsigned nW = ((W - K) / S) * S + K; // Nearest W without unused pixels on the right
+    constexpr unsigned nW = ((W - K) / S) * S + K;           // Nearest W without unused pixels on the right
     constexpr unsigned sW = (DIV_ROUNDUP(S, K) - 1) * S + K; // Scaled W that behaves like original W
     if (idx >= nW) {
         return sW;
     }
 
     const unsigned r = nW - idx;
     if (r <= S - K) {
         return sW - r;
     }
 
     return S - K + (idx - (S - K)) % S;
 }
 
-template<unsigned K, unsigned S, unsigned W>
-unsigned scale_index(const unsigned idx) {
-    #pragma HLS INLINE
-    
-    if (K >= S) {
-        return scale_index_K_gte_S<K, S, W>(idx);
-    } else {
-        return scale_index_K_lt_S<K, S, W>(idx);
+template <unsigned K, unsigned S, unsigned W> class scale_index_regular {
+  public:
+    static unsigned scale_index(const unsigned idx) {
+        #pragma HLS INLINE
+
+        if (K >= S) {
+            return scale_index_K_gte_S<K, S, W>(idx);
+        } else {
+            return scale_index_K_lt_S<K, S, W>(idx);
+        }
     }
-}
+};
 
-template<class data_T, class res_T, typename CONFIG_T>
-void mult_buffer(
-    hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    res_T& res_pack,
-    hls::stream<res_T>& res_stream,
-    unsigned & outputs_ready,
-    typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]
-) {
+template <unsigned K, unsigned S, unsigned W> class scale_index_unscaled {
+  public:
+    static unsigned scale_index(const unsigned idx) {
+        #pragma HLS INLINE
+        return idx;
+    }
+};
+
+template <class data_T, class res_T, typename CONFIG_T>
+void mult_buffer(hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                 res_T &res_pack, hls::stream<res_T> &res_stream, unsigned &outputs_ready,
+                 typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                 typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     #pragma HLS INLINE
 
     typename data_T::value_type data[CONFIG_T::kernel_size * CONFIG_T::n_chan];
-    #pragma HLS ARRAY_PARTITION variable=data complete
+    #pragma HLS ARRAY_PARTITION variable = data complete
     typename res_T::value_type res[CONFIG_T::n_filt];
-    #pragma HLS ARRAY_PARTITION variable=res complete
+    #pragma HLS ARRAY_PARTITION variable = res complete
 
-    InitData: for (int id = 0; id < CONFIG_T::kernel_size * CONFIG_T::n_chan; id++) {
+InitData:
+    for (int id = 0; id < CONFIG_T::kernel_size * CONFIG_T::n_chan; id++) {
         #pragma HLS UNROLL
         data[id] = data_window[id].read();
     }
 
-    #pragma HLS INLINE region
+    #pragma HLS INLINE recursive
     if (CONFIG_T::strategy == nnet::latency) {
-        dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(data, res, weights, biases);
+        dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+            data, res, weights, biases);
     } else {
-        dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(data, res, weights, biases);
+        dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+            data, res, weights, biases);
     }
 
-    CastLoop: for (unsigned jj = 0; jj < CONFIG_T::n_filt; jj++) {
+CastLoop:
+    for (unsigned jj = 0; jj < CONFIG_T::n_filt; jj++) {
         #pragma HLS UNROLL
         if (res_T::size / CONFIG_T::n_filt == 1) {
             res_pack[jj] = res[jj];
         } else {
             res_pack[outputs_ready * CONFIG_T::n_filt + jj] = res[jj];
         }
     }
@@ -113,263 +121,276 @@
             outputs_ready = 0;
         } else {
             outputs_ready++;
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_output_encoded(
-    const data_T& in_elem,
-    hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    hls::stream<res_T> &res,
-    res_T &res_pack,
-    unsigned &outputs_ready,
-    typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt],
-    ap_uint<CONFIG_T::kernel_size> *pixel_idx
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_output_encoded(const data_T &in_elem,
+                            hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                            hls::stream<res_T> &res, res_T &res_pack, unsigned &outputs_ready,
+                            typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan * CONFIG_T::n_filt],
+                            typename CONFIG_T::bias_t biases[CONFIG_T::n_filt], ap_uint<CONFIG_T::kernel_size> *pixel_idx) {
     #pragma HLS INLINE
 
-    MultLoop: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
-        CopyDataFilt: for (unsigned f = 0; f < CONFIG_T::kernel_size; f++) {
+MultLoop:
+    for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
+        #pragma HLS PIPELINE II = CONFIG_T::reuse_factor
+    CopyDataFilt:
+        for (unsigned f = 0; f < CONFIG_T::kernel_size; f++) {
             #pragma HLS UNROLL
-            CopyDataChan: for (unsigned c = 0; c < CONFIG_T::n_chan; c++) {
+        CopyDataChan:
+            for (unsigned c = 0; c < CONFIG_T::n_chan; c++) {
                 #pragma HLS UNROLL
-                if (pixel_idx[p][f]) data_window[f * CONFIG_T::n_chan + c].write(in_elem[p * CONFIG_T::n_chan + c]);
+                if (pixel_idx[p][f])
+                    data_window[f * CONFIG_T::n_chan + c].write(in_elem[p * CONFIG_T::n_chan + c]);
             }
         }
         if (pixel_idx[p][CONFIG_T::kernel_size - 1]) {
             mult_buffer<data_T, res_T, CONFIG_T>(data_window, res_pack, res, outputs_ready, weights, biases);
         }
     }
 }
 
-
-
 // *************************************************
 //       Line Buffer Implementation (Phil's)
 // *************************************************
 template <class data_T, typename CONFIG_T>
-void kernel_shift_1d(
-    const data_T& in_elem,
-    typename data_T::value_type kernel_window[CONFIG_T::filt_width * CONFIG_T::n_chan]
-) {
+void kernel_shift_1d(const data_T &in_elem,
+                     typename data_T::value_type kernel_window[CONFIG_T::filt_width * CONFIG_T::n_chan]) {
     #pragma HLS inline
-    #pragma HLS PIPELINE II = 1
-    
+
     // Shift kernel_window by one step to the left (manual shift operation)
     static const int filt_width = CONFIG_T::filt_width - 1;
-    KernelShiftWidth: for (int i_iw = 0; i_iw < filt_width; i_iw++) {
-        #pragma HLS UNROLL
-        KernelShiftChannel: for (unsigned i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+KernelShiftWidth:
+    for (int i_iw = 0; i_iw < filt_width; i_iw++) {
+        #pragma HLS PIPELINE II = 1
+    KernelShiftChannel:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+            #pragma HLS UNROLL
             // Shift every element in kernel_window to the left
             kernel_window[i_iw * CONFIG_T::n_chan + i_ic] = kernel_window[(i_iw + 1) * CONFIG_T::n_chan + i_ic];
         }
     }
 
     // Insert shift_buffer column into right-most column of kernel
     static const int lastheight = (CONFIG_T::filt_width - 1) * CONFIG_T::n_chan;
-    KernelPushChannel: for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+KernelPushChannel:
+    for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
         #pragma HLS UNROLL
         kernel_window[lastheight + i_ic] = in_elem[i_ic];
     }
 }
 
 template <class data_T, typename CONFIG_T>
 void kernel_shift_2d(
     typename data_T::value_type shift_buffer[CONFIG_T::filt_height][CONFIG_T::n_chan],
-    typename data_T::value_type kernel_window[CONFIG_T::filt_width * CONFIG_T::filt_height * CONFIG_T::n_chan]
-) {
+    typename data_T::value_type kernel_window[CONFIG_T::filt_width * CONFIG_T::filt_height * CONFIG_T::n_chan]) {
     #pragma HLS inline
-        
+
     // Shift kernel_window by one step to the left (manual shift operation)
     static const int filt_width = CONFIG_T::filt_width - 1;
-    KernelShiftWidth: for (int i_iw = 0; i_iw < filt_width; i_iw++) {
+KernelShiftWidth:
+    for (int i_iw = 0; i_iw < filt_width; i_iw++) {
         #pragma HLS PIPELINE II = 1
-        KernelShiftHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::filt_height; i_ih++) {
-            KernelShiftChannel: for (unsigned i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
-            // Shift every element in kernel_window to the left
-                kernel_window[i_ih * CONFIG_T::filt_width * CONFIG_T::n_chan + i_iw * CONFIG_T::n_chan + i_ic] = kernel_window[i_ih * CONFIG_T::filt_width * CONFIG_T::n_chan + (i_iw + 1) * CONFIG_T::n_chan + i_ic];
+    KernelShiftHeight:
+        for (unsigned i_ih = 0; i_ih < CONFIG_T::filt_height; i_ih++) {
+        KernelShiftChannel:
+            for (unsigned i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+                // Shift every element in kernel_window to the left
+                kernel_window[i_ih * CONFIG_T::filt_width * CONFIG_T::n_chan + i_iw * CONFIG_T::n_chan + i_ic] =
+                    kernel_window[i_ih * CONFIG_T::filt_width * CONFIG_T::n_chan + (i_iw + 1) * CONFIG_T::n_chan + i_ic];
             }
         }
     }
 
     // Insert shift_buffer column into right-most column of kernel
     static const int lastheight = (CONFIG_T::filt_width - 1) * CONFIG_T::n_chan;
-    KernelPushHeight: for (int i_ih = 0; i_ih < CONFIG_T::filt_height; i_ih++) {
+KernelPushHeight:
+    for (int i_ih = 0; i_ih < CONFIG_T::filt_height; i_ih++) {
         #pragma HLS UNROLL
-        KernelPushChannel: for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+    KernelPushChannel:
+        for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
             kernel_window[lastheight + i_ih * CONFIG_T::filt_width * CONFIG_T::n_chan + i_ic] = shift_buffer[i_ih][i_ic];
         }
     }
 }
 
 template <class data_T, typename CONFIG_T>
-void shift_line_buffer(const data_T& in_elem, 
-                    ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1,1)][CONFIG_T::n_chan],
-                    typename data_T::value_type kernel_window[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan]
-) {
-    
+void shift_line_buffer(
+    const data_T &in_elem,
+    ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1, 1)]
+                                                                             [CONFIG_T::n_chan],
+    typename data_T::value_type kernel_window[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan]) {
+
     #pragma HLS PIPELINE
 
     // Temporary buffer for popped (shifted) elements
     typename data_T::value_type shift_buffer[CONFIG_T::filt_height][CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable = shift_buffer complete dim = 0
 
-    UpdateBuffer: for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+UpdateBuffer:
+    for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
         #pragma HLS UNROLL
 
         // Insert pixel(s) at end of shift buffer
         shift_buffer[CONFIG_T::filt_height - 1][i_ic] = in_elem[i_ic];
     }
 
-    LineBufferDataIn: for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
-        // Shift the shift buffer into the line buffer
-        LineBufferShift: for (unsigned i_ih = 1; i_ih < CONFIG_T::filt_height; i_ih++) {
+LineBufferDataIn:
+    for (int i_ic = 0; i_ic < CONFIG_T::n_chan; i_ic++) {
+    // Shift the shift buffer into the line buffer
+    LineBufferShift:
+        for (unsigned i_ih = 1; i_ih < CONFIG_T::filt_height; i_ih++) {
             #pragma HLS UNROLL
-            typename data_T::value_type pop_elem = line_buffer[i_ih - 1][i_ic].shift(shift_buffer[CONFIG_T::filt_height - i_ih][i_ic]); // Shift the line buffer, return the popped pixel
-            shift_buffer[CONFIG_T::filt_height - i_ih - 1][i_ic] = pop_elem; // Popped element placed back into shift_buffer, one row up.
+            typename data_T::value_type pop_elem = line_buffer[i_ih - 1][i_ic].shift(
+                shift_buffer[CONFIG_T::filt_height - i_ih][i_ic]); // Shift the line buffer, return the popped pixel
+            shift_buffer[CONFIG_T::filt_height - i_ih - 1][i_ic] =
+                pop_elem; // Popped element placed back into shift_buffer, one row up.
         }
     }
     kernel_shift_2d<data_T, CONFIG_T>(shift_buffer, kernel_window);
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void compute_output_buffer_2d(
-    const data_T& in_elem,
-    ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1,1)][CONFIG_T::n_chan],
+    const data_T &in_elem,
+    ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1, 1)]
+                                                                             [CONFIG_T::n_chan],
     hls::stream<res_T> &res_stream,
     typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]
-) {
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     #pragma HLS INLINE
 
     // Thresholds
     const static int lShiftX = CONFIG_T::filt_width - 1;
     const static int lShiftY = CONFIG_T::filt_height - 1;
 
     // Counters
     static int pX = 0; // Pixel X
     static int pY = 0; // Pixel Y
 
     static int sX = 0; // Stride X
     static int sY = 0; // Stride Y
 
     static typename data_T::value_type kernel_data[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan];
-    #pragma HLS ARRAY_PARTITION variable=kernel_data complete
+    #pragma HLS ARRAY_PARTITION variable = kernel_data complete
 
     typename res_T::value_type res_out[CONFIG_T::n_filt];
-    #pragma HLS ARRAY_PARTITION variable=res_out complete dim = 0
+    #pragma HLS ARRAY_PARTITION variable = res_out complete dim = 0
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
 
     // Add pixel to buffer
     nnet::shift_line_buffer<data_T, CONFIG_T>(in_elem, line_buffer, kernel_data);
 
     // Check to see if we have a full kernel
-    if ( (sX - lShiftX) == 0 && (sY - lShiftY) == 0 && pY > lShiftY - 1 && pX > lShiftX - 1) {
-        
+    if ((sX - lShiftX) == 0 && (sY - lShiftY) == 0 && pY > lShiftY - 1 && pX > lShiftX - 1) {
+
         // Dense multiply
-        #pragma HLS INLINE region
+        #pragma HLS INLINE recursive
         if (CONFIG_T::strategy == nnet::latency) {
-            dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(kernel_data, res_out, weights, biases);
+            dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+                kernel_data, res_out, weights, biases);
         } else {
-            dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(kernel_data, res_out, weights, biases);
+            dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+                kernel_data, res_out, weights, biases);
         }
 
-        // Pack output
-        CastLoop: for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
+    // Pack output
+    CastLoop:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
             #pragma HLS UNROLL
             res_pack[i_ic] = res_out[i_ic];
         }
 
         // Write output to stream when output ready
         res_stream.write(res_pack);
     }
 
     // Counter Housekeeping
-    if (pX + 1 == CONFIG_T::in_width)  // Includes padding, end of line (padded)
+    if (pX + 1 == CONFIG_T::in_width) // Includes padding, end of line (padded)
     {
-        pX = 0; 
+        pX = 0;
         sX = 0;
-        if (pY + 1 == CONFIG_T::in_height) {  // Reached bottom of image
-            pY = 0; 
+        if (pY + 1 == CONFIG_T::in_height) { // Reached bottom of image
+            pY = 0;
             sY = 0;
         } else {
             pY = pY + 1;
             // Update stride (threshold) ? subtract stride : increment stride
-            sY = ((sY - lShiftY) == 0) ? sY - CONFIG_T::stride_height + 1 : sY + 1; 
+            sY = ((sY - lShiftY) == 0) ? sY - CONFIG_T::stride_height + 1 : sY + 1;
         }
     } else {
         pX = pX + 1;
         // Update stride (threshold) ? subtract stride : increment stride
-        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1; 
+        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1;
     }
 }
 
 // Conv 1D compute output
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void compute_output_buffer_1d(
-    const data_T& in_elem,
-    hls::stream<res_T> &res_stream,
+    const data_T &in_elem, hls::stream<res_T> &res_stream,
     typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]
-) {
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     #pragma HLS INLINE
 
     // Thresholds
     const static int lShiftX = CONFIG_T::filt_width - 1;
 
     // Counters
     static int pX = 0; // pixel counter
     static int sX = 0; // stride counter
 
     static typename data_T::value_type kernel_data[CONFIG_T::filt_width * CONFIG_T::n_chan];
-    #pragma HLS ARRAY_PARTITION variable=kernel_data complete
+    #pragma HLS ARRAY_PARTITION variable = kernel_data complete
 
     typename res_T::value_type res_out[CONFIG_T::n_filt];
-    #pragma HLS ARRAY_PARTITION variable=res_out complete dim = 0
+    #pragma HLS ARRAY_PARTITION variable = res_out complete dim = 0
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
 
     // Add pixel to buffer
     nnet::kernel_shift_1d<data_T, CONFIG_T>(in_elem, kernel_data);
 
     // Check to see if we have a full kernel
-    if ( (sX - lShiftX) == 0 && pX > lShiftX - 1 ) {
-        
+    if ((sX - lShiftX) == 0 && pX > lShiftX - 1) {
+
         // Dense multiply
-        #pragma HLS INLINE region
+        #pragma HLS INLINE recursive
         if (CONFIG_T::strategy == nnet::latency) {
-            dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(kernel_data, res_out, weights, biases);
+            dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+                kernel_data, res_out, weights, biases);
         } else {
-            dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(kernel_data, res_out, weights, biases);
+            dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+                kernel_data, res_out, weights, biases);
         }
 
-        // Pack output
-        CastLoop: for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
+    // Pack output
+    CastLoop:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
             #pragma HLS UNROLL
             res_pack[i_ic] = res_out[i_ic];
         }
 
         // Write output to stream when output ready
         res_stream.write(res_pack);
     }
 
     // Counter Housekeeping
-    if (pX + 1 == CONFIG_T::in_width)  // Includes padding, end of line (padded)
+    if (pX + 1 == CONFIG_T::in_width) // Includes padding, end of line (padded)
     {
         pX = 0;
         sX = 0;
     } else {
         pX = pX + 1;
         // Update stride (threshold) ? subtract stride : increment stride
-        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1; 
+        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1;
     }
 }
 
-}
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,54 +1,49 @@
 #ifndef NNET_DENSE_H_
 #define NNET_DENSE_H_
 
+#include "hls_stream.h"
 #include "nnet_common.h"
-#include "nnet_mult.h"
 #include "nnet_dense_latency.h"
 #include "nnet_dense_resource.h"
 #include "nnet_helpers.h"
-#include "hls_stream.h"
+#include "nnet_mult.h"
 #include <math.h>
 
 namespace nnet {
 
-struct dense_config
-{
+struct dense_config {
     // Internal data type definitions
     typedef float bias_t;
     typedef float weight_t;
     typedef float accum_t;
 
     // Layer Sizes
     static const unsigned n_in = 10;
     static const unsigned n_out = 10;
 
     // Resource reuse info
     static const unsigned io_type = io_parallel;
-    static const unsigned strategy = latency; 
+    static const unsigned strategy = latency;
     static const unsigned reuse_factor = 1;
     static const bool store_weights_in_bram = false;
     static const unsigned n_zeros = 0;
     // partitioning arrays cyclically to go with roll factors?
     // Product function to use
-    template<class x_T, class y_T, class res_T>
-    using product = nnet::product::mult<x_T, y_T, res_T>;
+    template <class x_T, class y_T> using product = nnet::product::mult<x_T, y_T>;
 };
 
-template<class data_T, class res_T, typename CONFIG_T>
-void dense(
-    data_T    data[CONFIG_T::n_in],
-    res_T     res[CONFIG_T::n_out],
-    typename CONFIG_T::weight_t  weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t    biases[CONFIG_T::n_out])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void dense(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_out],
+           typename CONFIG_T::weight_t weights[CONFIG_T::n_in * CONFIG_T::n_out],
+           typename CONFIG_T::bias_t biases[CONFIG_T::n_out]) {
     #pragma HLS inline
     if (CONFIG_T::strategy == nnet::latency) {
         dense_latency<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     } else {
         dense_resource<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense_resource.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_dense_resource.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,83 +1,63 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_DENSE_RESOURCE_H_
 #define NNET_DENSE_RESOURCE_H_
 
+#include "hls_stream.h"
 #include "nnet_common.h"
 #include "nnet_mult.h"
-#include "hls_stream.h"
-#include <math.h>
 #include <assert.h>
+#include <math.h>
 
 namespace nnet {
 
-template<class data_T, class res_T, typename CONFIG_T>
-void dense_resource_rf_leq_nin(
-    data_T data[CONFIG_T::n_in],
-    res_T  res[CONFIG_T::n_out],
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_out]) {
+template <class data_T, class res_T, typename CONFIG_T>
+void dense_resource_rf_leq_nin(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_out],
+                               typename CONFIG_T::weight_t weights[CONFIG_T::n_in * CONFIG_T::n_out],
+                               typename CONFIG_T::bias_t biases[CONFIG_T::n_out]) {
 
     const int rufactor = CONFIG_T::reuse_factor;
-    const int multfactor = MIN(CONFIG_T::n_in,CONFIG_T::reuse_factor);
-    const int multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in*CONFIG_T::n_out, multfactor);
-    const int block_factor = DIV_ROUNDUP(CONFIG_T::n_in*CONFIG_T::n_out, CONFIG_T::reuse_factor);
-    const int multscale = multiplier_limit/CONFIG_T::n_out;
+    const int multfactor = MIN(CONFIG_T::n_in, CONFIG_T::reuse_factor);
+    const int multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in * CONFIG_T::n_out, multfactor);
+    const int block_factor = DIV_ROUNDUP(CONFIG_T::n_in * CONFIG_T::n_out, CONFIG_T::reuse_factor);
+    const int multscale = multiplier_limit / CONFIG_T::n_out;
     const int nin = CONFIG_T::n_in;
     const int nout = CONFIG_T::n_out;
 
     assert((multiplier_limit % nout == 0 || rufactor >= nin) && "The current Reuse Factor is not allowed");
     assert((multiplier_limit == block_factor) && "This function is correct only for RF <= N_IN");
 
     #pragma HLS function_instantiate variable=weights,biases
     //#pragma HLS RESOURCE variable=weights core=RAM_2P_BRAM Commenting out the deisgnation HLS seems to choose correctly
     #pragma HLS ARRAY_RESHAPE   variable=weights block factor=block_factor
     #pragma HLS ARRAY_PARTITION variable=biases complete
 
     typename CONFIG_T::accum_t acc[CONFIG_T::n_out];
     #pragma HLS ARRAY_PARTITION variable=acc complete
 
-    InitAccum:
+InitAccum:
     for (int iacc = 0; iacc < nout; iacc++) {
         #pragma HLS UNROLL
-        acc[iacc] = (typename CONFIG_T::accum_t) biases[iacc];
+        acc[iacc] = (typename CONFIG_T::accum_t)biases[iacc];
     }
 
-    ReuseLoop:
+ReuseLoop:
     for (int ir = 0; ir < rufactor; ir++) {
         #pragma HLS PIPELINE II=1 rewind
 
         int w_index = ir;
         int in_index = ir;
         int out_index = 0;
         int acc_step = 0;
 
-        MultLoop:
+    MultLoop:
         for (int im = 0; im < block_factor; im++) {
             #pragma HLS UNROLL
 
-            acc[out_index] += CONFIG_T::template product<data_T, typename CONFIG_T::weight_t, typename CONFIG_T::accum_t>::product(data[in_index], weights[w_index]);
+            acc[out_index] += static_cast<typename CONFIG_T::accum_t>(
+                CONFIG_T::template product<data_T, typename CONFIG_T::weight_t>::product(data[in_index], weights[w_index]));
 
             // Increment w_index
             w_index += rufactor;
             // Increment in_index
             in_index += rufactor;
             if (in_index >= nin) {
                 in_index = ir;
@@ -88,197 +68,196 @@
                 out_index++;
             } else {
                 acc_step++;
             }
         }
     }
 
-    // Cast to "res_t" type
-    Result:
+// Cast to "res_t" type
+Result:
     for (int ires = 0; ires < CONFIG_T::n_out; ires++) {
         #pragma HLS UNROLL
         res[ires] = cast<data_T, res_T, CONFIG_T>(acc[ires]);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void dense_resource_rf_gt_nin_rem0(
-    data_T data[CONFIG_T::n_in],
-    res_T  res[CONFIG_T::n_out],
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_out]) {
+template <class data_T, class res_T, typename CONFIG_T>
+void dense_resource_rf_gt_nin_rem0(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_out],
+                                   typename CONFIG_T::weight_t weights[CONFIG_T::n_in * CONFIG_T::n_out],
+                                   typename CONFIG_T::bias_t biases[CONFIG_T::n_out]) {
 
     const int rufactor = MIN(CONFIG_T::reuse_factor, CONFIG_T::n_in * CONFIG_T::n_out);
-    const int multfactor = MIN(CONFIG_T::n_in,CONFIG_T::reuse_factor);
-    const int multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in*CONFIG_T::n_out, multfactor);
-    const int block_factor = DIV_ROUNDUP(CONFIG_T::n_in*CONFIG_T::n_out, CONFIG_T::reuse_factor);
-    const int multscale = multiplier_limit/CONFIG_T::n_out;
+    const int multfactor = MIN(CONFIG_T::n_in, CONFIG_T::reuse_factor);
+    const int multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in * CONFIG_T::n_out, multfactor);
+    const int block_factor = DIV_ROUNDUP(CONFIG_T::n_in * CONFIG_T::n_out, CONFIG_T::reuse_factor);
+    const int multscale = multiplier_limit / CONFIG_T::n_out;
     const int nin = CONFIG_T::n_in;
     const int nout = CONFIG_T::n_out;
 
     assert((multiplier_limit % nout == 0 || rufactor >= nin) && "The current Reuse Factor is not allowed");
     assert((rufactor > nin && rufactor % nin == 0) && "This function is correct only for RF > N_IN && RF % N_IN == 0");
 
     #pragma HLS function_instantiate variable=weights,biases
     //#pragma HLS RESOURCE variable=weights core=RAM_2P_BRAM Commenting out the deisgnation HLS seems to choose correctly
     #pragma HLS ARRAY_RESHAPE   variable=weights block factor=block_factor
     #pragma HLS ARRAY_PARTITION variable=biases complete
 
     typename CONFIG_T::accum_t acc[CONFIG_T::n_out];
     #pragma HLS ARRAY_PARTITION variable=acc complete
 
-    InitAccum:
+InitAccum:
     for (int iacc = 0; iacc < nout; iacc++) {
         #pragma HLS UNROLL
-        acc[iacc] = (typename CONFIG_T::accum_t) biases[iacc];
+        acc[iacc] = (typename CONFIG_T::accum_t)biases[iacc];
     }
 
     int w_index;
     int in_index = 0;
     int out_index;
     int outstep = 0;
     const int outscale = rufactor / nin;
 
     int outidx[rufactor];
-    IndexLoop:
+IndexLoop:
     for (int ir = 0; ir < rufactor; ir++) {
         outidx[ir] = outstep;
         if ((ir + 1) % nin == 0) {
             outstep++;
         }
     }
 
-    ReuseLoop:
+ReuseLoop:
     for (int ir = 0; ir < rufactor; ir++) {
         #pragma HLS PIPELINE II=1 rewind
 
         w_index = ir;
-        out_index = outidx[ir]/*outstep*/;
+        out_index = outidx[ir] /*outstep*/;
 
-        MultLoop:
+    MultLoop:
         for (int im = 0; im < block_factor; im++) {
             #pragma HLS UNROLL
-            acc[out_index] += CONFIG_T::template product<data_T, typename CONFIG_T::weight_t, typename CONFIG_T::accum_t>::product(data[in_index], weights[w_index]);
+            acc[out_index] += static_cast<typename CONFIG_T::accum_t>(
+                CONFIG_T::template product<data_T, typename CONFIG_T::weight_t>::product(data[in_index], weights[w_index]));
 
             w_index += rufactor;
-            if (w_index >= CONFIG_T::n_in * CONFIG_T::n_out) break; // check out of bounds
+            if (w_index >= CONFIG_T::n_in * CONFIG_T::n_out)
+                break; // check out of bounds
             out_index += outscale;
         }
 
         in_index++;
         if (in_index >= nin) {
             in_index = 0;
-            //outstep++; // This causes a huge increase in scheduling and RTL generation times, hence the above workaround.
+            // outstep++; // This causes a huge increase in scheduling and RTL generation times, hence the above workaround.
         }
     }
 
-    // Cast to "res_t" type
-    Result:
+// Cast to "res_t" type
+Result:
     for (int ires = 0; ires < CONFIG_T::n_out; ires++) {
         #pragma HLS UNROLL
         res[ires] = cast<data_T, res_T, CONFIG_T>(acc[ires]);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void dense_resource_rf_gt_nin(
-    data_T data[CONFIG_T::n_in],
-    res_T  res[CONFIG_T::n_out],
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_out]) {
+template <class data_T, class res_T, typename CONFIG_T>
+void dense_resource_rf_gt_nin(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_out],
+                              typename CONFIG_T::weight_t weights[CONFIG_T::n_in * CONFIG_T::n_out],
+                              typename CONFIG_T::bias_t biases[CONFIG_T::n_out]) {
 
     const int rufactor = CONFIG_T::reuse_factor;
-    const int multfactor = MIN(CONFIG_T::n_in,CONFIG_T::reuse_factor);
-    const int multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in*CONFIG_T::n_out, multfactor);
-    const int block_factor = DIV_ROUNDUP(CONFIG_T::n_in*CONFIG_T::n_out, CONFIG_T::reuse_factor);
-    const int multscale = multiplier_limit/CONFIG_T::n_out;
+    const int multfactor = MIN(CONFIG_T::n_in, CONFIG_T::reuse_factor);
+    const int multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in * CONFIG_T::n_out, multfactor);
+    const int block_factor = DIV_ROUNDUP(CONFIG_T::n_in * CONFIG_T::n_out, CONFIG_T::reuse_factor);
+    const int multscale = multiplier_limit / CONFIG_T::n_out;
     const int nin = CONFIG_T::n_in;
     const int nout = CONFIG_T::n_out;
 
     assert((multiplier_limit % nout == 0 || rufactor >= nin) && "The current Reuse Factor is not allowed");
     assert((rufactor > nin) && "This function is correct only for RF > N_IN");
 
     #pragma HLS function_instantiate variable=weights,biases
     //#pragma HLS RESOURCE variable=weights core=RAM_2P_BRAM Commenting out the deisgnation HLS seems to choose correctly
     #pragma HLS ARRAY_RESHAPE   variable=weights block factor=block_factor
     #pragma HLS ARRAY_PARTITION variable=biases complete
 
     typename CONFIG_T::accum_t acc[CONFIG_T::n_out];
     #pragma HLS ARRAY_PARTITION variable=acc complete
 
-    InitAccum:
+InitAccum:
     for (int iacc = 0; iacc < nout; iacc++) {
         #pragma HLS UNROLL
-        acc[iacc] = (typename CONFIG_T::accum_t) biases[iacc];
+        acc[iacc] = (typename CONFIG_T::accum_t)biases[iacc];
     }
 
-    ReuseLoop:
+ReuseLoop:
     for (int ir = 0; ir < rufactor; ir++) {
         #pragma HLS PIPELINE II=1 rewind
         typename CONFIG_T::accum_t tmpmult[block_factor];
         #pragma HLS ARRAY_PARTITION variable=tmpmult complete
 
-        MultLoop:
+    MultLoop:
         for (int im = 0; im < block_factor; im++) {
             #pragma HLS UNROLL
             int w_index = ir + rufactor * im;
             int in_index = w_index % nin;
-            if (w_index >= CONFIG_T::n_in*CONFIG_T::n_out) continue; // check out of bounds
-            tmpmult[im] = CONFIG_T::template product<data_T, typename CONFIG_T::weight_t, typename CONFIG_T::accum_t>::product(data[in_index], weights[w_index]);
+            if (w_index >= CONFIG_T::n_in * CONFIG_T::n_out)
+                continue; // check out of bounds
+            tmpmult[im] =
+                CONFIG_T::template product<data_T, typename CONFIG_T::weight_t>::product(data[in_index], weights[w_index]);
         }
 
         typename CONFIG_T::accum_t mult[multiplier_limit];
         #pragma HLS ARRAY_PARTITION variable=mult complete
 
-        ResetMult:
+    ResetMult:
         for (int imult = 0; imult < multiplier_limit; imult++) {
             #pragma HLS UNROLL
             mult[imult] = 0;
         }
 
-        AccumLoop1:
+    AccumLoop1:
         for (int im = 0; im < block_factor; im++) {
             #pragma HLS UNROLL
             int w_index = ir + rufactor * im;
             int out_index = w_index / multfactor;
-            if (out_index >= multiplier_limit) continue; // check out of bounds
+            if (out_index >= multiplier_limit)
+                continue; // check out of bounds
             mult[out_index] += tmpmult[im];
         }
 
-        AccumLoop2:
+    AccumLoop2:
         for (int im = 0; im < multiplier_limit; im++) {
             #pragma HLS UNROLL
-            //int out_index = im/multscale; // This is the general case
-            //acc[out_index] += mult[im];
+            // int out_index = im/multscale; // This is the general case
+            // acc[out_index] += mult[im];
             acc[im] += mult[im]; // If RF > N_IN then multiplier_limit == n_out
         }
     }
 
-    // Cast to "res_t" type
-    Result:
+// Cast to "res_t" type
+Result:
     for (int ires = 0; ires < CONFIG_T::n_out; ires++) {
         #pragma HLS UNROLL
         res[ires] = cast<data_T, res_T, CONFIG_T>(acc[ires]);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void dense_resource(
-    data_T data[CONFIG_T::n_in],
-    res_T  res[CONFIG_T::n_out],
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_out]) {
+template <class data_T, class res_T, typename CONFIG_T>
+void dense_resource(data_T data[CONFIG_T::n_in], res_T res[CONFIG_T::n_out],
+                    typename CONFIG_T::weight_t weights[CONFIG_T::n_in * CONFIG_T::n_out],
+                    typename CONFIG_T::bias_t biases[CONFIG_T::n_out]) {
 
-    #pragma HLS INLINE region
+    #pragma HLS INLINE recursive
 
     if (CONFIG_T::reuse_factor <= CONFIG_T::n_in) {
         dense_resource_rf_leq_nin<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     } else if (CONFIG_T::reuse_factor % CONFIG_T::n_in == 0) {
         dense_resource_rf_gt_nin_rem0<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     } else {
         dense_resource_rf_gt_nin<data_T, res_T, CONFIG_T>(data, res, weights, biases);
     }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_dense_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_dense_stream.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,70 +1,46 @@
 #ifndef NNET_DENSE_STREAM_H_
 #define NNET_DENSE_STREAM_H_
 
 #include "nnet_common.h"
+#include "nnet_dense.h"
 #include "nnet_types.h"
-#include "hls_stream.h"
-#include <math.h>
-#include <assert.h>
 
 namespace nnet {
 
-template<class data_T, class res_T, typename CONFIG_T>
-void dense_wrapper(
-    data_T data[CONFIG_T::n_in],
-    res_T  res[CONFIG_T::n_out],
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_out]
-) {
-    #pragma HLS INLINE region
-    if (CONFIG_T::strategy == nnet::latency) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
-        dense_latency<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-    } else {
-        dense_resource<data_T, res_T, CONFIG_T>(data, res, weights, biases);
-    }
-}
-
-template<class data_T, class res_T, typename CONFIG_T>
-void dense(
-    hls::stream<data_T> &data_stream,
-    hls::stream<res_T>  &res_stream,
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_in*CONFIG_T::n_out],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_out])
-{
-    typename data_T::value_type data[CONFIG_T::n_in];
-    #pragma HLS ARRAY_PARTITION variable=data complete
-
-    typename res_T::value_type res[CONFIG_T::n_out];
-    #pragma HLS ARRAY_PARTITION variable=res complete
-
-    DataPrepare: for(int i_in = 0; i_in < CONFIG_T::n_in / data_T::size; i_in++) {
-        if (CONFIG_T::n_in / data_T::size > 1) {
-            #pragma HLS PIPELINE
-        }
+template <class data_T, class res_T, typename CONFIG_T>
+void dense_resource(stream<data_T> &data_stream, stream<res_T> &res_stream,
+                    const typename CONFIG_T::weight_t weights[CONFIG_T::n_in * CONFIG_T::n_out],
+                    const typename CONFIG_T::bias_t biases[CONFIG_T::n_out]) {
+    hls_register typename data_T::value_type data[CONFIG_T::n_in];
+    hls_register typename res_T::value_type res[CONFIG_T::n_out];
+
+DataPrepare:
+    #pragma ii 1
+    for (int i_in = 0; i_in < CONFIG_T::n_in / data_T::size; i_in++) {
         data_T data_pack = data_stream.read();
-        DataPack: for (int i_pack = 0; i_pack < data_T::size; i_pack++) {
-            #pragma HLS UNROLL
+    DataPack:
+        #pragma unroll
+        for (int i_pack = 0; i_pack < data_T::size; i_pack++) {
             data[i_in * data_T::size + i_pack] = data_pack[i_pack];
         }
     }
 
-    dense_wrapper<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(data, res, weights, biases);
+    dense_resource<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(data, res, weights, biases);
 
-    ResWrite: for(unsigned i_out = 0; i_out < CONFIG_T::n_out / res_T::size; i_out++) {
-        if (CONFIG_T::n_out / res_T::size > 1) {
-            #pragma HLS PIPELINE
-        }
+ResWrite:
+    #pragma ii 1
+    for (unsigned i_out = 0; i_out < CONFIG_T::n_out / res_T::size; i_out++) {
         res_T res_pack;
-        #pragma HLS DATA_PACK variable=res_pack
-        ResPack: for (int i_pack = 0; i_pack < res_T::size; i_pack++) {
-            #pragma HLS UNROLL
+    ResPack:
+        #pragma unroll
+        for (int i_pack = 0; i_pack < res_T::size; i_pack++) {
             res_pack[i_pack] = res[i_out * res_T::size + i_pack];
         }
+
         res_stream.write(res_pack);
     }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_garnet.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_garnet.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,663 +1,566 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_GARNET_H_
 #define NNET_GARNET_H_
 
-#include "nnet_common.h"
-#include "hls_stream.h"
 #include "hls_math.h"
+#include "hls_stream.h"
+#include "nnet_common.h"
 
 namespace nnet {
-  namespace garnet_utils {
+namespace garnet_utils {
 
-    template<class CONFIG_T>
-    inline
-    typename std::enable_if<std::is_class<typename CONFIG_T::distance_t>::value>::type
-    initialize_edge_weights_table(typename CONFIG_T::edge_weight_t edge_weights_table[])
-    {
-      typedef ap_uint<CONFIG_T::distance_width> index_t;
+template <class CONFIG_T>
+inline typename std::enable_if<std::is_class<typename CONFIG_T::distance_t>::value>::type
+initialize_edge_weights_table(typename CONFIG_T::edge_weight_t edge_weights_table[]) {
+    typedef ap_uint<CONFIG_T::distance_width> index_t;
 
-      unsigned const table_size = (1 << CONFIG_T::distance_width);
+    unsigned const table_size = (1 << CONFIG_T::distance_width);
 
-      index_t index;
-      typename CONFIG_T::distance_t distance;
+    index_t index;
+    typename CONFIG_T::distance_t distance;
 
-      // edge_weight_t is ap_ufixed with 0 iwidth -> let index 0 be a saturated version of 1
-      edge_weights_table[0] = ap_ufixed<CONFIG_T::edge_weight_t::width, 0, AP_TRN, AP_SAT>(1.);
+    // edge_weight_t is ap_ufixed with 0 iwidth -> let index 0 be a saturated version of 1
+    edge_weights_table[0] = ap_ufixed<CONFIG_T::edge_weight_t::width, 0, AP_TRN, AP_SAT>(1.);
 
-      for (unsigned iw = 1; iw < table_size; ++iw) {
+    for (unsigned iw = 1; iw < table_size; ++iw) {
         index = iw;
         distance.range(CONFIG_T::distance_width - 1, 0) = index.range(CONFIG_T::distance_width - 1, 0);
         edge_weights_table[iw] = hls::exp(-distance * distance);
-      }
     }
+}
 
-    template<class CONFIG_T>
-    inline
-    typename std::enable_if<not std::is_class<typename CONFIG_T::distance_t>::value>::type
-    initialize_edge_weights_table(typename CONFIG_T::edge_weight_t edge_weights_table[])
-    {
-      unsigned const table_size = (1 << CONFIG_T::distance_width);
-      double const step = 64. / table_size;
+template <class CONFIG_T>
+inline typename std::enable_if<not std::is_class<typename CONFIG_T::distance_t>::value>::type
+initialize_edge_weights_table(typename CONFIG_T::edge_weight_t edge_weights_table[]) {
+    unsigned const table_size = (1 << CONFIG_T::distance_width);
+    double const step = 64. / table_size;
 
-      typename CONFIG_T::distance_t v = -32.;
-      for (unsigned iw = 0; iw < table_size; ++iw) {
+    typename CONFIG_T::distance_t v = -32.;
+    for (unsigned iw = 0; iw < table_size; ++iw) {
         edge_weights_table[iw] = std::exp(-v * v);
         v += step;
-      }
     }
+}
 
-    template<class CONFIG_T>
-    inline
-    typename std::enable_if<std::is_class<typename CONFIG_T::distance_t>::value, typename CONFIG_T::edge_weight_t>::type
-    get_edge_weight(typename CONFIG_T::distance_t distance, typename CONFIG_T::edge_weight_t edge_weights_table[])
-    {
-      typedef ap_uint<CONFIG_T::distance_width> index_t;
+template <class CONFIG_T>
+inline typename std::enable_if<std::is_class<typename CONFIG_T::distance_t>::value, typename CONFIG_T::edge_weight_t>::type
+get_edge_weight(typename CONFIG_T::distance_t distance, typename CONFIG_T::edge_weight_t edge_weights_table[]) {
+    typedef ap_uint<CONFIG_T::distance_width> index_t;
 
-      index_t index(distance.range(CONFIG_T::distance_width - 1, 0));
+    index_t index(distance.range(CONFIG_T::distance_width - 1, 0));
 
-      return edge_weights_table[index];
-    }
+    return edge_weights_table[index];
+}
 
-    template<class CONFIG_T>
-    inline
+template <class CONFIG_T>
+inline
     typename std::enable_if<not std::is_class<typename CONFIG_T::distance_t>::value, typename CONFIG_T::edge_weight_t>::type
-    get_edge_weight(typename CONFIG_T::distance_t distance, typename CONFIG_T::edge_weight_t edge_weights_table[])
-    {
-      unsigned const table_size = (1 << CONFIG_T::distance_width);
-      double const step = 64. / table_size;
+    get_edge_weight(typename CONFIG_T::distance_t distance, typename CONFIG_T::edge_weight_t edge_weights_table[]) {
+    unsigned const table_size = (1 << CONFIG_T::distance_width);
+    double const step = 64. / table_size;
 
-      int index = (distance + 32.) / step;
-      if (index < 0)
+    int index = (distance + 32.) / step;
+    if (index < 0)
         index = 0;
-      else if (index >= table_size)
+    else if (index >= table_size)
         index = table_size - 1;
 
-      return edge_weights_table[index];
-    }
+    return edge_weights_table[index];
+}
 
-    template<class CONFIG_T>
-    typename CONFIG_T::edge_weight_t
-    compute_edge_weight(typename CONFIG_T::distance_t distance)
-    {
-      if (CONFIG_T::is_stack) {
+template <class CONFIG_T> typename CONFIG_T::edge_weight_t compute_edge_weight(typename CONFIG_T::distance_t distance) {
+    if (CONFIG_T::is_stack) {
         #pragma HLS INLINE OFF
-      }
+    }
 #ifdef __SYNTHESIS__
-      typename CONFIG_T::edge_weight_t edge_weights_table[1 << CONFIG_T::distance_width];
-      // unsigned const reshape_factor = CONFIG_T::n_aggregators * CONFIG_T::n_in_features * (CONFIG_T::n_vertices / CONFIG_T::reuse_factor);
-      // #pragma HLS ARRAY_RESHAPE variable=edge_weights_table cyclic factor=reshape_factor dim=1
-      bool initialized = false;
+    typename CONFIG_T::edge_weight_t edge_weights_table[1 << CONFIG_T::distance_width];
+    // unsigned const reshape_factor = CONFIG_T::n_aggregators * CONFIG_T::n_in_features * (CONFIG_T::n_vertices /
+    // CONFIG_T::reuse_factor);
+    // #pragma HLS ARRAY_RESHAPE variable=edge_weights_table cyclic factor=reshape_factor dim=1
+    bool initialized = false;
 #else
-      static typename CONFIG_T::edge_weight_t edge_weights_table[1 << CONFIG_T::distance_width];
-      static bool initialized = false;
+    static typename CONFIG_T::edge_weight_t edge_weights_table[1 << CONFIG_T::distance_width];
+    static bool initialized = false;
 #endif
-      if (not initialized) {
+    if (not initialized) {
         initialize_edge_weights_table<CONFIG_T>(edge_weights_table);
         initialized = true;
-      }
-
-      return get_edge_weight<CONFIG_T>(distance, edge_weights_table);
     }
 
-    template<class dividend_T, class exponent_T>
-    inline
-    typename std::enable_if<std::is_class<dividend_T>::value, dividend_T>::type
-    normalize_log2(dividend_T dividend, exponent_T exponent)
-    {
-      #pragma HLS INLINE
-      return dividend >> exponent;
-    }
+    return get_edge_weight<CONFIG_T>(distance, edge_weights_table);
+}
 
-    template<class dividend_T, class exponent_T>
-    inline
-    typename std::enable_if<not std::is_class<dividend_T>::value, dividend_T>::type
-    normalize_log2(dividend_T dividend, exponent_T exponent)
-    {
-      #pragma HLS INLINE
-      return dividend / std::pow(2., exponent);
-    }
+template <class dividend_T, class exponent_T>
+inline typename std::enable_if<std::is_class<dividend_T>::value, dividend_T>::type normalize_log2(dividend_T dividend,
+                                                                                                  exponent_T exponent) {
+    #pragma HLS INLINE
+    return dividend >> exponent;
+}
+
+template <class dividend_T, class exponent_T>
+inline typename std::enable_if<not std::is_class<dividend_T>::value, dividend_T>::type normalize_log2(dividend_T dividend,
+                                                                                                      exponent_T exponent) {
+    #pragma HLS INLINE
+    return dividend / std::pow(2., exponent);
+}
+
+template <class CONFIG_T, class E = typename CONFIG_T::edge_weight_t> struct Means {
+    typedef E edge_weight_t;
 
-    template<class CONFIG_T, class E = typename CONFIG_T::edge_weight_t>
-    struct Means {
-      typedef E edge_weight_t;
-      
-      edge_weight_t edge_weight_mean[CONFIG_T::n_aggregators];
-      typename CONFIG_T::aggr_t weighted_feature_mean[CONFIG_T::n_aggregators * CONFIG_T::n_in_features];
+    edge_weight_t edge_weight_mean[CONFIG_T::n_aggregators];
+    typename CONFIG_T::aggr_t weighted_feature_mean[CONFIG_T::n_aggregators * CONFIG_T::n_in_features];
 
-      Means() {
+    Means() {
         #pragma HLS INLINE
         #pragma HLS ARRAY_PARTITION variable=edge_weight_mean complete
         #pragma HLS ARRAY_PARTITION variable=weighted_feature_mean complete
         #pragma HLS UNROLL region
 
-       Aggregators:
+    Aggregators:
         for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-          edge_weight_mean[ia] = 0.;
+            edge_weight_mean[ia] = 0.;
 
-         InFeatures:
-          for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
-            unsigned const iax = ia * CONFIG_T::n_in_features + ix;
-            weighted_feature_mean[iax] = 0.;
-          }
+        InFeatures:
+            for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
+                unsigned const iax = ia * CONFIG_T::n_in_features + ix;
+                weighted_feature_mean[iax] = 0.;
+            }
         }
-      }
+    }
 
-      void set_weight(unsigned, edge_weight_t const&) {
+    void set_weight(unsigned, edge_weight_t const &) {
         #pragma HLS INLINE
-      }
+    }
 
-      void add_means_normalized(Means<CONFIG_T, edge_weight_t> const& local) {
+    void add_means_normalized(Means<CONFIG_T, edge_weight_t> const &local) {
         #pragma HLS INLINE
         // Always called within a pipelined region - no UNROLL needed
 
         unsigned const log2_unroll_factor = CONFIG_T::n_vertices_width - CONFIG_T::log2_reuse_factor;
-        
-       Aggregators:
+
+    Aggregators:
         for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-          edge_weight_mean[ia] += normalize_log2(local.edge_weight_mean[ia], log2_unroll_factor);
+            edge_weight_mean[ia] += normalize_log2(local.edge_weight_mean[ia], log2_unroll_factor);
 
-         InFeatures:
-          for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
-            unsigned const iax = ia * CONFIG_T::n_in_features + ix;
-            weighted_feature_mean[iax] += normalize_log2(local.weighted_feature_mean[iax], log2_unroll_factor);
-          }
+        InFeatures:
+            for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
+                unsigned const iax = ia * CONFIG_T::n_in_features + ix;
+                weighted_feature_mean[iax] += normalize_log2(local.weighted_feature_mean[iax], log2_unroll_factor);
+            }
         }
-      }
+    }
 
-      template<class nvtx_T, class arrays_T, class T = CONFIG_T>
-      typename std::enable_if<T::mean_by_nvert>::type set_means_normalized(nvtx_T const nvtx, arrays_T const& accum) {
+    template <class nvtx_T, class arrays_T, class T = CONFIG_T>
+    typename std::enable_if<T::mean_by_nvert>::type set_means_normalized(nvtx_T const nvtx, arrays_T const &accum) {
         #pragma HLS INLINE
         #pragma HLS UNROLL region
 
         // accum comes divided by unroll factor
         typename T::norm_t nvtx_norm = (T::n_vertices / T::reuse_factor) / nvtx;
 
-       Aggregators:
+    Aggregators:
         for (unsigned ia = 0; ia < T::n_aggregators; ++ia) {
-          edge_weight_mean[ia] = accum.edge_weight_mean[ia] * nvtx_norm;
+            edge_weight_mean[ia] = accum.edge_weight_mean[ia] * nvtx_norm;
 
-         InFeatures:
-          for (unsigned ix = 0; ix < T::n_in_features; ++ix) {
-            unsigned const iax = ia * T::n_in_features + ix;
+        InFeatures:
+            for (unsigned ix = 0; ix < T::n_in_features; ++ix) {
+                unsigned const iax = ia * T::n_in_features + ix;
 
-            weighted_feature_mean[iax] = accum.weighted_feature_mean[iax] * nvtx_norm;
-          }
+                weighted_feature_mean[iax] = accum.weighted_feature_mean[iax] * nvtx_norm;
+            }
         }
-      }
+    }
 
-      template<class nvtx_T, class arrays_T, class T = CONFIG_T>
-      typename std::enable_if<not T::mean_by_nvert>::type set_means_normalized(nvtx_T const nvtx, arrays_T const& accum) {
+    template <class nvtx_T, class arrays_T, class T = CONFIG_T>
+    typename std::enable_if<not T::mean_by_nvert>::type set_means_normalized(nvtx_T const nvtx, arrays_T const &accum) {
         #pragma HLS INLINE
         #pragma HLS UNROLL region
 
-       Aggregators:
+    Aggregators:
         for (unsigned ia = 0; ia < T::n_aggregators; ++ia) {
-  
-          edge_weight_mean[ia] = normalize_log2(accum.edge_weight_mean[ia], T::log2_reuse_factor);
-  
-         InFeatures:
-          for (unsigned ix = 0; ix < T::n_in_features; ++ix) {
-            unsigned const iax = ia * T::n_in_features + ix;
-  
-            weighted_feature_mean[iax] = normalize_log2(accum.weighted_feature_mean[iax], T::log2_reuse_factor);
-          }
-        }
-      }
-    };
-
-    template<class CONFIG_T, class E = typename CONFIG_T::edge_weight_t>
-    struct WeightsAndMeans : public Means<CONFIG_T, E> {
-      typedef E edge_weight_t;
 
-      edge_weight_t edge_weights[CONFIG_T::n_vertices * CONFIG_T::n_aggregators];
+            edge_weight_mean[ia] = normalize_log2(accum.edge_weight_mean[ia], T::log2_reuse_factor);
+
+        InFeatures:
+            for (unsigned ix = 0; ix < T::n_in_features; ++ix) {
+                unsigned const iax = ia * T::n_in_features + ix;
+
+                weighted_feature_mean[iax] = normalize_log2(accum.weighted_feature_mean[iax], T::log2_reuse_factor);
+            }
+        }
+    }
+};
+
+template <class CONFIG_T, class E = typename CONFIG_T::edge_weight_t> struct WeightsAndMeans : public Means<CONFIG_T, E> {
+    typedef E edge_weight_t;
+
+    edge_weight_t edge_weights[CONFIG_T::n_vertices * CONFIG_T::n_aggregators];
 
-      WeightsAndMeans() : Means<CONFIG_T, E>() {
+    WeightsAndMeans() : Means<CONFIG_T, E>() {
         #pragma HLS INLINE
         unsigned const reshape_factor = CONFIG_T::n_aggregators * (CONFIG_T::n_vertices / CONFIG_T::reuse_factor);
         #pragma HLS ARRAY_PARTITION variable=edge_weights cyclic factor=reshape_factor
-      }
+    }
 
-      void set_weight(unsigned iva, edge_weight_t const& weight) {
+    void set_weight(unsigned iva, edge_weight_t const &weight) {
         #pragma HLS INLINE
         edge_weights[iva] = weight;
-      }
-    };
-
+    }
+};
 
-    template<class CONFIG_T, class nvtx_T, class Enable = void>
-    struct OutputBiasNormalizer;
+template <class CONFIG_T, class nvtx_T, class Enable = void> struct OutputBiasNormalizer;
 
-    template<class CONFIG_T, class nvtx_T>
-    struct OutputBiasNormalizer<CONFIG_T, nvtx_T, typename std::enable_if<CONFIG_T::mean_by_nvert>::type> {
-      typedef typename CONFIG_T::output_transform_biases_t biases_t;
+template <class CONFIG_T, class nvtx_T>
+struct OutputBiasNormalizer<CONFIG_T, nvtx_T, typename std::enable_if<CONFIG_T::mean_by_nvert>::type> {
+    typedef typename CONFIG_T::output_transform_biases_t biases_t;
 
-      biases_t const (&output_biases)[CONFIG_T::n_out_features];
+    biases_t const (&output_biases)[CONFIG_T::n_out_features];
 
-      OutputBiasNormalizer(nvtx_T const) : output_biases{CONFIG_T::output_transform_biases} {
+    OutputBiasNormalizer(nvtx_T const) : output_biases{CONFIG_T::output_transform_biases} {
         #pragma HLS INLINE
-      }
-    };
+    }
+};
+
+template <class CONFIG_T, class nvtx_T>
+struct OutputBiasNormalizer<CONFIG_T, nvtx_T, typename std::enable_if<not CONFIG_T::mean_by_nvert>::type> {
+    typedef typename CONFIG_T::output_transform_biases_t biases_t;
 
-    template<class CONFIG_T, class nvtx_T>
-    struct OutputBiasNormalizer<CONFIG_T, nvtx_T, typename std::enable_if<not CONFIG_T::mean_by_nvert>::type> {
-      typedef typename CONFIG_T::output_transform_biases_t biases_t;
+    biases_t output_biases[CONFIG_T::n_out_features];
 
-      biases_t output_biases[CONFIG_T::n_out_features];
-      
-      OutputBiasNormalizer(nvtx_T const nvtx) {
+    OutputBiasNormalizer(nvtx_T const nvtx) {
         #pragma HLS ARRAY_PARTITION variable=output_biases complete
         #pragma HLS UNROLL region
-        
+
         // Cannot add a loop label here due to a Vivado HLS bug, apparently
         for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
-          typename CONFIG_T::aggr_t bias = CONFIG_T::output_transform_biases[io];
-          bias *= nvtx;
-          output_biases[io] = normalize_log2(bias, CONFIG_T::n_vertices_width);
+            typename CONFIG_T::aggr_t bias = CONFIG_T::output_transform_biases[io];
+            bias *= nvtx;
+            output_biases[io] = normalize_log2(bias, CONFIG_T::n_vertices_width);
         }
-      }
-    };
+    }
+};
 
-    template<class CONFIG_T, class data_T>
-    struct InputDataGetter {
-      typedef data_T data_t;
+template <class CONFIG_T, class data_T> struct InputDataGetter {
+    typedef data_T data_t;
 
-      data_T const* dataref;
+    data_T const *dataref;
 
-      InputDataGetter(data_T const* d) : dataref{d} {
+    InputDataGetter(data_T const *d) : dataref{d} {
         #pragma HLS INLINE
-      }
-      data_T const& get(unsigned iv, unsigned ix) const {
+    }
+    data_T const &get(unsigned iv, unsigned ix) const {
         #pragma HLS INLINE
         unsigned const ivx = iv * CONFIG_T::n_in_features + ix;
         return dataref[ivx];
-      }
-    };
+    }
+};
 
-    template<class CONFIG_T, class data_T>
-    struct SingleVertexDataGetter {
-      typedef data_T data_t;
+template <class CONFIG_T, class data_T> struct SingleVertexDataGetter {
+    typedef data_T data_t;
 
-      data_T const (&dataref)[CONFIG_T::n_in_features];
+    data_T const (&dataref)[CONFIG_T::n_in_features];
 
-      SingleVertexDataGetter(data_T const (&d)[CONFIG_T::n_in_features]) : dataref{d} {
+    SingleVertexDataGetter(data_T const (&d)[CONFIG_T::n_in_features]) : dataref{d} {
         #pragma HLS INLINE
-      }
-      data_T const& get(unsigned, unsigned ix) const {
+    }
+    data_T const &get(unsigned, unsigned ix) const {
         #pragma HLS INLINE
         return dataref[ix];
-      }
-    };
+    }
+};
 
-    template<class CONFIG_T, class res_T>
-    struct OutputResSetter {
-      typedef res_T res_t;
+template <class CONFIG_T, class res_T> struct OutputResSetter {
+    typedef res_T res_t;
 
-      res_T* resref;
+    res_T *resref;
 
-      OutputResSetter(res_T* r) : resref{r} {
+    OutputResSetter(res_T *r) : resref{r} {
         #pragma HLS INLINE
-      }
-      void set(unsigned iv, unsigned io, res_T const& acc) {
+    }
+    void set(unsigned iv, unsigned io, res_T const &acc) {
         #pragma HLS INLINE
-        unsigned const ivo = iv * CONFIG_T::n_out_features + io;    
+        unsigned const ivo = iv * CONFIG_T::n_out_features + io;
         resref[ivo] = acc;
-      }
-    };
+    }
+};
 
-    template<class CONFIG_T, class res_T>
-    struct SingleVertexResSetter {
-      typedef res_T res_t;
+template <class CONFIG_T, class res_T> struct SingleVertexResSetter {
+    typedef res_T res_t;
 
-      res_T (&resref)[CONFIG_T::n_out_features];
+    res_T (&resref)[CONFIG_T::n_out_features];
 
-      SingleVertexResSetter(res_T (&r)[CONFIG_T::n_out_features]) : resref{r} {
+    SingleVertexResSetter(res_T (&r)[CONFIG_T::n_out_features]) : resref{r} {
         #pragma HLS INLINE
-      }
-      void set(unsigned, unsigned io, res_T const& acc) {
+    }
+    void set(unsigned, unsigned io, res_T const &acc) {
         #pragma HLS INLINE
         resref[io] = acc;
-      }
-    };
+    }
+};
 
-    template<class CONFIG_T, class data_getter_T, class arrays_local_T, class arrays_T>
-    inline
-    void
-    compute_weights_aggregates(
-      data_getter_T const& data_getter,
-      unsigned iv,
-      arrays_local_T& arrays_local,
-      arrays_T& arrays
-    )
-    {
-      #pragma HLS INLINE
+template <class CONFIG_T, class data_getter_T, class arrays_local_T, class arrays_T>
+inline void compute_weights_aggregates(data_getter_T const &data_getter, unsigned iv, arrays_local_T &arrays_local,
+                                       arrays_T &arrays) {
+    #pragma HLS INLINE
 
-     Aggregators:
-      for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
+Aggregators:
+    for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
         typename CONFIG_T::distance_t distance = CONFIG_T::aggregator_distance_biases[ia];
 
-       InFeatures1:
+    InFeatures1:
         for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
-          unsigned const iax = ia * CONFIG_T::n_in_features + ix;
+            unsigned const iax = ia * CONFIG_T::n_in_features + ix;
 
-          typename CONFIG_T::distance_t incr = data_getter.get(iv, ix) * CONFIG_T::aggregator_distance_weights[iax];
+            typename CONFIG_T::distance_t incr = data_getter.get(iv, ix) * CONFIG_T::aggregator_distance_weights[iax];
 
-          distance += incr;
+            distance += incr;
         }
 
-        typename CONFIG_T::edge_weight_t edge_weight = garnet_utils::compute_edge_weight<typename CONFIG_T::base_t>(distance);
+        typename CONFIG_T::edge_weight_t edge_weight =
+            garnet_utils::compute_edge_weight<typename CONFIG_T::base_t>(distance);
 
         arrays_local.edge_weight_mean[ia] += edge_weight;
 
-       InFeatures2:
+    InFeatures2:
         for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
-          unsigned const iax = ia * CONFIG_T::n_in_features + ix;
+            unsigned const iax = ia * CONFIG_T::n_in_features + ix;
 
-          typename data_getter_T::data_t incr = data_getter.get(iv, ix) * edge_weight;
+            typename data_getter_T::data_t incr = data_getter.get(iv, ix) * edge_weight;
 
-          arrays_local.weighted_feature_mean[iax] += incr;
+            arrays_local.weighted_feature_mean[iax] += incr;
         }
 
         unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
         arrays.set_weight(iva, edge_weight);
-      }
     }
+}
 
-    template<class CONFIG_T, class arrays_T>
-    inline
-    typename CONFIG_T::aggr_t
-    compute_output_base_core(
-      arrays_T const& arrays,
-      unsigned io,
-      unsigned ia
-    )
-    {
-      #pragma HLS INLINE
-      #pragma HLS UNROLL region
+template <class CONFIG_T, class arrays_T>
+inline typename CONFIG_T::aggr_t compute_output_base_core(arrays_T const &arrays, unsigned io, unsigned ia) {
+    #pragma HLS INLINE
+    #pragma HLS UNROLL region
 
-      unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
-      typename CONFIG_T::aggr_t aggr = arrays.edge_weight_mean[ia] * CONFIG_T::input_transform_biases[ioa];
+    unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
+    typename CONFIG_T::aggr_t aggr = arrays.edge_weight_mean[ia] * CONFIG_T::input_transform_biases[ioa];
 
-     InFeatures:
-      for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
+InFeatures:
+    for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
         unsigned const ioax = ioa * CONFIG_T::n_in_features + ix;
         unsigned const iax = ia * CONFIG_T::n_in_features + ix;
 
         aggr += arrays.weighted_feature_mean[iax] * CONFIG_T::input_transform_weights[ioax];
-      }
-
-      return aggr;
     }
 
-    template<class CONFIG_T, class arrays_T>
-    inline
-    void
-    compute_output_base(
-      arrays_T const& arrays,
-      typename CONFIG_T::aggr_t output_base[CONFIG_T::n_out_features * CONFIG_T::n_aggregators]
-    )
-    {
-      #pragma HLS INLINE
-      #pragma HLS UNROLL region
-
-     OutFeatures:
-      for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
-       Aggregators:
+    return aggr;
+}
+
+template <class CONFIG_T, class arrays_T>
+inline void compute_output_base(arrays_T const &arrays,
+                                typename CONFIG_T::aggr_t output_base[CONFIG_T::n_out_features * CONFIG_T::n_aggregators]) {
+    #pragma HLS INLINE
+    #pragma HLS UNROLL region
+
+OutFeatures:
+    for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
+    Aggregators:
         for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-          unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
+            unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
 
-          output_base[ioa] = compute_output_base_core<CONFIG_T>(arrays, io, ia);
+            output_base[ioa] = compute_output_base_core<CONFIG_T>(arrays, io, ia);
         }
-      }
     }
+}
 
-    template<class CONFIG_T, class arrays_T, class res_setter_T>
-    inline
-    void
-    compute_vertex_output(
-      arrays_T const& arrays,
-      unsigned iv,
-      typename CONFIG_T::aggr_t const output_base[CONFIG_T::n_out_features * CONFIG_T::n_aggregators],
-      res_setter_T& res_setter
-    )
-    {
-      #pragma HLS INLINE
+template <class CONFIG_T, class arrays_T, class res_setter_T>
+inline void
+compute_vertex_output(arrays_T const &arrays, unsigned iv,
+                      typename CONFIG_T::aggr_t const output_base[CONFIG_T::n_out_features * CONFIG_T::n_aggregators],
+                      res_setter_T &res_setter) {
+    #pragma HLS INLINE
 
-      typename arrays_T::edge_weight_t edge_weights[CONFIG_T::n_aggregators];
-      #pragma HLS ARRAY_PARTITION variable=edge_weights complete
+    typename arrays_T::edge_weight_t edge_weights[CONFIG_T::n_aggregators];
+    #pragma HLS ARRAY_PARTITION variable=edge_weights complete
 
-     Aggregators1:
-      for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
+Aggregators1:
+    for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
         unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
 
         edge_weights[ia] = arrays.edge_weights[iva];
-      }
-      
-     OutFeatures:
-      for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
+    }
+
+OutFeatures:
+    for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
         typename res_setter_T::res_t acc = CONFIG_T::output_transform_biases[io];
-    
-       Aggregators2:
+
+    Aggregators2:
         for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-          unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
+            unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
 
-          typename res_setter_T::res_t incr = edge_weights[ia] * output_base[ioa];
-          acc += incr;
+            typename res_setter_T::res_t incr = edge_weights[ia] * output_base[ioa];
+            acc += incr;
         }
 
         res_setter.set(iv, io, acc);
-      }
     }
+}
 
-    template<class CONFIG_T, class data_T, class nvtx_T, class arrays_T>
-    void
-    aggregate(
-      data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-      nvtx_T const nvtx,
-      arrays_T& arrays
-    )
-    {
-      InputDataGetter<CONFIG_T, data_T> data_getter(data);
-
-      unsigned const unroll_factor = CONFIG_T::n_vertices >> CONFIG_T::log2_reuse_factor;
-
-      Means<CONFIG_T, typename CONFIG_T::edge_weight_aggr_t> means_accum;
-      
-     VerticesOuter:
-      for (unsigned ivv = 0; ivv < CONFIG_T::reuse_factor; ++ivv) {
+template <class CONFIG_T, class data_T, class nvtx_T, class arrays_T>
+void aggregate(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx, arrays_T &arrays) {
+    InputDataGetter<CONFIG_T, data_T> data_getter(data);
+
+    unsigned const unroll_factor = CONFIG_T::n_vertices >> CONFIG_T::log2_reuse_factor;
+
+    Means<CONFIG_T, typename CONFIG_T::edge_weight_aggr_t> means_accum;
+
+VerticesOuter:
+    for (unsigned ivv = 0; ivv < CONFIG_T::reuse_factor; ++ivv) {
         #pragma HLS PIPELINE
 
         if (ivv * unroll_factor >= nvtx)
-          break;
+            break;
 
         Means<CONFIG_T, typename CONFIG_T::edge_weight_aggr_t> means_local;
 
-       VerticesInner:
+    VerticesInner:
         for (unsigned ir = 0; ir < unroll_factor; ++ir) {
-          unsigned iv = ivv * unroll_factor + ir;
+            unsigned iv = ivv * unroll_factor + ir;
 
-          if (iv == nvtx)
-            break;
-    
-          compute_weights_aggregates<CONFIG_T>(data_getter, iv, means_local, arrays);
+            if (iv == nvtx)
+                break;
+
+            compute_weights_aggregates<CONFIG_T>(data_getter, iv, means_local, arrays);
         }
 
         means_accum.add_means_normalized(means_local);
-      }
-
-      arrays.set_means_normalized(nvtx, means_accum);
     }
 
-    template<class CONFIG_T, class nvtx_T, class arrays_T, class res_T>
-    void
-    distribute(
-      nvtx_T const nvtx,
-      arrays_T const& arrays,
-      res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]
-    )
-    {
-      OutputResSetter<CONFIG_T, res_T> res_setter(res);
+    arrays.set_means_normalized(nvtx, means_accum);
+}
+
+template <class CONFIG_T, class nvtx_T, class arrays_T, class res_T>
+void distribute(nvtx_T const nvtx, arrays_T const &arrays, res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]) {
+    OutputResSetter<CONFIG_T, res_T> res_setter(res);
 
-      typename CONFIG_T::aggr_t output_base[CONFIG_T::n_out_features * CONFIG_T::n_aggregators];
-      #pragma HLS ARRAY_PARTITION variable=output_base complete      
+    typename CONFIG_T::aggr_t output_base[CONFIG_T::n_out_features * CONFIG_T::n_aggregators];
+    #pragma HLS ARRAY_PARTITION variable=output_base complete
 
-      compute_output_base<CONFIG_T>(arrays, output_base);
+    compute_output_base<CONFIG_T>(arrays, output_base);
 
-      unsigned const unroll_factor = CONFIG_T::n_vertices >> CONFIG_T::log2_reuse_factor;
+    unsigned const unroll_factor = CONFIG_T::n_vertices >> CONFIG_T::log2_reuse_factor;
 
-     VerticesOuter:
-      for (unsigned ivv = 0; ivv < CONFIG_T::reuse_factor; ++ivv) {
+VerticesOuter:
+    for (unsigned ivv = 0; ivv < CONFIG_T::reuse_factor; ++ivv) {
         #pragma HLS PIPELINE
 
         if (ivv * unroll_factor >= nvtx)
-          break;
+            break;
 
-       VerticesInner:
+    VerticesInner:
         for (unsigned ir = 0; ir < unroll_factor; ++ir) {
-          unsigned iv = ivv * unroll_factor + ir;
+            unsigned iv = ivv * unroll_factor + ir;
 
-          if (iv == nvtx)
-            break;
-    
-          compute_vertex_output<CONFIG_T>(arrays, iv, output_base, res_setter);
+            if (iv == nvtx)
+                break;
+
+            compute_vertex_output<CONFIG_T>(arrays, iv, output_base, res_setter);
         }
-      }
     }
-    
-    template<class CONFIG_T, class output_biases_T, class arrays_T, class res_T>
-    void
-    set_output(
-      output_biases_T const& output_transform_biases,
-      arrays_T const& arrays,
-      res_T res[CONFIG_T::n_out_features]
-    )
-    {
-      #pragma HLS PIPELINE
-    
-     OutFeatures:
-      for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
+}
+
+template <class CONFIG_T, class output_biases_T, class arrays_T, class res_T>
+void set_output(output_biases_T const &output_transform_biases, arrays_T const &arrays,
+                res_T res[CONFIG_T::n_out_features]) {
+    #pragma HLS PIPELINE
+
+OutFeatures:
+    for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
         res_T acc = output_transform_biases.output_biases[io];
 
-       Aggregators:
+    Aggregators:
         for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-          typename CONFIG_T::aggr_t aggr = compute_output_base_core<CONFIG_T>(arrays, io, ia);
+            typename CONFIG_T::aggr_t aggr = compute_output_base_core<CONFIG_T>(arrays, io, ia);
 
-          acc += arrays.edge_weight_mean[ia] * aggr;
+            acc += arrays.edge_weight_mean[ia] * aggr;
         }
-    
+
         res[io] = acc;
-      }
     }
+}
+
+template <class prev_layer_t, class current_layer_t, class nvtx_T, class prev_arrays_T, class current_arrays_T>
+void distribute_aggregate(nvtx_T const nvtx, prev_arrays_T const &prev_arrays, current_arrays_T &current_arrays) {
+    typedef typename prev_layer_t::output_t data_T;
+
+    typename prev_layer_t::aggr_t prev_output_base[prev_layer_t::n_out_features * prev_layer_t::n_aggregators];
+    #pragma HLS ARRAY_PARTITION variable=prev_output_base complete
+
+    compute_output_base<prev_layer_t>(prev_arrays, prev_output_base);
+
+    unsigned const unroll_factor = current_layer_t::n_vertices >> current_layer_t::log2_reuse_factor;
+
+    Means<current_layer_t, typename current_layer_t::edge_weight_aggr_t> means_accum;
 
-    template<class prev_layer_t, class current_layer_t, class nvtx_T, class prev_arrays_T, class current_arrays_T>
-    void
-    distribute_aggregate(
-      nvtx_T const nvtx,
-      prev_arrays_T const& prev_arrays,
-      current_arrays_T& current_arrays
-    )
-    {
-      typedef typename prev_layer_t::output_t data_T;
-
-      typename prev_layer_t::aggr_t prev_output_base[prev_layer_t::n_out_features * prev_layer_t::n_aggregators];
-      #pragma HLS ARRAY_PARTITION variable=prev_output_base complete
-
-      compute_output_base<prev_layer_t>(prev_arrays, prev_output_base);
-
-      unsigned const unroll_factor = current_layer_t::n_vertices >> current_layer_t::log2_reuse_factor;
-
-      Means<current_layer_t, typename current_layer_t::edge_weight_aggr_t> means_accum;
-      
-     VerticesOuter:
-      for (unsigned ivv = 0; ivv < current_layer_t::reuse_factor; ++ivv) {
+VerticesOuter:
+    for (unsigned ivv = 0; ivv < current_layer_t::reuse_factor; ++ivv) {
         #pragma HLS PIPELINE
 
         if (ivv * unroll_factor >= nvtx)
-          break;
+            break;
 
         Means<current_layer_t, typename current_layer_t::edge_weight_aggr_t> means_local;
 
-       VerticesInner:
+    VerticesInner:
         for (unsigned ir = 0; ir < unroll_factor; ++ir) {
-          unsigned iv = ivv * unroll_factor + ir;
+            unsigned iv = ivv * unroll_factor + ir;
 
-          if (iv == nvtx)
-            break;
-    
-          data_T data[prev_layer_t::n_out_features];
-          #pragma HLS ARRAY_PARTITION variable=data complete
+            if (iv == nvtx)
+                break;
+
+            data_T data[prev_layer_t::n_out_features];
+            #pragma HLS ARRAY_PARTITION variable=data complete
 
-          SingleVertexResSetter<prev_layer_t, data_T> res_setter(data);
+            SingleVertexResSetter<prev_layer_t, data_T> res_setter(data);
 
-          compute_vertex_output<prev_layer_t>(prev_arrays, iv, prev_output_base, res_setter);
+            compute_vertex_output<prev_layer_t>(prev_arrays, iv, prev_output_base, res_setter);
 
-          SingleVertexDataGetter<current_layer_t, data_T> data_getter(data);
-          
-          compute_weights_aggregates<current_layer_t>(data_getter, iv, means_local, current_arrays);
+            SingleVertexDataGetter<current_layer_t, data_T> data_getter(data);
+
+            compute_weights_aggregates<current_layer_t>(data_getter, iv, means_local, current_arrays);
         }
 
         means_accum.add_means_normalized(means_local);
-      }
-
-      current_arrays.set_means_normalized(nvtx, means_accum);
     }
 
-    template<class prev_layer_t, class current_layer_t, class last_layer_t, class nvtx_T, class prev_arrays_T, class last_arrays_T>
-    inline
-    typename std::enable_if<std::is_same<current_layer_t, last_layer_t>::value>::type
-    sublayer(
-      nvtx_T const nvtx,
-      prev_arrays_T const& prev_arrays,
-      last_arrays_T& last_arrays
-    )
-    {
-      #pragma HLS INLINE
-
-      distribute_aggregate<prev_layer_t, current_layer_t>(nvtx, prev_arrays, last_arrays);
-    }
-      
-    template<class prev_layer_t, class current_layer_t, class last_layer_t, class nvtx_T, class prev_arrays_T, class last_arrays_T>
-    inline
-    typename std::enable_if<not std::is_same<current_layer_t, last_layer_t>::value>::type
-    sublayer(
-      nvtx_T const nvtx,
-      prev_arrays_T const& prev_arrays,
-      last_arrays_T& last_arrays
-    )
-    {
-      #pragma HLS INLINE
-
-      WeightsAndMeans<current_layer_t> current_arrays;
-
-      distribute_aggregate<prev_layer_t, current_layer_t>(nvtx, prev_arrays, current_arrays);
-
-      sublayer<current_layer_t, typename current_layer_t::next_layer_t, last_layer_t>(nvtx, current_arrays, last_arrays);
-    }
-  }
- 
-  struct garnet_config
-  {
+    current_arrays.set_means_normalized(nvtx, means_accum);
+}
+
+template <class prev_layer_t, class current_layer_t, class last_layer_t, class nvtx_T, class prev_arrays_T,
+          class last_arrays_T>
+inline typename std::enable_if<std::is_same<current_layer_t, last_layer_t>::value>::type
+sublayer(nvtx_T const nvtx, prev_arrays_T const &prev_arrays, last_arrays_T &last_arrays) {
+    #pragma HLS INLINE
+
+    distribute_aggregate<prev_layer_t, current_layer_t>(nvtx, prev_arrays, last_arrays);
+}
+
+template <class prev_layer_t, class current_layer_t, class last_layer_t, class nvtx_T, class prev_arrays_T,
+          class last_arrays_T>
+inline typename std::enable_if<not std::is_same<current_layer_t, last_layer_t>::value>::type
+sublayer(nvtx_T const nvtx, prev_arrays_T const &prev_arrays, last_arrays_T &last_arrays) {
+    #pragma HLS INLINE
+
+    WeightsAndMeans<current_layer_t> current_arrays;
+
+    distribute_aggregate<prev_layer_t, current_layer_t>(nvtx, prev_arrays, current_arrays);
+
+    sublayer<current_layer_t, typename current_layer_t::next_layer_t, last_layer_t>(nvtx, current_arrays, last_arrays);
+}
+} // namespace garnet_utils
+
+struct garnet_config {
     // Layer specs
     static const unsigned n_vertices_width = 8;
     static const unsigned n_vertices = (1 << n_vertices_width);
     static const unsigned n_in_features = 4;
     static const unsigned n_propagate = 4;
     static const unsigned n_aggregators = 4;
     static const unsigned n_out_features = 4;
@@ -666,318 +569,248 @@
     // Internal data type definitions
     typedef float input_transform_weights_t;
     typedef float input_transform_biases_t;
     typedef float output_transform_weights_t;
     typedef float output_transform_biases_t;
     typedef float aggregator_distance_weights_t;
     typedef float aggregator_distance_biases_t;
-  
+
     typedef float norm_t;
     typedef float distance_t;
     typedef float edge_weight_t;
     typedef float edge_weight_aggr_t;
     typedef float aggr_t;
     typedef float output_t;
 
     /* static const input_transform_weights_t (&input_transform_weights)[n_out_features * n_aggregators * n_in_features]; */
     /* static const input_transform_biases_t (&input_transform_biases)[n_out_features * n_aggregators]; */
     /* static const aggregator_distance_weights_t (&aggregator_distance_weights)[n_aggregators * n_in_features]; */
     /* static const aggregator_distance_biases_t (&aggregator_distance_biases)[n_aggregators]; */
     /* static const output_transform_biases_t (&output_transform_biases)[n_out_features]; */
 
-    enum OutputCollapse {
-      no_collapse,
-      collapse_mean,
-      collapse_max
-    };
-  
+    enum OutputCollapse { no_collapse, collapse_mean, collapse_max };
+
     static const unsigned output_collapse = no_collapse;
 
     static const bool mean_by_nvert = false;
     static const bool is_stack = false;
- 
+
     // Optimization specs
     static const unsigned reuse_factor = 64;
     static const unsigned log2_reuse_factor = 6;
-  };
+};
 
-  // vertices -> vertices
-  template<class data_T, class nvtx_T, class res_T, typename CONFIG_T>
-  typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::no_collapse>::type
-  garnet(
-    data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-    nvtx_T const nvtx[1],
-    res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]
-  )
-  {
+// vertices -> vertices
+template <class data_T, class nvtx_T, class res_T, typename CONFIG_T>
+typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::no_collapse>::type
+garnet(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx[1],
+       res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]) {
     #pragma HLS DATAFLOW
 
     garnet_utils::WeightsAndMeans<CONFIG_T> arrays;
 
-    garnet_utils::aggregate<CONFIG_T>(
-      data,
-      nvtx[0],
-      arrays
-    );
-  
-    garnet_utils::distribute<CONFIG_T>(
-      nvtx[0],
-      arrays,
-      res
-    );
-  }
-
-  // vertices -> out features
-  template<class data_T, class nvtx_T, class res_T, class CONFIG_T>
-  typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::collapse_mean>::type
-  garnet(
-    data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-    nvtx_T const nvtx[1],
-    res_T res[CONFIG_T::n_out_features]
-  )
-  {
+    garnet_utils::aggregate<CONFIG_T>(data, nvtx[0], arrays);
+
+    garnet_utils::distribute<CONFIG_T>(nvtx[0], arrays, res);
+}
+
+// vertices -> out features
+template <class data_T, class nvtx_T, class res_T, class CONFIG_T>
+typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::collapse_mean>::type
+garnet(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx[1],
+       res_T res[CONFIG_T::n_out_features]) {
     #pragma HLS DATAFLOW
 
     garnet_utils::Means<CONFIG_T> arrays;
-  
-    garnet_utils::aggregate<CONFIG_T>(
-      data,
-      nvtx[0],
-      arrays
-    );
+
+    garnet_utils::aggregate<CONFIG_T>(data, nvtx[0], arrays);
 
     garnet_utils::OutputBiasNormalizer<CONFIG_T, nvtx_T> normalize_bias(nvtx[0]);
 
-    garnet_utils::set_output<CONFIG_T>(
-      normalize_bias,
-      arrays,
-      res
-    );
-  }
-
-  // vertices -> vertices
-  template<class data_T, class nvtx_T, class res_T, class CONFIG_T>
-  typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::no_collapse>::type
-  garnet_stack(
-    data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-    nvtx_T const nvtx[1],
-    res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]
-  )
-  {
+    garnet_utils::set_output<CONFIG_T>(normalize_bias, arrays, res);
+}
+
+// vertices -> vertices
+template <class data_T, class nvtx_T, class res_T, class CONFIG_T>
+typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::no_collapse>::type
+garnet_stack(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx[1],
+             res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]) {
     #pragma HLS DATAFLOW
 
     typedef typename CONFIG_T::template sublayer_t<0> first_layer_t;
     unsigned const ilast = CONFIG_T::n_sublayers - 1;
     typedef typename CONFIG_T::template sublayer_t<ilast> last_layer_t;
 
     garnet_utils::WeightsAndMeans<first_layer_t> arrays_first;
     garnet_utils::Means<last_layer_t> arrays_last;
 
-    garnet_utils::aggregate<first_layer_t>(
-      data,
-      nvtx[0],
-      arrays_first
-    );
-
-    garnet_utils::sublayer<first_layer_t, typename first_layer_t::next_layer_t, last_layer_t>(
-      nvtx[0],
-      arrays_first,
-      arrays_last
-    );
-
-    garnet_utils::distribute<last_layer_t>(
-      nvtx[0],
-      arrays_last,
-      res
-    );
-  }
-
-  // vertices -> out features
-  template<class data_T, class nvtx_T, class res_T, class CONFIG_T>
-  typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::collapse_mean>::type
-  garnet_stack(
-    data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-    nvtx_T const nvtx[1],
-    res_T res[CONFIG_T::n_out_features]
-  )
-  {
+    garnet_utils::aggregate<first_layer_t>(data, nvtx[0], arrays_first);
+
+    garnet_utils::sublayer<first_layer_t, typename first_layer_t::next_layer_t, last_layer_t>(nvtx[0], arrays_first,
+                                                                                              arrays_last);
+
+    garnet_utils::distribute<last_layer_t>(nvtx[0], arrays_last, res);
+}
+
+// vertices -> out features
+template <class data_T, class nvtx_T, class res_T, class CONFIG_T>
+typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::collapse_mean>::type
+garnet_stack(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx[1],
+             res_T res[CONFIG_T::n_out_features]) {
     #pragma HLS DATAFLOW
 
     typedef typename CONFIG_T::template sublayer_t<0> first_layer_t;
     unsigned const ilast = CONFIG_T::n_sublayers - 1;
     typedef typename CONFIG_T::template sublayer_t<ilast> last_layer_t;
 
     garnet_utils::WeightsAndMeans<first_layer_t> arrays_first;
     garnet_utils::Means<last_layer_t> arrays_last;
 
-    garnet_utils::aggregate<first_layer_t>(
-      data,
-      nvtx[0],
-      arrays_first
-    );
-
-    garnet_utils::sublayer<first_layer_t, typename first_layer_t::next_layer_t, last_layer_t>(
-      nvtx[0],
-      arrays_first,
-      arrays_last
-    );
+    garnet_utils::aggregate<first_layer_t>(data, nvtx[0], arrays_first);
+
+    garnet_utils::sublayer<first_layer_t, typename first_layer_t::next_layer_t, last_layer_t>(nvtx[0], arrays_first,
+                                                                                              arrays_last);
 
     garnet_utils::OutputBiasNormalizer<last_layer_t, nvtx_T> normalize_bias(nvtx[0]);
 
-    garnet_utils::set_output<last_layer_t>(
-      normalize_bias,
-      arrays_last,
-      res
-    );
-  }
-
-  /* Reference (dumb) implementation returning (Vertices, Features) */
-  template<class data_T, class nvtx_T, class res_T, typename CONFIG_T>
-  typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::no_collapse>::type
-  garnet_ref(
-    data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-    nvtx_T const nvtx[1],
-    res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]
-  )
-  {
+    garnet_utils::set_output<last_layer_t>(normalize_bias, arrays_last, res);
+}
+
+/* Reference (dumb) implementation returning (Vertices, Features) */
+template <class data_T, class nvtx_T, class res_T, typename CONFIG_T>
+typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::no_collapse>::type
+garnet_ref(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx[1],
+           res_T res[CONFIG_T::n_vertices * CONFIG_T::n_out_features]) {
     typename CONFIG_T::edge_weight_t edge_weights[CONFIG_T::n_vertices * CONFIG_T::n_aggregators];
     typename CONFIG_T::aggr_t propagated_features[CONFIG_T::n_vertices * CONFIG_T::n_propagate];
-  
+
     for (unsigned iv = 0; iv < CONFIG_T::n_vertices; ++iv) {
-      if (iv == nvtx[0])
-        break;
-  
-      for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
-        unsigned const ivp = iv * CONFIG_T::n_propagate + ip;
-  
-        propagated_features[ivp] = CONFIG_T::input_transform_biases[ip];
-  
-        for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
-          unsigned const ivx = iv * CONFIG_T::n_in_features + ix;
-          unsigned const ipx = ip * CONFIG_T::n_in_features + ix;
-  
-          propagated_features[ivp] += data[ivx] * CONFIG_T::input_transform_weights[ipx];
-        }
-      }
-  
-      for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-        unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
-  
-        typename CONFIG_T::aggr_t distance = CONFIG_T::aggregator_distance_biases[ia];
-  
-        for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
-          unsigned const ivx = iv * CONFIG_T::n_in_features + ix;
-          unsigned const iax = ia * CONFIG_T::n_in_features + ix;
-  
-          distance += data[ivx] * CONFIG_T::aggregator_distance_weights[iax];
-        }
-  
-        edge_weights[iva] = garnet_utils::compute_edge_weight<CONFIG_T>(distance);
-      }
+        if (iv == nvtx[0])
+            break;
+
+        for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
+            unsigned const ivp = iv * CONFIG_T::n_propagate + ip;
+
+            propagated_features[ivp] = CONFIG_T::input_transform_biases[ip];
+
+            for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
+                unsigned const ivx = iv * CONFIG_T::n_in_features + ix;
+                unsigned const ipx = ip * CONFIG_T::n_in_features + ix;
+
+                propagated_features[ivp] += data[ivx] * CONFIG_T::input_transform_weights[ipx];
+            }
+        }
+
+        for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
+            unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
+
+            typename CONFIG_T::aggr_t distance = CONFIG_T::aggregator_distance_biases[ia];
+
+            for (unsigned ix = 0; ix < CONFIG_T::n_in_features; ++ix) {
+                unsigned const ivx = iv * CONFIG_T::n_in_features + ix;
+                unsigned const iax = ia * CONFIG_T::n_in_features + ix;
+
+                distance += data[ivx] * CONFIG_T::aggregator_distance_weights[iax];
+            }
+
+            edge_weights[iva] = garnet_utils::compute_edge_weight<CONFIG_T>(distance);
+        }
     }
-  
+
     typename CONFIG_T::aggr_t aggregated_features[CONFIG_T::n_aggregators * CONFIG_T::n_propagate];
-  
+
     for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-      for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
-        unsigned const iap = ia * CONFIG_T::n_propagate + ip;
-  
-        aggregated_features[iap] = 0.;
-  
-        for (unsigned iv = 0; iv < CONFIG_T::n_vertices; ++iv) {
-          if (iv == nvtx[0])
-            break;
-  
-          unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
-          unsigned const ivp = iv * CONFIG_T::n_propagate + ip;
-  
-          aggregated_features[iap] += edge_weights[iva] * propagated_features[ivp];
+        for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
+            unsigned const iap = ia * CONFIG_T::n_propagate + ip;
+
+            aggregated_features[iap] = 0.;
+
+            for (unsigned iv = 0; iv < CONFIG_T::n_vertices; ++iv) {
+                if (iv == nvtx[0])
+                    break;
+
+                unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
+                unsigned const ivp = iv * CONFIG_T::n_propagate + ip;
+
+                aggregated_features[iap] += edge_weights[iva] * propagated_features[ivp];
+            }
         }
-      }
     }
-  
+
     for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-      for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
-        unsigned const iap = ia * CONFIG_T::n_propagate + ip;
-  
-        if (CONFIG_T::mean_by_nvert)
-          aggregated_features[iap] /= nvtx[0];
-        else {
-          // Not using right shift in case aggr_t is float or double
-          aggregated_features[iap] /= CONFIG_T::n_vertices;
+        for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
+            unsigned const iap = ia * CONFIG_T::n_propagate + ip;
+
+            if (CONFIG_T::mean_by_nvert)
+                aggregated_features[iap] /= nvtx[0];
+            else {
+                // Not using right shift in case aggr_t is float or double
+                aggregated_features[iap] /= CONFIG_T::n_vertices;
+            }
         }
-      }
     }
-  
+
     for (unsigned iv = 0; iv < CONFIG_T::n_vertices; ++iv) {
-      if (iv == nvtx[0])
-        break;
-  
-      for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
-        unsigned const ivo = iv * CONFIG_T::n_out_features + io;
-  
-        typename CONFIG_T::aggr_t acc = CONFIG_T::output_transform_biases[io];
-  
-        for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
-          unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
-          unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
-  
-          typename CONFIG_T::aggr_t aggr = 0.;
-  
-          for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
-            unsigned const iap = ia * CONFIG_T::n_propagate + ip;
-            unsigned const ioap = ioa * CONFIG_T::n_propagate + ip;
-  
-            aggr += CONFIG_T::output_transform_weights[ioap] * aggregated_features[iap];
-          }
-  
-          acc += edge_weights[iva] * aggr;
-        }
-  
-        res[ivo] = acc;
-      }
-    }
-  }
-
-  /* Reference (dumb) implementation returning (Features) - output averaged over vertices already */
-  template<class data_T, class nvtx_T, class res_T, typename CONFIG_T>
-  typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::collapse_mean>::type
-  garnet_ref(
-    data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features],
-    nvtx_T const nvtx[1],
-    res_T res[CONFIG_T::n_out_features]
-  )
-  {
+        if (iv == nvtx[0])
+            break;
+
+        for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
+            unsigned const ivo = iv * CONFIG_T::n_out_features + io;
+
+            typename CONFIG_T::aggr_t acc = CONFIG_T::output_transform_biases[io];
+
+            for (unsigned ia = 0; ia < CONFIG_T::n_aggregators; ++ia) {
+                unsigned const iva = iv * CONFIG_T::n_aggregators + ia;
+                unsigned const ioa = io * CONFIG_T::n_aggregators + ia;
+
+                typename CONFIG_T::aggr_t aggr = 0.;
+
+                for (unsigned ip = 0; ip < CONFIG_T::n_propagate; ++ip) {
+                    unsigned const iap = ia * CONFIG_T::n_propagate + ip;
+                    unsigned const ioap = ioa * CONFIG_T::n_propagate + ip;
+
+                    aggr += CONFIG_T::output_transform_weights[ioap] * aggregated_features[iap];
+                }
+
+                acc += edge_weights[iva] * aggr;
+            }
+
+            res[ivo] = acc;
+        }
+    }
+}
+
+/* Reference (dumb) implementation returning (Features) - output averaged over vertices already */
+template <class data_T, class nvtx_T, class res_T, typename CONFIG_T>
+typename std::enable_if<CONFIG_T::output_collapse == CONFIG_T::collapse_mean>::type
+garnet_ref(data_T const data[CONFIG_T::n_vertices * CONFIG_T::n_in_features], nvtx_T const nvtx[1],
+           res_T res[CONFIG_T::n_out_features]) {
     typename CONFIG_T::aggr_t vertex_res[CONFIG_T::n_vertices * CONFIG_T::n_out_features];
 
-    garnet_ref<CONFIG_T>(
-      data,
-      nvtx,
-      vertex_res
-    );
-  
+    garnet_ref<CONFIG_T>(data, nvtx, vertex_res);
+
     for (unsigned io = 0; io < CONFIG_T::n_out_features; ++io) {
-      typename CONFIG_T::aggr_t acc = 0.;
-  
-      for (unsigned iv = 0; iv < CONFIG_T::n_vertices; ++iv) {
-        if (iv == nvtx[0])
-          break;
-  
-        unsigned const ivo = iv * CONFIG_T::n_out_features + io;
-  
-        acc += vertex_res[ivo];
-      }
-  
-      if (CONFIG_T::mean_by_nvert)
-        acc /= nvtx[0];
-      else {
-        // Not using right shift in case aggr_t is float or double
-        acc /= CONFIG_T::n_vertices;
-      }
-  
-      res[io] = acc;
-    }
-  }
+        typename CONFIG_T::aggr_t acc = 0.;
 
+        for (unsigned iv = 0; iv < CONFIG_T::n_vertices; ++iv) {
+            if (iv == nvtx[0])
+                break;
+
+            unsigned const ivo = iv * CONFIG_T::n_out_features + io;
+
+            acc += vertex_res[ivo];
+        }
+
+        if (CONFIG_T::mean_by_nvert)
+            acc /= nvtx[0];
+        else {
+            // Not using right shift in case aggr_t is float or double
+            acc /= CONFIG_T::n_vertices;
+        }
+
+        res[io] = acc;
+    }
 }
 
+} // namespace nnet
+
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_helpers.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_helpers.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,49 +1,29 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_HELPERS_H
 #define NNET_HELPERS_H
 
+#include "hls_stream.h"
+#include <algorithm>
+#include <fstream>
+#include <iostream>
+#include <map>
+#include <math.h>
 #include <stdio.h>
 #include <stdlib.h>
-#include <math.h>
-#include <fstream>
-#include <algorithm>
 #include <vector>
-#include <map>
-#include <iostream>
-#include "hls_stream.h"
 
 namespace nnet {
 
 #ifndef __SYNTHESIS__
 
 #ifndef WEIGHTS_DIR
 #define WEIGHTS_DIR "weights"
 #endif
 
-template<class T, size_t SIZE>
-void load_weights_from_txt(T *w, const char* fname) {
+template <class T, size_t SIZE> void load_weights_from_txt(T *w, const char *fname) {
 
     std::string full_path = std::string(WEIGHTS_DIR) + "/" + std::string(fname);
     std::ifstream infile(full_path.c_str(), std::ios::binary);
 
     if (infile.fail()) {
         std::cerr << "ERROR: file " << std::string(fname) << " does not exist" << std::endl;
         exit(1);
@@ -51,28 +31,27 @@
 
     std::string line;
     if (std::getline(infile, line)) {
         std::istringstream iss(line);
         std::string token;
 
         size_t i = 0;
-        while(std::getline(iss, token, ',')) {
+        while (std::getline(iss, token, ',')) {
             std::istringstream(token) >> w[i];
             i++;
         }
 
         if (SIZE != i) {
             std::cerr << "ERROR: Expected " << SIZE << " values";
             std::cerr << " but read only " << i << " values" << std::endl;
         }
     }
 }
 
-template<class T, size_t SIZE>
-void load_compressed_weights_from_txt(T *w, const char* fname) {
+template <class T, size_t SIZE> void load_compressed_weights_from_txt(T *w, const char *fname) {
 
     std::string full_path = std::string(WEIGHTS_DIR) + "/" + std::string(fname);
     std::ifstream infile(full_path.c_str(), std::ios::binary);
 
     if (infile.fail()) {
         std::cerr << "ERROR: file " << std::string(fname) << " does not exist" << std::endl;
         exit(1);
@@ -81,44 +60,43 @@
     std::string line;
     if (std::getline(infile, line)) {
         std::istringstream iss(line);
         std::string token;
         std::string extra_chars = "} ";
 
         size_t i = 0;
-        while(std::getline(iss, token, '{')) {
+        while (std::getline(iss, token, '{')) {
             if (token.length() == 0) {
                 continue;
             }
-            for (char c: extra_chars) {
+            for (char c : extra_chars) {
                 token.erase(std::remove(token.begin(), token.end(), c), token.end());
             }
             if (token.back() == ',') {
                 token.erase(token.end() - 1);
             }
 
             std::replace(token.begin(), token.end(), ',', ' ');
             std::istringstream structss(token);
 
-            if(!(structss >> w[i].row_index >> w[i].col_index >> w[i].weight)) {
+            if (!(structss >> w[i].row_index >> w[i].col_index >> w[i].weight)) {
                 std::cerr << "ERROR: Unable to parse file " << std::string(fname);
                 exit(1);
             }
             i++;
         }
 
         if (SIZE != i) {
             std::cerr << "ERROR: Expected " << SIZE << " values";
             std::cerr << " but read only " << i << " values" << std::endl;
         }
     }
 }
 
-template<class T, size_t SIZE>
-void load_exponent_weights_from_txt(T *w, const char* fname) {
+template <class T, size_t SIZE> void load_exponent_weights_from_txt(T *w, const char *fname) {
 
     std::string full_path = std::string(WEIGHTS_DIR) + "/" + std::string(fname);
     std::ifstream infile(full_path.c_str(), std::ios::binary);
 
     if (infile.fail()) {
         std::cerr << "ERROR: file " << std::string(fname) << " does not exist" << std::endl;
         exit(1);
@@ -127,142 +105,137 @@
     std::string line;
     if (std::getline(infile, line)) {
         std::istringstream iss(line);
         std::string token;
         std::string extra_chars = "} ";
 
         size_t i = 0;
-        while(std::getline(iss, token, '{')) {
+        while (std::getline(iss, token, '{')) {
             if (token.length() == 0) {
                 continue;
             }
-            for (char c: extra_chars) {
+            for (char c : extra_chars) {
                 token.erase(std::remove(token.begin(), token.end(), c), token.end());
             }
             if (token.back() == ',') {
                 token.erase(token.end() - 1);
             }
 
             std::replace(token.begin(), token.end(), ',', ' ');
             std::istringstream structss(token);
 
-            if(!(structss >> w[i].sign >> w[i].weight)) {
+            if (!(structss >> w[i].sign >> w[i].weight)) {
                 std::cerr << "ERROR: Unable to parse file " << std::string(fname);
                 exit(1);
             }
             i++;
         }
 
         if (SIZE != i) {
             std::cerr << "ERROR: Expected " << SIZE << " values";
             std::cerr << " but read only " << i << " values" << std::endl;
         }
     }
 }
-template<class srcType, class dstType, size_t SIZE>
-void convert_data(srcType *src, dstType *dst) {
+template <class srcType, class dstType, size_t SIZE> void convert_data(srcType *src, dstType *dst) {
     for (size_t i = 0; i < SIZE; i++) {
         dst[i] = dstType(src[i]);
     }
 }
 
-template<class srcType, class dstType, size_t SIZE>
-void convert_data(srcType *src, hls::stream<dstType> &dst) {
+template <class srcType, class dstType, size_t SIZE> void convert_data(srcType *src, hls::stream<dstType> &dst) {
     for (size_t i = 0; i < SIZE / dstType::size; i++) {
         dstType ctype;
         for (size_t j = 0; j < dstType::size; j++) {
             ctype[j] = typename dstType::value_type(src[i * dstType::size + j]);
         }
         dst.write(ctype);
     }
 }
 
-template<class srcType, class dstType, size_t SIZE>
-void convert_data(hls::stream<srcType> &src, dstType *dst) {
+template <class srcType, class dstType, size_t SIZE> void convert_data(hls::stream<srcType> &src, dstType *dst) {
     for (size_t i = 0; i < SIZE / srcType::size; i++) {
         srcType ctype = src.read();
         for (size_t j = 0; j < srcType::size; j++) {
             dst[i * srcType::size + j] = dstType(ctype[j]);
         }
     }
 }
 
 extern bool trace_enabled;
 extern std::map<std::string, void *> *trace_outputs;
 extern size_t trace_type_size;
 
-template<class data_T, class save_T>
-void save_output_array(data_T *data, save_T *ptr, size_t layer_size) {
-    for(int i = 0; i < layer_size; i++) {
+template <class data_T, class save_T> void save_output_array(data_T *data, save_T *ptr, size_t layer_size) {
+    for (int i = 0; i < layer_size; i++) {
         ptr[i] = save_T(data[i]);
     }
 }
 
-template<class data_T, class save_T>
-void save_output_array(hls::stream<data_T> &data, save_T *ptr, size_t layer_size) {
+template <class data_T, class save_T> void save_output_array(hls::stream<data_T> &data, save_T *ptr, size_t layer_size) {
     for (size_t i = 0; i < layer_size / data_T::size; i++) {
         data_T ctype = data.read();
         for (size_t j = 0; j < data_T::size; j++) {
             ptr[i * data_T::size + j] = save_T(ctype[j]);
         }
         data.write(ctype);
     }
 }
 
 // We don't want to include save_T in this function because it will be inserted into myproject.cpp
 // so a workaround with element size is used
-template<class data_T>
-void save_layer_output(data_T *data, const char *layer_name, size_t layer_size) {
-    if (!trace_enabled) return;
-    
+template <class data_T> void save_layer_output(data_T *data, const char *layer_name, size_t layer_size) {
+    if (!trace_enabled)
+        return;
+
     if (trace_outputs) {
         if (trace_outputs->count(layer_name) > 0) {
             if (trace_type_size == 4) {
-                save_output_array<data_T, float>(data, (float *) (*trace_outputs)[layer_name], layer_size);
+                save_output_array<data_T, float>(data, (float *)(*trace_outputs)[layer_name], layer_size);
             } else if (trace_type_size == 8) {
-                save_output_array<data_T, double>(data, (double *) (*trace_outputs)[layer_name], layer_size);
+                save_output_array<data_T, double>(data, (double *)(*trace_outputs)[layer_name], layer_size);
             } else {
                 std::cout << "Unknown trace type!" << std::endl;
             }
         } else {
             std::cout << "Layer name: " << layer_name << " not found in debug storage!" << std::endl;
         }
     } else {
         std::ostringstream filename;
-        filename << "./tb_data/" << layer_name << "_output.log"; //TODO if run as a shared lib, path should be ../tb_data
+        filename << "./tb_data/" << layer_name << "_output.log"; // TODO if run as a shared lib, path should be ../tb_data
         std::fstream out;
         out.open(filename.str(), std::ios::app);
         assert(out.is_open());
-        for(int i = 0; i < layer_size; i++) {
+        for (int i = 0; i < layer_size; i++) {
             out << float(data[i]) << " "; // We don't care about precision in text files
         }
         out << std::endl;
         out.close();
     }
 }
 
-template<class data_T>
-void save_layer_output(hls::stream<data_T> &data, const char *layer_name, size_t layer_size) {
-    if (!trace_enabled) return;
-    
+template <class data_T> void save_layer_output(hls::stream<data_T> &data, const char *layer_name, size_t layer_size) {
+    if (!trace_enabled)
+        return;
+
     if (trace_outputs) {
         if (trace_outputs->count(layer_name) > 0) {
             if (trace_type_size == 4) {
-                save_output_array<data_T, float>(data, (float *) (*trace_outputs)[layer_name], layer_size);
+                save_output_array<data_T, float>(data, (float *)(*trace_outputs)[layer_name], layer_size);
             } else if (trace_type_size == 8) {
-                save_output_array<data_T, double>(data, (double *) (*trace_outputs)[layer_name], layer_size);
+                save_output_array<data_T, double>(data, (double *)(*trace_outputs)[layer_name], layer_size);
             } else {
                 std::cout << "Unknown trace type!" << std::endl;
             }
         } else {
             std::cout << "Layer name: " << layer_name << " not found in debug storage!" << std::endl;
         }
     } else {
         std::ostringstream filename;
-        filename << "./tb_data/" << layer_name << "_output.log"; //TODO if run as a shared lib, path should be ../tb_data
+        filename << "./tb_data/" << layer_name << "_output.log"; // TODO if run as a shared lib, path should be ../tb_data
         std::fstream out;
         out.open(filename.str(), std::ios::app);
         assert(out.is_open());
         for (size_t i = 0; i < layer_size / data_T::size; i++) {
             data_T ctype = data.read();
             for (size_t j = 0; j < data_T::size; j++) {
                 out << float(ctype[j]) << " "; // We don't care about precision in text files
@@ -270,25 +243,23 @@
             data.write(ctype);
         }
         out << std::endl;
         out.close();
     }
 }
 
-
 #endif
 
-template<class src_T, class dst_T, size_t OFFSET, size_t SIZE>
-void copy_data(std::vector<src_T> src, dst_T dst[SIZE]) {
+template <class src_T, class dst_T, size_t OFFSET, size_t SIZE> void copy_data(std::vector<src_T> src, dst_T dst[SIZE]) {
     typename std::vector<src_T>::const_iterator in_begin = src.cbegin() + OFFSET;
     typename std::vector<src_T>::const_iterator in_end = in_begin + SIZE;
     std::copy(in_begin, in_end, dst);
 }
 
-template<class src_T, class dst_T, size_t OFFSET, size_t SIZE>
+template <class src_T, class dst_T, size_t OFFSET, size_t SIZE>
 void copy_data(std::vector<src_T> src, hls::stream<dst_T> &dst) {
     typename std::vector<src_T>::const_iterator in_begin = src.cbegin() + OFFSET;
     typename std::vector<src_T>::const_iterator in_end = in_begin + SIZE;
 
     size_t i_pack = 0;
     dst_T dst_pack;
     for (typename std::vector<src_T>::const_iterator i = in_begin; i != in_end; ++i) {
@@ -296,138 +267,116 @@
         if (i_pack == dst_T::size) {
             i_pack = 0;
             dst.write(dst_pack);
         }
     }
 }
 
-template<class src_T, class dst_T, size_t OFFSET, size_t SIZE>
-void copy_data_axi(std::vector<src_T> src, dst_T dst[SIZE]) {
-    for(auto i = 0; i < SIZE; i++)
-    	if(i == SIZE - 1)
-    	{
-    		dst[i].data = src[i];
-    		dst[i].last = 1;
-    	}
-    	else
-    	{
-    		dst[i].data = src[i];
-    		dst[i].last = 0;
-    	}
-}
-
-template<class res_T, size_t SIZE>
-void print_result(res_T result[SIZE], std::ostream &out, bool keep = false) {
-    for(int i = 0; i < SIZE; i++) {
+template <class src_T, class dst_T, size_t OFFSET, size_t SIZE> void copy_data_axi(std::vector<src_T> src, dst_T dst[SIZE]) {
+    for (auto i = 0; i < SIZE; i++)
+        if (i == SIZE - 1) {
+            dst[i].data = src[i];
+            dst[i].last = 1;
+        } else {
+            dst[i].data = src[i];
+            dst[i].last = 0;
+        }
+}
+
+template <class res_T, size_t SIZE> void print_result(res_T result[SIZE], std::ostream &out, bool keep = false) {
+    for (int i = 0; i < SIZE; i++) {
         out << result[i] << " ";
     }
     out << std::endl;
 }
 
-template<class res_T, size_t SIZE>
-void print_result(hls::stream<res_T> &result, std::ostream &out, bool keep = false) {
-    for(int i = 0; i < SIZE / res_T::size; i++) {
+template <class res_T, size_t SIZE> void print_result(hls::stream<res_T> &result, std::ostream &out, bool keep = false) {
+    for (int i = 0; i < SIZE / res_T::size; i++) {
         res_T res_pack = result.read();
-        for(int j = 0; j < res_T::size; j++) {
+        for (int j = 0; j < res_T::size; j++) {
             out << res_pack[j] << " ";
         }
-        if (keep) result.write(res_pack);
+        if (keep)
+            result.write(res_pack);
     }
     out << std::endl;
 }
 
-template<class data_T, size_t SIZE>
-void fill_zero(data_T data[SIZE]) {
-    std::fill_n(data, SIZE, 0.);
-}
+template <class data_T, size_t SIZE> void fill_zero(data_T data[SIZE]) { std::fill_n(data, SIZE, 0.); }
 
-template<class data_T, size_t SIZE>
-void fill_zero(hls::stream<data_T> &data) {
-    for(int i = 0; i < SIZE / data_T::size; i++) {
+template <class data_T, size_t SIZE> void fill_zero(hls::stream<data_T> &data) {
+    for (int i = 0; i < SIZE / data_T::size; i++) {
         data_T data_pack;
-        for(int j = 0; j < data_T::size; j++) {
+        for (int j = 0; j < data_T::size; j++) {
             data_pack[j] = 0.;
         }
         data.write(data_pack);
     }
 }
 
-template <class dataType, unsigned int nrows>
-int read_file_1D(const char * filename, dataType data[nrows])
-{
-  FILE *fp;
-  fp = fopen(filename, "r");
-  if (fp == 0) {
-    return -1;
-  }
-  // Read data from file
-  float newval;
-  for (int ii = 0; ii < nrows; ii++){
-    if (fscanf(fp, "%f\n", &newval) != 0){
-      data[ii] = newval;
-    } else {
-      return -2;
+template <class dataType, unsigned int nrows> int read_file_1D(const char *filename, dataType data[nrows]) {
+    FILE *fp;
+    fp = fopen(filename, "r");
+    if (fp == 0) {
+        return -1;
+    }
+    // Read data from file
+    float newval;
+    for (int ii = 0; ii < nrows; ii++) {
+        if (fscanf(fp, "%f\n", &newval) != 0) {
+            data[ii] = newval;
+        } else {
+            return -2;
+        }
     }
-  }
-  fclose(fp);
-  return 0;
+    fclose(fp);
+    return 0;
 }
 
 template <class dataType, unsigned int nrows, unsigned int ncols>
-int read_file_2D(const char * filename, dataType data[nrows][ncols])
-{
-  FILE *fp;
-  fp = fopen(filename, "r");
-  if (fp == 0) {
-    return -1;
-  }
-  // Read data from file
-  float newval;
-  for (int ii = 0; ii < nrows; ii++) {
-    for (int jj = 0; jj < ncols; jj++){
-      if (fscanf(fp, "%f\n", &newval) != 0){
-        data[ii][jj] = newval;
-      } else {
-        return -2;
-      }
-    }
-  }
-  fclose(fp);
-  return 0;
-}
-
-template<class in_T, class out_T, int N_IN>
-void change_type(hls::stream<in_T> &in, hls::stream<out_T> &out)
-{
+int read_file_2D(const char *filename, dataType data[nrows][ncols]) {
+    FILE *fp;
+    fp = fopen(filename, "r");
+    if (fp == 0) {
+        return -1;
+    }
+    // Read data from file
+    float newval;
+    for (int ii = 0; ii < nrows; ii++) {
+        for (int jj = 0; jj < ncols; jj++) {
+            if (fscanf(fp, "%f\n", &newval) != 0) {
+                data[ii][jj] = newval;
+            } else {
+                return -2;
+            }
+        }
+    }
+    fclose(fp);
+    return 0;
+}
+
+template <class in_T, class out_T, int N_IN> void change_type(hls::stream<in_T> &in, hls::stream<out_T> &out) {
     in_T datareg;
     hls::stream<out_T> input_trunc;
-    for (int ii=0; ii<N_IN; ii++) {
-        out << (out_T) in.read();
+    for (int ii = 0; ii < N_IN; ii++) {
+        out << (out_T)in.read();
     }
 }
 
-template<class data_T, int N_IN>
-void  hls_stream_debug(hls::stream<data_T> &data, hls::stream<data_T> &res)
-{
+template <class data_T, int N_IN> void hls_stream_debug(hls::stream<data_T> &data, hls::stream<data_T> &res) {
     data_T datareg;
-    for (int ii=0; ii<N_IN; ii++) {
+    for (int ii = 0; ii < N_IN; ii++) {
         datareg = data.read();
         std::cout << "[" << ii << "]: " << datareg << std::endl;
         res << datareg;
     }
 }
 
-constexpr int ceillog2(int x){
-  return (x <= 2) ? 1 : 1 + ceillog2((x+1) / 2);
-}
+constexpr int ceillog2(int x) { return (x <= 2) ? 1 : 1 + ceillog2((x + 1) / 2); }
 
-constexpr int floorlog2(int x){
-  return (x < 2) ? 0 : 1 + floorlog2(x / 2);
-}
+constexpr int floorlog2(int x) { return (x < 2) ? 0 : 1 + floorlog2(x / 2); }
 
-constexpr int pow2(int x){
-  return x == 0 ? 1 : 2 * pow2(x - 1);
-}
+constexpr int pow2(int x) { return x == 0 ? 1 : 2 * pow2(x - 1); }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_image.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_image.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 #ifndef NNET_IMAGE_H_
 #define NNET_IMAGE_H_
 
-#include "nnet_common.h"
 #include "hls_stream.h"
+#include "nnet_common.h"
 #include <math.h>
 
 namespace nnet {
 
 struct resize_config {
     static const unsigned height = 10;
     static const unsigned width = 10;
     static const unsigned n_chan = 10;
     static const unsigned new_height = 10;
     static const unsigned new_width = 10;
 };
 
-template<class data_T, typename CONFIG_T>
-void resize_nearest(
-    data_T image[CONFIG_T::height * CONFIG_T::width * CONFIG_T::n_chan],
-    data_T resized[CONFIG_T::new_height * CONFIG_T::new_width * CONFIG_T::n_chan]
-) {
+template <class data_T, typename CONFIG_T>
+void resize_nearest(data_T image[CONFIG_T::height * CONFIG_T::width * CONFIG_T::n_chan],
+                    data_T resized[CONFIG_T::new_height * CONFIG_T::new_width * CONFIG_T::n_chan]) {
     int y_ratio = (int)((CONFIG_T::height << 16) / CONFIG_T::new_height) + 1;
     int x_ratio = (int)((CONFIG_T::width << 16) / CONFIG_T::new_width) + 1;
     int x2, y2;
 
     #pragma HLS PIPELINE
 
     for (int i = 0; i < CONFIG_T::new_height; i++) {
         for (int j = 0; j < CONFIG_T::new_width; j++) {
             x2 = ((j * x_ratio) >> 16);
             y2 = ((i * y_ratio) >> 16);
             for (int k = 0; k < CONFIG_T::n_chan; k++) {
-                resized[(i * CONFIG_T::new_width * CONFIG_T::n_chan) + j * CONFIG_T::n_chan + k] = image[(y2 * CONFIG_T::width * CONFIG_T::n_chan) + x2 * CONFIG_T::n_chan + k];
+                resized[(i * CONFIG_T::new_width * CONFIG_T::n_chan) + j * CONFIG_T::n_chan + k] =
+                    image[(y2 * CONFIG_T::width * CONFIG_T::n_chan) + x2 * CONFIG_T::n_chan + k];
             }
         }
-    }  
+    }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_merge.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_merge.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,302 +1,256 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_MERGE_H_
 #define NNET_MERGE_H_
 
+#include "hls_stream.h"
 #include "nnet_common.h"
 #include "nnet_mult.h"
-#include "hls_stream.h"
 #include <math.h>
 
 namespace nnet {
 
-struct merge_config
-{
+struct merge_config {
     static const unsigned n_elem = 10;
 };
 
 struct dot_config {
     static const unsigned n_in = 10;
     static const unsigned n_out = 1;
     static const unsigned reuse_factor = 1;
     typedef float accum_t;
     // Product function to use
-    template<class x_T, class y_T, class res_T>
-    using product = nnet::product::mult<x_T, y_T, res_T>;
+    template <class x_T, class y_T> using product = nnet::product::mult<x_T, y_T>;
 };
 
 struct concat_config {
     static const unsigned n_elem1_0 = 10;
     static const unsigned n_elem1_1 = 10;
     static const unsigned n_elem1_2 = 10;
     static const unsigned n_elem2_0 = 10;
     static const unsigned n_elem2_1 = 10;
     static const unsigned n_elem2_2 = 10;
 
     static const unsigned axis = -1;
 };
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void add(
-    input1_T data1[CONFIG_T::n_elem],
-    input2_T data2[CONFIG_T::n_elem],
-    res_T res[CONFIG_T::n_elem])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem; ii++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void add(input1_T data1[CONFIG_T::n_elem], input2_T data2[CONFIG_T::n_elem], res_T res[CONFIG_T::n_elem]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem; ii++) {
         res[ii] = data1[ii] + data2[ii];
     }
 }
 
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void subtract(input1_T data1[CONFIG_T::n_elem], input2_T data2[CONFIG_T::n_elem], res_T res[CONFIG_T::n_elem]) {
+    #pragma HLS PIPELINE
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void subtract(
-    input1_T data1[CONFIG_T::n_elem],
-    input2_T data2[CONFIG_T::n_elem],
-    res_T res[CONFIG_T::n_elem])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem; ii++) {
+    for (int ii = 0; ii < CONFIG_T::n_elem; ii++) {
         res[ii] = data1[ii] - data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void multiply(
-    input1_T data1[CONFIG_T::n_elem],
-    input2_T data2[CONFIG_T::n_elem],
-    res_T res[CONFIG_T::n_elem])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem; ii++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void multiply(input1_T data1[CONFIG_T::n_elem], input2_T data2[CONFIG_T::n_elem], res_T res[CONFIG_T::n_elem]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem; ii++) {
         res[ii] = data1[ii] * data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void average(
-    input1_T data1[CONFIG_T::n_elem],
-    input2_T data2[CONFIG_T::n_elem],
-    res_T res[CONFIG_T::n_elem])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem; ii++) {
-        res[ii] = (data1[ii] + data2[ii]) / (res_T) 2;
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void average(input1_T data1[CONFIG_T::n_elem], input2_T data2[CONFIG_T::n_elem], res_T res[CONFIG_T::n_elem]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem; ii++) {
+        res[ii] = (data1[ii] + data2[ii]) / (res_T)2;
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void maximum(
-    input1_T data1[CONFIG_T::n_elem],
-    input2_T data2[CONFIG_T::n_elem],
-    res_T res[CONFIG_T::n_elem])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem; ii++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void maximum(input1_T data1[CONFIG_T::n_elem], input2_T data2[CONFIG_T::n_elem], res_T res[CONFIG_T::n_elem]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem; ii++) {
         res[ii] = (data1[ii] > data2[ii]) ? data1[ii] : data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void minimum(
-    input1_T data1[CONFIG_T::n_elem],
-    input2_T data2[CONFIG_T::n_elem],
-    res_T res[CONFIG_T::n_elem])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem; ii++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void minimum(input1_T data1[CONFIG_T::n_elem], input2_T data2[CONFIG_T::n_elem], res_T res[CONFIG_T::n_elem]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem; ii++) {
         res[ii] = (data1[ii] < data2[ii]) ? data1[ii] : data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void dot1d(
-    input1_T data1[CONFIG_T::n_in],
-    input2_T data2[CONFIG_T::n_in],
-    res_T res[CONFIG_T::n_out])
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void dot1d(input1_T data1[CONFIG_T::n_in], input2_T data2[CONFIG_T::n_in], res_T res[CONFIG_T::n_out]) {
     #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
 
-    constexpr unsigned multiplier_limit = DIV_ROUNDUP(CONFIG_T::n_in, CONFIG_T::reuse_factor);
-    CONFIG_T::template product<input1_T, input2_T, typename CONFIG_T::accum_t>::limit(multiplier_limit);
+    #pragma HLS ALLOCATION operation instances=mul limit=CONFIG_T::multiplier_limit
 
     typename CONFIG_T::accum_t mult[CONFIG_T::n_in];
     #pragma HLS ARRAY_PARTITION variable=mult complete
     typename CONFIG_T::accum_t acc = 0;
 
-    Product: for(int i_mult=0; i_mult < CONFIG_T::n_in; i_mult++) {
+Product:
+    for (int i_mult = 0; i_mult < CONFIG_T::n_in; i_mult++) {
         #pragma HLS UNROLL
-        mult[i_mult] = CONFIG_T::template product<input1_T, input2_T, typename CONFIG_T::accum_t>::product(data1[i_mult], data2[i_mult]);
+        mult[i_mult] = CONFIG_T::template product<input1_T, input2_T>::product(data1[i_mult], data2[i_mult]);
     }
 
-    Accum: for(int i_acc = 0; i_acc < CONFIG_T::n_in; i_acc++) {
+Accum:
+    for (int i_acc = 0; i_acc < CONFIG_T::n_in; i_acc++) {
         #pragma HLS UNROLL
         acc += mult[i_acc];
     }
 
     res[0] = cast<input1_T, res_T, CONFIG_T>(acc);
 }
 
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate1d(input1_T data1[CONFIG_T::n_elem1_0], input2_T data2[CONFIG_T::n_elem2_0],
+                   res_T res[CONFIG_T::n_elem1_0 + CONFIG_T::n_elem2_0]) {
+    #pragma HLS PIPELINE
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate1d(
-    input1_T data1[CONFIG_T::n_elem1_0],
-    input2_T data2[CONFIG_T::n_elem2_0],
-    res_T res[CONFIG_T::n_elem1_0 + CONFIG_T::n_elem2_0])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem1_0; ii++) {
+    for (int ii = 0; ii < CONFIG_T::n_elem1_0; ii++) {
         res[ii] = data1[ii];
     }
-    for (int ii=0; ii<CONFIG_T::n_elem2_0; ii++) {
+    for (int ii = 0; ii < CONFIG_T::n_elem2_0; ii++) {
         res[CONFIG_T::n_elem1_0 + ii] = data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate2d_0(
-    input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1; ii++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate2d_0(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1],
+                     input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1],
+                     res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1; ii++) {
         res[ii] = data1[ii];
     }
-    for (int ii=0; ii<CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1; ii++) {
+    for (int ii = 0; ii < CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1; ii++) {
         res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + ii] = data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate2d_1(
-    input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem1_0; ii++) {
-        for (int jj=0; jj<CONFIG_T::n_elem1_1; jj++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate2d_1(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1],
+                     input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1],
+                     res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem1_0; ii++) {
+        for (int jj = 0; jj < CONFIG_T::n_elem1_1; jj++) {
             res[ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) + jj] = data1[ii * CONFIG_T::n_elem1_1 + jj];
         }
-        for (int jj=0; jj<CONFIG_T::n_elem2_1; jj++) {
-            res[ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) + CONFIG_T::n_elem1_1 + jj] = data2[ii * CONFIG_T::n_elem2_1 + jj];
+        for (int jj = 0; jj < CONFIG_T::n_elem2_1; jj++) {
+            res[ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) + CONFIG_T::n_elem1_1 + jj] =
+                data2[ii * CONFIG_T::n_elem2_1 + jj];
         }
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate2d(
-    input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1])
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate2d(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1],
+                   input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1],
+                   res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1]) {
+    #pragma HLS INLINE
+
     if (CONFIG_T::axis == 2 || CONFIG_T::axis == -1) {
         concatenate2d_1<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     } else {
         concatenate2d_0<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d_0(
-input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2; ii++) {
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d_0(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
+                     input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
+                     res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 +
+                               CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2; ii++) {
         res[ii] = data1[ii];
     }
-    for (int ii=0; ii<CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2; ii++) {
+    for (int ii = 0; ii < CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2; ii++) {
         res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + ii] = data2[ii];
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d_1(
-input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem1_0; ii++) {
-        for (int jj=0; jj<CONFIG_T::n_elem1_1; jj++) {
-            for (int kk=0; kk<CONFIG_T::n_elem1_2; kk++) {
-                int res_idx = ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) * CONFIG_T::n_elem1_2
-                            + jj * CONFIG_T::n_elem1_2
-                            + kk;
-                int data_idx = ii * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2
-                             + jj * CONFIG_T::n_elem1_2
-                             + kk;
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d_1(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
+                     input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
+                     res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 +
+                               CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem1_0; ii++) {
+        for (int jj = 0; jj < CONFIG_T::n_elem1_1; jj++) {
+            for (int kk = 0; kk < CONFIG_T::n_elem1_2; kk++) {
+                int res_idx =
+                    ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) * CONFIG_T::n_elem1_2 + jj * CONFIG_T::n_elem1_2 + kk;
+                int data_idx = ii * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + jj * CONFIG_T::n_elem1_2 + kk;
                 res[res_idx] = data1[data_idx];
             }
         }
-        for (int jj=0; jj<CONFIG_T::n_elem2_1; jj++) {
-            for (int kk=0; kk<CONFIG_T::n_elem2_2; kk++) {
-                int res_idx = ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) * CONFIG_T::n_elem1_2
-                            + (jj + CONFIG_T::n_elem1_1) * CONFIG_T::n_elem1_2
-                            + kk;
-                int data_idx = ii * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2
-                             + jj * CONFIG_T::n_elem2_2
-                             + kk;
+        for (int jj = 0; jj < CONFIG_T::n_elem2_1; jj++) {
+            for (int kk = 0; kk < CONFIG_T::n_elem2_2; kk++) {
+                int res_idx = ii * (CONFIG_T::n_elem1_1 + CONFIG_T::n_elem2_1) * CONFIG_T::n_elem1_2 +
+                              (jj + CONFIG_T::n_elem1_1) * CONFIG_T::n_elem1_2 + kk;
+                int data_idx = ii * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2 + jj * CONFIG_T::n_elem2_2 + kk;
                 res[res_idx] = data2[data_idx];
             }
         }
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d_2(
-    input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2])
-{
-    for (int ii=0; ii<CONFIG_T::n_elem1_0; ii++) {
-        for (int jj=0; jj<CONFIG_T::n_elem1_1; jj++) {
-            for (int kk=0; kk<CONFIG_T::n_elem1_2; kk++) {
-                int res_idx = ii * CONFIG_T::n_elem1_1 * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2)
-                            + jj * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2)
-                            + kk;
-                int data_idx = ii * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2
-                             + jj * CONFIG_T::n_elem1_2
-                             + kk;
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d_2(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
+                     input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
+                     res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 +
+                               CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2]) {
+    #pragma HLS PIPELINE
+
+    for (int ii = 0; ii < CONFIG_T::n_elem1_0; ii++) {
+        for (int jj = 0; jj < CONFIG_T::n_elem1_1; jj++) {
+            for (int kk = 0; kk < CONFIG_T::n_elem1_2; kk++) {
+                int res_idx = ii * CONFIG_T::n_elem1_1 * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2) +
+                              jj * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2) + kk;
+                int data_idx = ii * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + jj * CONFIG_T::n_elem1_2 + kk;
                 res[res_idx] = data1[data_idx];
             }
-            for (int kk=0; kk<CONFIG_T::n_elem1_2; kk++) {
-                int res_idx = ii * CONFIG_T::n_elem1_1 * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2)
-                            + jj * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2)
-                            + kk + CONFIG_T::n_elem1_2;
-                int data_idx = ii * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2
-                             + jj * CONFIG_T::n_elem2_2
-                             + kk;
+            for (int kk = 0; kk < CONFIG_T::n_elem1_2; kk++) {
+                int res_idx = ii * CONFIG_T::n_elem1_1 * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2) +
+                              jj * (CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_2) + kk + CONFIG_T::n_elem1_2;
+                int data_idx = ii * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2 + jj * CONFIG_T::n_elem2_2 + kk;
                 res[res_idx] = data2[data_idx];
             }
         }
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d(
-    input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
-    input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
-    res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 + CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2])
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d(input1_T data1[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2],
+                   input2_T data2[CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2],
+                   res_T res[CONFIG_T::n_elem1_0 * CONFIG_T::n_elem1_1 * CONFIG_T::n_elem1_2 +
+                             CONFIG_T::n_elem2_0 * CONFIG_T::n_elem2_1 * CONFIG_T::n_elem2_2]) {
+    #pragma HLS INLINE
+
     if (CONFIG_T::axis == 3 || CONFIG_T::axis == -1) {
         concatenate3d_2<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     } else if (CONFIG_T::axis == 2 || CONFIG_T::axis == -2) {
         concatenate3d_1<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     } else {
         concatenate3d_0<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_merge_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_merge_stream.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,304 +1,357 @@
-//
-//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
-//
-//    Copyright (C) 2017 EJ Kreinar
-//
-//    This program is free software: you can redistribute it and/or modify
-//    it under the terms of the GNU General Public License as published by
-//    the Free Software Foundation, either version 3 of the License, or
-//    (at your option) any later version.
-//
-//    This program is distributed in the hope that it will be useful,
-//    but WITHOUT ANY WARRANTY; without even the implied warranty of
-//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-//    GNU General Public License for more details.
-//
-//    You should have received a copy of the GNU General Public License
-//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-//
-
 #ifndef NNET_MERGE_STREAM_H_
 #define NNET_MERGE_STREAM_H_
 
-#include "nnet_common.h"
-#include "hls_stream.h"
-#include <math.h>
-
 namespace nnet {
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void add(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void add(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     assert(input1_T::size == input2_T::size && input1_T::size == res_T::size);
 
-    AddLoop: for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
-        #pragma HLS PIPELINE
-
-        input1_T in_data1 = data1.read();
-        input2_T in_data2 = data2.read();
-        res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
-
-        AddPack: for (int j = 0; j < res_T::size; j++) {
-            #pragma HLS UNROLL
-            out_data[j] = in_data1[j] + in_data2[j];
+AddLoop:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+
+        hls_register res_T out_data;
+
+    AddPack:
+        #pragma unroll
+        for (int j = 0; j < res_T::size; j++) {
+            out_data[j] = static_cast<typename res_T::value_type>(in_data1[j] + in_data2[j]);
         }
 
         res.write(out_data);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void subtract(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void subtract(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     assert(input1_T::size == input2_T::size && input1_T::size == res_T::size);
 
-    SubtractLoop: for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
-        #pragma HLS PIPELINE
-
-        input1_T in_data1 = data1.read();
-        input2_T in_data2 = data2.read();
-        res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
-
-        SubtractPack: for (int j = 0; j < res_T::size; j++) {
-            #pragma HLS UNROLL
-            out_data[j] = in_data1[j] - in_data2[j];
+SubtractLoop:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+
+        hls_register res_T out_data;
+
+    SubtractPack:
+        #pragma unroll
+        for (int j = 0; j < res_T::size; j++) {
+            out_data[j] = static_cast<typename res_T::value_type>(in_data1[j] - in_data2[j]);
         }
 
         res.write(out_data);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void multiply(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void multiply(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     assert(input1_T::size == input2_T::size && input1_T::size == res_T::size);
 
-    MultiplyLoop: for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
-
-        input1_T in_data1 = data1.read();
-        input2_T in_data2 = data2.read();
-        res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
-
-        MultiplyPack: for (int j = 0; j < res_T::size; j++) {
-            #pragma HLS UNROLL
-            out_data[j] = in_data1[j] * in_data2[j];
+MultLoop:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+
+        hls_register res_T out_data;
+
+    MultPack:
+        #pragma unroll
+        for (int j = 0; j < res_T::size; j++) {
+            out_data[j] = static_cast<typename res_T::value_type>(in_data1[j] * in_data2[j]);
         }
 
         res.write(out_data);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void average(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void average(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     assert(input1_T::size == input2_T::size && input1_T::size == res_T::size);
 
-    AverageLoop: for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
-
-        input1_T in_data1 = data1.read();
-        input2_T in_data2 = data2.read();
-        res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
-
-        AveragePack: for (int j = 0; j < res_T::size; j++) {
-            #pragma HLS UNROLL
-            out_data[j] = (in_data1[j] + in_data2[j]) / (typename res_T::value_type) 2;
+AvgLoop:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+
+        hls_register res_T out_data;
+
+    AvgPack:
+        #pragma unroll
+        for (int j = 0; j < res_T::size; j++) {
+            out_data[j] =
+                static_cast<typename res_T::value_type>((in_data1[j] + in_data2[j]) / (typename res_T::value_type)2);
         }
 
         res.write(out_data);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void maximum(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void maximum(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     assert(input1_T::size == input2_T::size && input1_T::size == res_T::size);
 
-    MaximumLoop: for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
-
-        input1_T in_data1 = data1.read();
-        input2_T in_data2 = data2.read();
-        res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
-
-        MaximumPack: for (int j = 0; j < res_T::size; j++) {
-            #pragma HLS UNROLL
-            out_data[j] = (in_data1[j] > in_data2[j]) ? in_data1[j] : in_data2[j];
+MaxLoop:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+
+        hls_register res_T out_data;
+
+    MaxPack:
+        #pragma unroll
+        for (int j = 0; j < res_T::size; j++) {
+            out_data[j] = static_cast<typename res_T::value_type>(out_data[j] = (in_data1[j] > in_data2[j]) ? in_data1[j]
+                                                                                                            : in_data2[j]);
         }
 
         res.write(out_data);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void minimum(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void minimum(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     assert(input1_T::size == input2_T::size && input1_T::size == res_T::size);
 
-    MinimumLoop: for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
+MinLoop:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+
+        hls_register res_T out_data;
+
+    MinPack:
+        #pragma unroll
+        for (int j = 0; j < res_T::size; j++) {
+            out_data[j] = static_cast<typename res_T::value_type>(out_data[j] = (in_data1[j] < in_data2[j]) ? in_data1[j]
+                                                                                                            : in_data2[j]);
+        }
+
+        res.write(out_data);
+    }
+}
+
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate1d(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+    hls_register res_T out_data;
+
+ConcatLoop1:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem1_0 / input1_T::size; i++) {
+        hls_register input1_T in_data1 = data1.read();
+    ConcatPack1:
+        #pragma unroll
+        for (int j = 0; j < input1_T::size; j++) {
+            out_data[j + (i * input1_T::size)] = static_cast<typename res_T::value_type>(in_data1[j]);
+        }
+    }
+
+ConcatLoop2:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem2_0 / input2_T::size; i++) {
+        hls_register input2_T in_data2 = data2.read();
+    ConcatPack2:
+        #pragma unroll
+        for (int j = 0; j < input2_T::size; j++) {
+            out_data[j + (i * input2_T::size) + (CONFIG_T::n_elem1_0)] =
+                static_cast<typename res_T::value_type>(in_data2[j]);
+        }
+    }
+    res.write(out_data);
+}
+
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate2d_0(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+ConcatLoopHeight1:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
+
+        hls_register input1_T in_data1 = data1.read();
+        hls_register res_T out_data;
+
+    ConcatPackInput1:
+        #pragma unroll
+        for (int k = 0; k < input1_T::size; k++) {
+            out_data[k] = static_cast<typename res_T::value_type>(in_data1[k]);
+        }
+
+        res.write(out_data);
+    }
+
+ConcatLoopHeight2:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem2_0; i++) {
+        hls_register input2_T in_data2 = data2.read();
+        hls_register res_T out_data;
+
+    ConcatPackInput2:
+        #pragma unroll
+        for (int k = 0; k < input2_T::size; k++) {
+            out_data[k] = static_cast<typename res_T::value_type>(in_data2[k]);
+        }
+
+        res.write(out_data);
+    }
+}
 
-        input1_T in_data1 = data1.read();
-        input2_T in_data2 = data2.read();
-        res_T out_data;
-        #pragma HLS DATA_PACK variable=out_data
-
-        MinimumPack: for (int j = 0; j < res_T::size; j++) {
-            #pragma HLS UNROLL
-            out_data[j] = (in_data1[j] < in_data2[j]) ? in_data1[j] : in_data2[j];
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate2d_1(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+ConcatLoopHeight:
+    #pragma ii 1
+    for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
+        hls_register input1_T in_data1 = data1.read();
+        hls_register input2_T in_data2 = data2.read();
+        hls_register res_T out_data;
+
+    ConcatPackInput1:
+        #pragma unroll
+        for (int k = 0; k < input1_T::size; k++) {
+            out_data[k] = static_cast<typename res_T::value_type>(in_data1[k]);
+        }
+
+    ConcatPackInput2:
+        #pragma unroll
+        for (int k = 0; k < input2_T::size; k++) {
+            out_data[input1_T::size + k] = static_cast<typename res_T::value_type>(in_data2[k]);
         }
 
         res.write(out_data);
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d_0(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
-    ConcatLoopHeight1: for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
-        ConcatLoopWidth1: for (int j = 0; j < CONFIG_T::n_elem1_1; j++) {
-            #pragma HLS PIPELINE II=1
-
-            input1_T in_data1 = data1.read();
-            res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
-
-            ConcatPackInput1: for (int k = 0; k < input1_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[k] = in_data1[k];
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate2d(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+    if (CONFIG_T::axis == 2 || CONFIG_T::axis == -1) {
+        concatenate2d_1<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
+    } else {
+        concatenate2d_0<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
+    }
+}
+
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d_0(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+ConcatLoopHeight1:
+    for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
+    ConcatLoopWidth1:
+        #pragma ii 1
+        for (int j = 0; j < CONFIG_T::n_elem1_1; j++) {
+
+            hls_register input1_T in_data1 = data1.read();
+            hls_register res_T out_data;
+        ConcatPackInput1:
+            #pragma unroll
+            for (int k = 0; k < input1_T::size; k++) {
+                out_data[k] = static_cast<typename res_T::value_type>(in_data1[k]);
             }
 
             res.write(out_data);
         }
     }
-    ConcatLoopHeight2: for (int i = 0; i < CONFIG_T::n_elem2_0; i++) {
-        ConcatLoopWidth2: for (int j = 0; j < CONFIG_T::n_elem2_1; j++) {
-            #pragma HLS PIPELINE II=1
-
-            input2_T in_data2 = data2.read();
-            res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
-
-            ConcatPackInput2: for (int k = 0; k < input2_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[k] = in_data2[k];
+
+ConcatLoopHeight2:
+    for (int i = 0; i < CONFIG_T::n_elem2_0; i++) {
+    ConcatLoopWidth2:
+        #pragma ii 1
+        for (int j = 0; j < CONFIG_T::n_elem2_1; j++) {
+
+            hls_register input2_T in_data2 = data2.read();
+            hls_register res_T out_data;
+
+        ConcatPackInput2:
+            #pragma unroll
+            for (int k = 0; k < input2_T::size; k++) {
+                out_data[k] = static_cast<typename res_T::value_type>(in_data2[k]);
             }
 
             res.write(out_data);
         }
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d_1(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
-    ConcatLoopHeight: for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
-        ConcatLoopWidth1: for (int j = 0; j < CONFIG_T::n_elem1_1; j++) {
-            #pragma HLS PIPELINE II=1
-
-            input1_T in_data1 = data1.read();
-            res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
-
-            ConcatPackInput1: for (int k = 0; k < input1_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[k] = in_data1[k];
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d_1(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+ConcatLoopHeight:
+    for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
+    ConcatLoopWidth1:
+        #pragma ii 1
+        for (int j = 0; j < CONFIG_T::n_elem1_1; j++) {
+
+            hls_register input1_T in_data1 = data1.read();
+            hls_register res_T out_data;
+
+        ConcatPackInput1:
+            #pragma unroll
+            for (int k = 0; k < input1_T::size; k++) {
+                out_data[k] = static_cast<typename res_T::value_type>(in_data1[k]);
             }
 
             res.write(out_data);
         }
-        ConcatLoopWidth2: for (int j = 0; j < CONFIG_T::n_elem2_1; j++) {
-            #pragma HLS PIPELINE II=1
-
-            input2_T in_data2 = data2.read();
-            res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
-
-            ConcatPackInput2: for (int k = 0; k < input2_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[k] = in_data2[k];
+    ConcatLoopWidth2:
+        #pragma ii 1
+        for (int j = 0; j < CONFIG_T::n_elem2_1; j++) {
+
+            hls_register input2_T in_data2 = data2.read();
+            hls_register res_T out_data;
+
+        ConcatPackInput2:
+            #pragma unroll
+            for (int k = 0; k < input2_T::size; k++) {
+                out_data[k] = static_cast<typename res_T::value_type>(in_data2[k]);
             }
 
             res.write(out_data);
         }
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d_2(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
-    ConcatLoopHeight: for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
-        ConcatLoopWidth: for (int j = 0; j < CONFIG_T::n_elem1_1; j++) {
-            #pragma HLS PIPELINE II=1
-
-            input1_T in_data1 = data1.read();
-            input2_T in_data2 = data2.read();
-            res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
-
-            ConcatPackInput1: for (int k = 0; k < input1_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[k] = in_data1[k];
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d_2(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
+ConcatLoopHeight:
+    for (int i = 0; i < CONFIG_T::n_elem1_0; i++) {
+    ConcatLoopWidth:
+        #pragma ii 1
+        for (int j = 0; j < CONFIG_T::n_elem1_1; j++) {
+
+            hls_register input1_T in_data1 = data1.read();
+            hls_register input2_T in_data2 = data2.read();
+            hls_register res_T out_data;
+
+        ConcatPackInput1:
+            #pragma unroll
+            for (int k = 0; k < input1_T::size; k++) {
+                out_data[k] = static_cast<typename res_T::value_type>(in_data1[k]);
             }
-            
-            ConcatPackInput2: for (int k = 0; k < input2_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[input1_T::size + k] = in_data2[k];
+
+        ConcatPackInput2:
+            #pragma unroll
+            for (int k = 0; k < input2_T::size; k++) {
+                out_data[input1_T::size + k] = static_cast<typename res_T::value_type>(in_data2[k]);
             }
 
             res.write(out_data);
         }
     }
 }
 
-template<class input1_T, class input2_T, class res_T, typename CONFIG_T>
-void concatenate3d(
-    hls::stream<input1_T> &data1,
-    hls::stream<input2_T> &data2,
-    hls::stream<res_T> &res)
-{
+template <class input1_T, class input2_T, class res_T, typename CONFIG_T>
+void concatenate3d(stream<input1_T> &data1, stream<input2_T> &data2, stream<res_T> &res) {
     if (CONFIG_T::axis == 3 || CONFIG_T::axis == -1) {
         concatenate3d_2<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     } else if (CONFIG_T::axis == 2 || CONFIG_T::axis == -2) {
         concatenate3d_1<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     } else {
         concatenate3d_0<input1_T, input2_T, res_T, CONFIG_T>(data1, data2, res);
     }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_padding.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_padding.h`

 * *Files 12% similar despite different names*

```diff
@@ -9,140 +9,137 @@
     static const unsigned n_chan = 10;
     static const unsigned in_width = 10;
     static const unsigned out_width = 10;
     static const unsigned pad_left = 0;
     static const unsigned pad_right = 0;
 };
 
-template<class data_T, class res_T, typename CONFIG_T>
-void zeropad1d_cf(
-    data_T data[CONFIG_T::n_chan * CONFIG_T::in_width],
-    data_T res[CONFIG_T::n_chan * CONFIG_T::out_width]
-) {
-    for(int j = 0; j < CONFIG_T::n_chan; j++) {
+template <class data_T, class res_T, typename CONFIG_T>
+void zeropad1d_cf(data_T data[CONFIG_T::n_chan * CONFIG_T::in_width], data_T res[CONFIG_T::n_chan * CONFIG_T::out_width]) {
+    #pragma HLS PIPELINE
+
+    for (int j = 0; j < CONFIG_T::n_chan; j++) {
         for (int i = 0; i < CONFIG_T::pad_left; i++) {
             *(res++) = 0;
         }
 
         for (int i = 0; i < CONFIG_T::in_width; i++) {
-            *(res++) = (res_T) *(data++);
+            *(res++) = (res_T) * (data++);
         }
 
         for (int i = 0; i < CONFIG_T::pad_right; i++) {
             *(res++) = 0;
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void zeropad1d_cl(
-    data_T data[CONFIG_T::n_chan * CONFIG_T::in_width],
-    res_T res[CONFIG_T::n_chan * CONFIG_T::out_width]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void zeropad1d_cl(data_T data[CONFIG_T::n_chan * CONFIG_T::in_width], res_T res[CONFIG_T::n_chan * CONFIG_T::out_width]) {
+    #pragma HLS PIPELINE
+
     for (int i = 0; i < CONFIG_T::pad_left; i++) {
-        for(int j = 0; j < CONFIG_T::n_chan; j++) {
+        for (int j = 0; j < CONFIG_T::n_chan; j++) {
             *(res++) = 0;
         }
     }
 
     for (int i = 0; i < CONFIG_T::in_width; i++) {
-        for(int j = 0; j < CONFIG_T::n_chan; j++) {
-            *(res++) = (res_T) *(data++);
+        for (int j = 0; j < CONFIG_T::n_chan; j++) {
+            *(res++) = (res_T) * (data++);
         }
     }
 
     for (int i = 0; i < CONFIG_T::pad_right; i++) {
-        for(int j = 0; j < CONFIG_T::n_chan; j++) {
+        for (int j = 0; j < CONFIG_T::n_chan; j++) {
             *(res++) = 0;
         }
     }
 }
 
-
 struct padding2d_config {
     static const unsigned n_chan = 10;
     static const unsigned in_height = 10;
     static const unsigned in_width = 10;
     static const unsigned out_height = 10;
     static const unsigned out_width = 10;
     static const unsigned pad_top = 0;
     static const unsigned pad_bottom = 0;
     static const unsigned pad_left = 0;
     static const unsigned pad_right = 0;
 };
 
-template<class data_T, class res_T, typename CONFIG_T>
-void zeropad2d_cf(
-    data_T data[CONFIG_T::n_chan * CONFIG_T::in_height * CONFIG_T::in_width],
-    data_T res[CONFIG_T::n_chan * CONFIG_T::out_height * CONFIG_T::out_width]
-) {
-    for(int k = 0; k < CONFIG_T::n_chan; k++) {
+template <class data_T, class res_T, typename CONFIG_T>
+void zeropad2d_cf(data_T data[CONFIG_T::n_chan * CONFIG_T::in_height * CONFIG_T::in_width],
+                  data_T res[CONFIG_T::n_chan * CONFIG_T::out_height * CONFIG_T::out_width]) {
+    #pragma HLS PIPELINE
+
+    for (int k = 0; k < CONFIG_T::n_chan; k++) {
 
         for (int i = 0; i < CONFIG_T::pad_top; i++) {
             for (int j = 0; j < CONFIG_T::out_width; j++) {
                 *(res++) = 0;
             }
         }
 
         for (int i = 0; i < CONFIG_T::in_height; i++) {
             for (int j = 0; j < CONFIG_T::pad_left; j++) {
                 *(res++) = 0;
             }
             for (int j = 0; j < CONFIG_T::in_width; j++) {
-                *(res++) = (res_T) *(data++);
+                *(res++) = (res_T) * (data++);
             }
             for (int j = 0; j < CONFIG_T::pad_right; j++) {
                 *(res++) = 0;
             }
         }
 
         for (int i = 0; i < CONFIG_T::pad_bottom; i++) {
             for (int j = 0; j < CONFIG_T::out_width; j++) {
                 *(res++) = 0;
             }
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void zeropad2d_cl(
-    data_T data[CONFIG_T::n_chan * CONFIG_T::in_height * CONFIG_T::in_width],
-    res_T res[CONFIG_T::n_chan * CONFIG_T::out_height * CONFIG_T::out_width]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void zeropad2d_cl(data_T data[CONFIG_T::n_chan * CONFIG_T::in_height * CONFIG_T::in_width],
+                  res_T res[CONFIG_T::n_chan * CONFIG_T::out_height * CONFIG_T::out_width]) {
+    #pragma HLS PIPELINE
+
     for (int i = 0; i < CONFIG_T::pad_top; i++) {
         for (int j = 0; j < CONFIG_T::out_width; j++) {
-            for(int k = 0; k < CONFIG_T::n_chan; k++) {
+            for (int k = 0; k < CONFIG_T::n_chan; k++) {
                 *(res++) = 0;
             }
         }
     }
 
     for (int i = 0; i < CONFIG_T::in_height; i++) {
         for (int j = 0; j < CONFIG_T::pad_left; j++) {
-            for(int k = 0; k < CONFIG_T::n_chan; k++) {
+            for (int k = 0; k < CONFIG_T::n_chan; k++) {
                 *(res++) = 0;
             }
         }
         for (int j = 0; j < CONFIG_T::in_width; j++) {
-            for(int k = 0; k < CONFIG_T::n_chan; k++) {
-                *(res++) = (res_T) *(data++);
+            for (int k = 0; k < CONFIG_T::n_chan; k++) {
+                *(res++) = (res_T) * (data++);
             }
         }
         for (int j = 0; j < CONFIG_T::pad_right; j++) {
-            for(int k = 0; k < CONFIG_T::n_chan; k++) {
+            for (int k = 0; k < CONFIG_T::n_chan; k++) {
                 *(res++) = 0;
             }
         }
     }
 
     for (int i = 0; i < CONFIG_T::pad_bottom; i++) {
         for (int j = 0; j < CONFIG_T::out_width; j++) {
-            for(int k = 0; k < CONFIG_T::n_chan; k++) {
+            for (int k = 0; k < CONFIG_T::n_chan; k++) {
                 *(res++) = 0;
             }
         }
     }
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_padding_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_padding_stream.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,82 +1,85 @@
 #ifndef NNET_PADDING_STREAM_H_
 #define NNET_PADDING_STREAM_H_
 
 #include <math.h>
 
 namespace nnet {
 
-template<class res_T, typename CONFIG_T>
-void fill_zero(hls::stream<res_T> &res) {
+template <class res_T, typename CONFIG_T> void fill_zero(hls::stream<res_T> &res) {
     #pragma HLS INLINE
     res_T res_part;
-	for (int c = 0; c < CONFIG_T::n_chan; c++) {
+    for (int c = 0; c < CONFIG_T::n_chan; c++) {
         #pragma HLS UNROLL
-	    res_part[c] = 0;
+        res_part[c] = 0;
     }
     res.write(res_part);
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void fill_data(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, typename CONFIG_T> void fill_data(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     #pragma HLS INLINE
     data_T data_part = data.read();
     res_T res_part;
     for (int c = 0; c < CONFIG_T::n_chan; c++) {
         #pragma HLS UNROLL
         res_part[c] = data_part[c];
     }
     res.write(res_part);
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void zeropad1d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res
-) {
-    PadLeft: for (int i = 0; i < CONFIG_T::pad_left; i++) {
+template <class data_T, class res_T, typename CONFIG_T>
+void zeropad1d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+PadLeft:
+    for (int i = 0; i < CONFIG_T::pad_left; i++) {
         fill_zero<res_T, CONFIG_T>(res);
     }
 
-    CopyMain: for (int i = 0; i < CONFIG_T::in_width; i++) {
+CopyMain:
+    for (int i = 0; i < CONFIG_T::in_width; i++) {
         fill_data<data_T, res_T, CONFIG_T>(data, res);
     }
 
-    PadRight: for (int i = 0; i < CONFIG_T::pad_right; i++) {
+PadRight:
+    for (int i = 0; i < CONFIG_T::pad_right; i++) {
         fill_zero<res_T, CONFIG_T>(res);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void zeropad2d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void zeropad2d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
 
-    PadTop: for (int i = 0; i < CONFIG_T::pad_top; i++) {
-        PadTopWidth: for (int j = 0; j < CONFIG_T::out_width; j++) {
+PadTop:
+    for (int i = 0; i < CONFIG_T::pad_top; i++) {
+    PadTopWidth:
+        for (int j = 0; j < CONFIG_T::out_width; j++) {
             fill_zero<res_T, CONFIG_T>(res);
         }
     }
 
-    PadMain: for (int i = 0; i < CONFIG_T::in_height; i++) {
-        PadLeft: for (int j = 0; j < CONFIG_T::pad_left; j++) {
+PadMain:
+    for (int i = 0; i < CONFIG_T::in_height; i++) {
+    PadLeft:
+        for (int j = 0; j < CONFIG_T::pad_left; j++) {
             fill_zero<res_T, CONFIG_T>(res);
         }
-        CopyMain: for (int j = 0; j < CONFIG_T::in_width; j++) {
+    CopyMain:
+        for (int j = 0; j < CONFIG_T::in_width; j++) {
             fill_data<data_T, res_T, CONFIG_T>(data, res);
         }
-        PadRight: for (int j = 0; j < CONFIG_T::pad_right; j++) {
+    PadRight:
+        for (int j = 0; j < CONFIG_T::pad_right; j++) {
             fill_zero<res_T, CONFIG_T>(res);
         }
     }
 
-    PadBottom: for (int i = 0; i < CONFIG_T::pad_bottom; i++) {
-        PadBottomWidth: for (int j = 0; j < CONFIG_T::out_width; j++) {
+PadBottom:
+    for (int i = 0; i < CONFIG_T::pad_bottom; i++) {
+    PadBottomWidth:
+        for (int j = 0; j < CONFIG_T::out_width; j++) {
             fill_zero<res_T, CONFIG_T>(res);
         }
     }
 }
 
-}
+} // namespace nnet
 
-#endif
+#endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_pooling.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_pooling.h`

 * *Files 17% similar despite different names*

```diff
@@ -1,298 +1,341 @@
 #ifndef NNET_POOLING_H_
 #define NNET_POOLING_H_
 
-#include <iostream>
 #include "nnet_helpers.h"
+#include <iostream>
 
-namespace nnet{
+namespace nnet {
 
 // Return the maximum value from an array
-template<typename T, int N>
-T max(T x[N]){
-  T y = x[0];
-  for(int i = 1; i < N; i++){
-    y = x[i] > y ? x[i] : y;
-  }
-  return y;
-}
-
-template<int W, int N>
-ap_int<W> avg(ap_int<W> (&x)[N]){
-  // Use a wider accumulator than the input to avoid overflow
-  ap_int<W + ceillog2(N)> tmp = 0;
-  for(int i = 0; i < N; i++){
-    tmp += x[i];
-  }
-  tmp /= N;
-  // Now cast back to original type
-  ap_int<W> y = tmp;
-  return tmp;
-}
-
-template<int W, int I, int N>
-ap_fixed<W, I> avg(ap_fixed<W, I> (&x)[N]){
-  // Use a wider accumulator than the input to avoid overflow
-  ap_fixed<W + ceillog2(N), I + ceillog2(N)> tmp = 0;
-  for(int i = 0; i < N; i++){
-    tmp += x[i];
-  }
-  tmp /= N;
-  // Now cast back to original type
-  ap_fixed<W, I> y = tmp;
-  return y;
+template <typename T, int N> T max(T x[N]) {
+    T y = x[0];
+    for (int i = 1; i < N; i++) {
+        y = x[i] > y ? x[i] : y;
+    }
+    return y;
+}
+
+template <int W, int N> ap_int<W> avg(ap_int<W> (&x)[N]) {
+    // Use a wider accumulator than the input to avoid overflow
+    ap_int<W + ceillog2(N)> tmp = 0;
+    for (int i = 0; i < N; i++) {
+        tmp += x[i];
+    }
+    tmp /= N;
+    // Now cast back to original type
+    ap_int<W> y = tmp;
+    return tmp;
+}
+
+template <int W, int I, int N> ap_fixed<W, I> avg(ap_fixed<W, I> (&x)[N]) {
+    // Use a wider accumulator than the input to avoid overflow
+    ap_fixed<W + ceillog2(N), I + ceillog2(N)> tmp = 0;
+    for (int i = 0; i < N; i++) {
+        tmp += x[i];
+    }
+    tmp /= N;
+    // Now cast back to original type
+    ap_fixed<W, I> y = tmp;
+    return y;
 }
 
 // Return the mean value of an array
-template<typename T, int N>
-T avg(T (&x)[N]){
-  T y = 0;
-  for(int i = 0; i < N; i++){
-    y += x[i];
-  }
-  y /= N;
-  return y;
+template <typename T, int N> T avg(T (&x)[N]) {
+    T y = 0;
+    for (int i = 0; i < N; i++) {
+        y += x[i];
+    }
+    y /= N;
+    return y;
 }
 
 // Enumeration for pooling operation (max, avg, l2norm pooling)
 enum Pool_Op { Max, Average }; // L2Norm };
-template<typename T, int N, Pool_Op op>
-T pool_op(T (&x)[N]){
-	switch(op){
-	case Max: return max<T, N>(x);
-	case Average: return avg(x);
-	// case L2Norm: return l2norm<T, N>(x);
-	}
-}
-
-template<typename T, Pool_Op op>
-T pad_val(){
-  /*---
-  *- In Tensorflow, pooling ignores the value in the padded cells
-  *- For Avg pooling, return 0 (the divisior is modified to the
-  *- area overlapping the unpadded image.
-  *- For max pooling, return the most negative value for the type.
-  *- TODO this is not really generic, it assumes fixed point or integer T
-  ---*/
-  switch(op){
-    case Max:{ 
-      T x = 0;
-      x[x.width - 1] = 1;
-      return x;
-      break;}
-    case Average: return 0;
-  }
-}
-
-struct pooling1d_config{
-  // IO size
-  static const unsigned n_in = 10;
-  static const unsigned pool_width = 2;
-  static const unsigned stride_width = 2;
-  static const unsigned n_out = (n_in - pool_width) / stride_width + 1;
-  static const unsigned pad_left = 0;
-  static const unsigned pad_right = 0;
-  // Pooling function
-  static const Pool_Op pool_op = Max;
+template <typename T, int N, Pool_Op op> T pool_op(T (&x)[N]) {
+    switch (op) {
+    case Max:
+        return max<T, N>(x);
+    case Average:
+        return avg(x);
+        // case L2Norm: return l2norm<T, N>(x);
+    }
+}
+
+template <typename T, Pool_Op op> T pad_val() {
+    /*---
+     *- In Tensorflow, pooling ignores the value in the padded cells
+     *- For Avg pooling, return 0 (the divisior is modified to the
+     *- area overlapping the unpadded image.
+     *- For max pooling, return the most negative value for the type.
+     *- TODO this is not really generic, it assumes fixed point or integer T
+    ---*/
+    switch (op) {
+    case Max: {
+        T x = 0;
+        x[x.width - 1] = 1;
+        return x;
+        break;
+    }
+    case Average:
+        return 0;
+    }
+}
+
+struct pooling1d_config {
+    // IO size
+    static const unsigned n_in = 10;
+    static const unsigned pool_width = 2;
+    static const unsigned stride_width = 2;
+    static const unsigned n_out = (n_in - pool_width) / stride_width + 1;
+    static const unsigned pad_left = 0;
+    static const unsigned pad_right = 0;
+    // Pooling function
+    static const Pool_Op pool_op = Max;
 };
 
-template<typename CONFIG_T>
-constexpr int pool_op_limit_1d() {
-  return CONFIG_T::n_in * CONFIG_T::n_filt / CONFIG_T::reuse;
+template <typename CONFIG_T> constexpr int pool_op_limit_1d() {
+    return CONFIG_T::n_in * CONFIG_T::n_filt / CONFIG_T::reuse_factor;
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void pooling1d_cl(data_T data[CONFIG_T::n_in * CONFIG_T::n_filt], res_T res[CONFIG_T::n_out * CONFIG_T::n_filt]) {
+    #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
+
+    // TODO partition the arrays according to the reuse factor
+    const int limit = pool_op_limit_1d<CONFIG_T>();
+    #pragma HLS ALLOCATION function instances=CONFIG_T::pool_op limit=limit
+    // Add any necessary padding
+    unsigned padded_width = CONFIG_T::n_in + CONFIG_T::pad_left + CONFIG_T::pad_right;
+    if (CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0) {
+        padded_width -= padded_width - (padded_width / CONFIG_T::stride_width * CONFIG_T::stride_width);
+    }
 
-  // TODO partition the arrays according to the reuse factor
-  const int limit = pool_op_limit_1d<CONFIG_T>();
-  #pragma HLS ALLOCATION instances=pool_op limit=limit function
-  // Add any necessary padding
-  unsigned padded_width = CONFIG_T::n_in + CONFIG_T::pad_left + CONFIG_T::pad_right;
-  if (CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0) {
-    padded_width -= padded_width - (padded_width / CONFIG_T::stride_width * CONFIG_T::stride_width);
-  }
-
-  for(int ff = 0; ff < CONFIG_T::n_filt; ff++) {
-    // Loop over input image x in steps of stride
-    for(int ii = 0; ii < padded_width; ii += CONFIG_T::stride_width) {
-      data_T pool[CONFIG_T::pool_width];
-      // Keep track of number of pixels in image vs padding region
-      unsigned img_overlap = 0;
-      // Loop over pool window x
-      for(int jj = 0; jj < CONFIG_T::stride_width; jj++) {
-        if(ii+jj < CONFIG_T::pad_left || ii+jj >= (padded_width - CONFIG_T::pad_right)) {
-          // Add padding
-          pool[jj] = pad_val<data_T, CONFIG_T::pool_op>();
-        }else{
-          pool[jj] = data[(ii + jj) * CONFIG_T::n_filt + ff];
-          img_overlap++;
+    for (int ff = 0; ff < CONFIG_T::n_filt; ff++) {
+        // Loop over input image x in steps of stride
+        for (int ii = 0; ii < padded_width; ii += CONFIG_T::stride_width) {
+            data_T pool[CONFIG_T::pool_width];
+            #pragma HLS ARRAY_PARTITION variable=pool complete dim=0
+            // Keep track of number of pixels in image vs padding region
+            unsigned img_overlap = 0;
+            // Loop over pool window x
+            for (int jj = 0; jj < CONFIG_T::stride_width; jj++) {
+                if (ii + jj < CONFIG_T::pad_left || ii + jj >= (padded_width - CONFIG_T::pad_right)) {
+                    // Add padding
+                    pool[jj] = pad_val<data_T, CONFIG_T::pool_op>();
+                } else {
+                    pool[jj] = data[(ii + jj - CONFIG_T::pad_left) * CONFIG_T::n_filt + ff];
+                    img_overlap++;
+                }
+            }
+            // do the pooling
+            // TODO in the case of average pooling, need to reduce width to area of pool window
+            // not overlapping padding region
+            res[(ii / CONFIG_T::stride_width) * CONFIG_T::n_filt + ff] =
+                pool_op<data_T, CONFIG_T::pool_width, CONFIG_T::pool_op>(pool);
+            // If the pool op is Average, the zero-padding needs to be removed from the results
+            if (CONFIG_T::pool_op == Average) {
+                data_T rescale = static_cast<data_T>(CONFIG_T::pool_width) / img_overlap;
+                res[(ii / CONFIG_T::stride_width) * CONFIG_T::n_filt + ff] *= rescale;
+            }
         }
-      }
-      // do the pooling
-      // TODO in the case of average pooling, need to reduce width to area of pool window
-      // not overlapping padding region
-      res[(ii/CONFIG_T::stride_width)* CONFIG_T::n_filt + ff] =
-          pool_op<data_T, CONFIG_T::pool_width, CONFIG_T::pool_op>(pool);
-      // If the pool op is Average, the zero-padding needs to be removed from the results
-      if(CONFIG_T::pool_op == Average) {
-        data_T rescale = CONFIG_T::pool_width / img_overlap;
-        res[(ii/CONFIG_T::stride_width)* CONFIG_T::n_filt + ff] *= rescale;
-      }
-	  }
-  }
+    }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void global_pooling1d_cl(data_T data[CONFIG_T::n_in * CONFIG_T::n_filt], res_T res[CONFIG_T::n_filt]) {
-  assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
-  assert(CONFIG_T::pool_width == CONFIG_T::stride_width);
+    #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
+
+    assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
+    assert(CONFIG_T::pool_width == CONFIG_T::stride_width);
+
+    // TODO partition the arrays according to the reuse factor
+    const int limit = pool_op_limit_1d<CONFIG_T>();
+    #pragma HLS ALLOCATION function instances=CONFIG_T::pool_op limit=limit
+
+    for (int ff = 0; ff < CONFIG_T::n_filt; ff++) {
+        data_T pool[CONFIG_T::n_in];
+        #pragma HLS ARRAY_PARTITION variable=pool complete dim=0
+        for (int jj = 0; jj < CONFIG_T::n_in; jj++) {
+            pool[jj] = data[jj * CONFIG_T::n_filt + ff];
+        }
+        // do the pooling
+        res[ff] = pool_op<data_T, CONFIG_T::n_in, CONFIG_T::pool_op>(pool);
+    }
+}
 
-  // TODO partition the arrays according to the reuse factor
-  const int limit = pool_op_limit_1d<CONFIG_T>();
-  #pragma HLS ALLOCATION instances=pool_op limit=limit function
-
-  for(int ff = 0; ff < CONFIG_T::n_filt; ff++) {
-    data_T pool[CONFIG_T::n_in];
-    for(int jj = 0; jj < CONFIG_T::n_in; jj++) {
-      pool[jj] = data[jj * CONFIG_T::n_filt + ff];
-    }
-  // do the pooling
-  res[ff] = pool_op<data_T, CONFIG_T::n_in, CONFIG_T::pool_op>(pool);
-  }
-}
-
-struct pooling2d_config{
-  // IO size
-  static const unsigned in_height = 10;
-  static const unsigned in_width = 10;
-  static const unsigned n_filt = 4;
-  static const unsigned stride_height = 2;
-  static const unsigned stride_width = 2;
-  static const unsigned pool_height = 2;
-  static const unsigned pool_width = 2;
-  static const unsigned out_height = (in_height - pool_height) / stride_height + 1;
-  static const unsigned out_width = (in_width - pool_width) / stride_width + 1;
-  // Padding
-  static const unsigned pad_top = 0;
-  static const unsigned pad_bottom = 0;
-  static const unsigned pad_left = 0;
-  static const unsigned pad_right = 0;
-  // Pooling function
-  static const Pool_Op pool_op = Max;
-  // Reuse
-  static const unsigned reuse = 1;
-  
-  // Internal data type definitions
-  typedef float accum_t;
+struct pooling2d_config {
+    // IO size
+    static const unsigned in_height = 10;
+    static const unsigned in_width = 10;
+    static const unsigned n_filt = 4;
+    static const unsigned stride_height = 2;
+    static const unsigned stride_width = 2;
+    static const unsigned pool_height = 2;
+    static const unsigned pool_width = 2;
+    static const unsigned out_height = (in_height - pool_height) / stride_height + 1;
+    static const unsigned out_width = (in_width - pool_width) / stride_width + 1;
+    // Padding
+    static const unsigned pad_top = 0;
+    static const unsigned pad_bottom = 0;
+    static const unsigned pad_left = 0;
+    static const unsigned pad_right = 0;
+    // Pooling function
+    static const Pool_Op pool_op = Max;
+    // Reuse factor
+    static const unsigned reuse_factor = 1;
+
+    // Internal data type definitions
+    typedef float accum_t;
 };
 
-template<typename CONFIG_T>
-constexpr int pool_op_limit(){
-  return (CONFIG_T::out_height * CONFIG_T::out_width) * CONFIG_T::n_filt / CONFIG_T::reuse;
+template <typename CONFIG_T> constexpr int pool_op_limit() {
+    return (CONFIG_T::out_height * CONFIG_T::out_width) * CONFIG_T::n_filt / CONFIG_T::reuse_factor;
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void pooling2d_cl(data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_filt],
-               res_T res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt]){
+                  res_T res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt]) {
+    #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
 
-  // TODO partition the arrays according to the reuse factor
-  const int limit = pool_op_limit<CONFIG_T>();
-  #pragma HLS ALLOCATION instances=pool_op limit=limit function
-  // Add any necessary padding
-  unsigned padded_height = CONFIG_T::in_height + CONFIG_T::pad_top + CONFIG_T::pad_bottom;
-  unsigned padded_width = CONFIG_T::in_width + CONFIG_T::pad_left + CONFIG_T::pad_right;
-  if (CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0) {
-    padded_height -= padded_height - (padded_height / CONFIG_T::stride_height * CONFIG_T::stride_height);
-    padded_width -= padded_width - (padded_width / CONFIG_T::stride_width * CONFIG_T::stride_width);
-  }
-
-  for(int ff = 0; ff < CONFIG_T::n_filt; ff++){
-	  // Loop over input image y in steps of stride
-	  for(int ii = 0; ii < padded_height; ii += CONFIG_T::stride_height){
-		  // Loop over input image x in steps of stride
-		  for(int jj = 0; jj < padded_width; jj += CONFIG_T::stride_width){
-			  data_T pool[CONFIG_T::pool_height * CONFIG_T::pool_width];
-        // Keep track of number of pixels in image vs padding region
-        unsigned img_overlap = 0;
-			  // Loop over pool window y
-			  for(int kk = 0; kk < CONFIG_T::stride_height; kk++){
-				  // Loop over pool window x
-				  for(int ll = 0; ll < CONFIG_T::stride_width; ll++){
-            if(ii+kk < CONFIG_T::pad_top || ii+kk >= (padded_height - CONFIG_T::pad_bottom) || jj+ll < CONFIG_T::pad_left || jj+ll >= (padded_width - CONFIG_T::pad_right)){
-              // Add padding
-              pool[kk * CONFIG_T::stride_width + ll] = pad_val<data_T, CONFIG_T::pool_op>();
-            }else{
-  					  pool[kk * CONFIG_T::stride_width + ll] = data[(ii + kk) * CONFIG_T::in_width * CONFIG_T::n_filt + (jj + ll) * CONFIG_T::n_filt + ff];
-              img_overlap++;
+    // TODO partition the arrays according to the reuse factor
+    const int limit = pool_op_limit<CONFIG_T>();
+    #pragma HLS ALLOCATION function instances=CONFIG_T::pool_op limit=limit
+    // Add any necessary padding
+    unsigned padded_height = CONFIG_T::in_height + CONFIG_T::pad_top + CONFIG_T::pad_bottom;
+    unsigned padded_width = CONFIG_T::in_width + CONFIG_T::pad_left + CONFIG_T::pad_right;
+    if (CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0) {
+        padded_height -= padded_height - (padded_height / CONFIG_T::stride_height * CONFIG_T::stride_height);
+        padded_width -= padded_width - (padded_width / CONFIG_T::stride_width * CONFIG_T::stride_width);
+    }
+
+    for (int ff = 0; ff < CONFIG_T::n_filt; ff++) {
+        // Loop over input image y in steps of stride
+        for (int ii = 0; ii < padded_height; ii += CONFIG_T::stride_height) {
+            // Loop over input image x in steps of stride
+            for (int jj = 0; jj < padded_width; jj += CONFIG_T::stride_width) {
+                data_T pool[CONFIG_T::pool_height * CONFIG_T::pool_width];
+                #pragma HLS ARRAY_PARTITION variable=pool complete dim=0
+                // Keep track of number of pixels in image vs padding region
+                unsigned img_overlap = 0;
+                // Loop over pool window y
+                for (int kk = 0; kk < CONFIG_T::stride_height; kk++) {
+                    // Loop over pool window x
+                    for (int ll = 0; ll < CONFIG_T::stride_width; ll++) {
+                        if (ii + kk < CONFIG_T::pad_top || ii + kk >= (padded_height - CONFIG_T::pad_bottom) ||
+                            jj + ll < CONFIG_T::pad_left || jj + ll >= (padded_width - CONFIG_T::pad_right)) {
+                            // Add padding
+                            pool[kk * CONFIG_T::stride_width + ll] = pad_val<data_T, CONFIG_T::pool_op>();
+                        } else {
+                            pool[kk * CONFIG_T::stride_width + ll] =
+                                data[(ii + kk - CONFIG_T::pad_top) * CONFIG_T::in_width * CONFIG_T::n_filt +
+                                     (jj + ll - CONFIG_T::pad_left) * CONFIG_T::n_filt + ff];
+                            img_overlap++;
+                        }
+                    }
+                }
+                // do the pooling
+                // TODO in the case of average pooling, need to reduce height * width to area of pool window
+                // not overlapping padding region
+                res[(ii / CONFIG_T::stride_height) * CONFIG_T::out_width * CONFIG_T::n_filt +
+                    (jj / CONFIG_T::stride_width) * CONFIG_T::n_filt + ff] =
+                    pool_op<data_T, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T::pool_op>(pool);
+                // If the pool op is Average, the zero-padding needs to be removed from the results
+                if (CONFIG_T::pool_op == Average) {
+                    data_T rescale =
+                        static_cast<data_T>(CONFIG_T::pool_height) * static_cast<data_T>(CONFIG_T::pool_width) / img_overlap;
+                    res[(ii / CONFIG_T::stride_height) * CONFIG_T::out_width * CONFIG_T::n_filt +
+                        (jj / CONFIG_T::stride_width) * CONFIG_T::n_filt + ff] *= rescale;
+                }
             }
-				  }
-			  }
-			  // do the pooling
-        // TODO in the case of average pooling, need to reduce height * width to area of pool window
-        // not overlapping padding region
-			  res[(ii/CONFIG_T::stride_height) * CONFIG_T::out_width * CONFIG_T::n_filt + (jj/CONFIG_T::stride_width)* CONFIG_T::n_filt + ff] =
-					  pool_op<data_T, CONFIG_T::pool_height*CONFIG_T::pool_width, CONFIG_T::pool_op>(pool);
-        // If the pool op is Average, the zero-padding needs to be removed from the results
-        if(CONFIG_T::pool_op == Average){
-          data_T rescale = CONFIG_T::pool_height * CONFIG_T::pool_width / img_overlap;
-          res[(ii/CONFIG_T::stride_height) * CONFIG_T::out_width * CONFIG_T::n_filt + (jj/CONFIG_T::stride_width)* CONFIG_T::n_filt + ff] *= rescale;
         }
-		  }
-	  }
-  }
+    }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void pooling2d_cf(data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_filt],
-               res_T res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt]){
+                  res_T res[CONFIG_T::out_height * CONFIG_T::out_width * CONFIG_T::n_filt]) {
+    #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
 
-  // TODO partition the arrays according to the reuse factor
-  const int limit = pool_op_limit<CONFIG_T>();
-  #pragma HLS ALLOCATION instances=pool_op limit=limit function
-  // Add any necessary padding
-  unsigned padded_height = CONFIG_T::in_height + CONFIG_T::pad_top + CONFIG_T::pad_bottom;
-  unsigned padded_width = CONFIG_T::in_width + CONFIG_T::pad_left + CONFIG_T::pad_right;
-  if (CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0) {
-    padded_height -= padded_height - (padded_height / CONFIG_T::stride_height * CONFIG_T::stride_height);
-    padded_width -= padded_width - (padded_width / CONFIG_T::stride_width * CONFIG_T::stride_width);
-  }
-
-  for(int ff = 0; ff < CONFIG_T::n_filt; ff++){
-	  // Loop over input image y in steps of stride
-	  for(int ii = 0; ii < padded_height; ii += CONFIG_T::stride_height){
-		  // Loop over input image x in steps of stride
-		  for(int jj = 0; jj < padded_width; jj += CONFIG_T::stride_width){
-			  data_T pool[CONFIG_T::pool_height * CONFIG_T::pool_width];
-        // Keep track of number of pixels in image vs padding region
-        unsigned img_overlap = 0;
-			  // Loop over pool window y
-			  for(int kk = 0; kk < CONFIG_T::stride_height; kk++){
-				  // Loop over pool window x
-				  for(int ll = 0; ll < CONFIG_T::stride_width; ll++){
-            if(ii+kk < CONFIG_T::pad_top || ii+kk >= (padded_height - CONFIG_T::pad_bottom) || jj+ll < CONFIG_T::pad_left || jj+ll >= (padded_width - CONFIG_T::pad_right)){
-              // Add padding
-              pool[kk * CONFIG_T::stride_width + ll] = pad_val<data_T, CONFIG_T::pool_op>();
-            }else{
-  					  pool[kk * CONFIG_T::stride_width + ll] = data[(ii + kk) * CONFIG_T::in_width + ff * CONFIG_T::in_width*CONFIG_T::in_height + ll + jj];
-              img_overlap++;
+    // TODO partition the arrays according to the reuse factor
+    const int limit = pool_op_limit<CONFIG_T>();
+    #pragma HLS ALLOCATION function instances=CONFIG_T::pool_op limit=limit
+    // Add any necessary padding
+    unsigned padded_height = CONFIG_T::in_height + CONFIG_T::pad_top + CONFIG_T::pad_bottom;
+    unsigned padded_width = CONFIG_T::in_width + CONFIG_T::pad_left + CONFIG_T::pad_right;
+    if (CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0) {
+        padded_height -= padded_height - (padded_height / CONFIG_T::stride_height * CONFIG_T::stride_height);
+        padded_width -= padded_width - (padded_width / CONFIG_T::stride_width * CONFIG_T::stride_width);
+    }
+
+    for (int ff = 0; ff < CONFIG_T::n_filt; ff++) {
+        // Loop over input image y in steps of stride
+        for (int ii = 0; ii < padded_height; ii += CONFIG_T::stride_height) {
+            // Loop over input image x in steps of stride
+            for (int jj = 0; jj < padded_width; jj += CONFIG_T::stride_width) {
+                data_T pool[CONFIG_T::pool_height * CONFIG_T::pool_width];
+                #pragma HLS ARRAY_PARTITION variable=pool complete dim=0
+                // Keep track of number of pixels in image vs padding region
+                unsigned img_overlap = 0;
+                // Loop over pool window y
+                for (int kk = 0; kk < CONFIG_T::stride_height; kk++) {
+                    // Loop over pool window x
+                    for (int ll = 0; ll < CONFIG_T::stride_width; ll++) {
+                        if (ii + kk < CONFIG_T::pad_top || ii + kk >= (padded_height - CONFIG_T::pad_bottom) ||
+                            jj + ll < CONFIG_T::pad_left || jj + ll >= (padded_width - CONFIG_T::pad_right)) {
+                            // Add padding
+                            pool[kk * CONFIG_T::stride_width + ll] = pad_val<data_T, CONFIG_T::pool_op>();
+                        } else {
+                            pool[kk * CONFIG_T::stride_width + ll] =
+                                data[(ii + kk - CONFIG_T::pad_top) * CONFIG_T::in_width +
+                                     ff * CONFIG_T::in_width * CONFIG_T::in_height + ll + jj - CONFIG_T::pad_left];
+                            img_overlap++;
+                        }
+                    }
+                }
+                // do the pooling
+                // TODO in the case of average pooling, need to reduce height * width to area of pool window
+                // not overlapping padding region
+                res[(ii / CONFIG_T::stride_height) * CONFIG_T::out_width + (jj / CONFIG_T::stride_width) +
+                    ff * CONFIG_T::out_height * CONFIG_T::out_width] =
+                    pool_op<data_T, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T::pool_op>(pool);
+                // If the pool op is Average, the zero-padding needs to be removed from the results
+                if (CONFIG_T::pool_op == Average) {
+                    data_T rescale =
+                        static_cast<data_T>(CONFIG_T::pool_height) * static_cast<data_T>(CONFIG_T::pool_width) / img_overlap;
+                    res[(ii / CONFIG_T::stride_height) * CONFIG_T::out_width + (jj / CONFIG_T::stride_width) +
+                        ff * CONFIG_T::out_height * CONFIG_T::out_width] *= rescale;
+                }
             }
-				  }
-			  }
-			  // do the pooling
-        // TODO in the case of average pooling, need to reduce height * width to area of pool window
-        // not overlapping padding region
-			  res[(ii/CONFIG_T::stride_height) * CONFIG_T::out_width + (jj/CONFIG_T::stride_width) + ff* CONFIG_T::out_height* CONFIG_T::out_width] =
-					  pool_op<data_T, CONFIG_T::pool_height*CONFIG_T::pool_width, CONFIG_T::pool_op>(pool);
-        // If the pool op is Average, the zero-padding needs to be removed from the results
-        if(CONFIG_T::pool_op == Average){
-          data_T rescale = CONFIG_T::pool_height * CONFIG_T::pool_width / img_overlap;
-          res[(ii/CONFIG_T::stride_height) * CONFIG_T::out_width + (jj/CONFIG_T::stride_width) + ff* CONFIG_T::out_height* CONFIG_T::out_width] *= rescale;
         }
-		  }
-	  }
-  }
+    }
 }
 
+template <class data_T, class res_T, typename CONFIG_T>
+void global_pooling2d_cl(data_T data[CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_filt],
+                         res_T res[CONFIG_T::n_filt]) {
+    assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
+    assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0);
+    assert(CONFIG_T::pool_width == CONFIG_T::stride_width);
+    assert(CONFIG_T::pool_height == CONFIG_T::stride_height);
+
+    #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
+
+    const int limit = pool_op_limit<CONFIG_T>();
+    #pragma HLS ALLOCATION instances=pool_op limit=limit function
+
+FiltLoop:
+    for (int filt = 0; filt < CONFIG_T::n_filt; filt++) {
+        data_T pool[CONFIG_T::in_height * CONFIG_T::in_width];
+
+    InputLoop:
+        for (int i = 0; i < CONFIG_T::in_height * CONFIG_T::in_width; i++) {
+            pool[i] = data[i * CONFIG_T::n_filt + filt];
+        }
+
+        res[filt] = static_cast<res_T>(pool_op<data_T, CONFIG_T::in_height * CONFIG_T::in_width, CONFIG_T::pool_op>(pool));
+    }
 }
 
+} // namespace nnet
+
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_pooling_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_pooling_stream.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,63 +1,59 @@
 #ifndef NNET_POOLING_STREAM_H_
 #define NNET_POOLING_STREAM_H_
 
-#include "utils/x_hls_utils.h"
 #include "ap_shift_reg.h"
+#include "hls_stream.h"
 #include "nnet_common.h"
-#include "nnet_pooling.h"
 #include "nnet_conv_stream.h"
-#include "hls_stream.h"
+#include "nnet_pooling.h"
+#include "utils/x_hls_utils.h"
 
 namespace nnet {
 
 // *************************************************
 //       Max/average pooling
 // *************************************************
 
-template <class T, int N, class CONFIG_T>
-T reduce_pool(T x[N]) {
+template <class T, int N, class CONFIG_T> T reduce_pool(T x[N]) {
     #pragma HLS INLINE
     if (CONFIG_T::pool_op == Max) {
         Op_max<T> op_max;
         return reduce<T, N, Op_max<T>>(x, op_max);
     } else {
         Op_add<T> op_add;
         T sum = reduce<T, N, Op_add<T>>(x, op_add);
         return sum / N;
     }
 }
 
-template<unsigned TABLE_SIZE, unsigned POOL_SIZE>
-void init_pool_table(
-    unsigned table[TABLE_SIZE]
-) {
+template <unsigned TABLE_SIZE, unsigned POOL_SIZE> void init_pool_table(unsigned table[TABLE_SIZE]) {
     for (unsigned ii = 0; ii < TABLE_SIZE; ii++) {
         table[ii] = ii % POOL_SIZE;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void compute_pool_encoded_2d(
-    const unsigned h_idx,
-    const unsigned w_idx,
-    const data_T& in_elem,
+    const unsigned h_idx, const unsigned w_idx, const data_T &in_elem,
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::pool_height * CONFIG_T::pool_width * CONFIG_T::n_filt],
-    hls::stream<res_T> &res,
-    res_T &res_pack,
-    unsigned &outputs_ready
-) {
+    hls::stream<res_T> &res, res_T &res_pack, unsigned &outputs_ready) {
     // Nearest H without unused pixels on the right
-    constexpr unsigned nH = ((CONFIG_T::in_height - CONFIG_T::pool_height) / CONFIG_T::stride_height) * CONFIG_T::stride_height + CONFIG_T::pool_height;
+    constexpr unsigned nH =
+        ((CONFIG_T::in_height - CONFIG_T::pool_height) / CONFIG_T::stride_height) * CONFIG_T::stride_height +
+        CONFIG_T::pool_height;
     // Scaled H that behaves like original H
-    constexpr unsigned sH = (DIV_ROUNDUP(CONFIG_T::pool_height, CONFIG_T::stride_height) - 1) * CONFIG_T::stride_height + CONFIG_T::pool_height;
+    constexpr unsigned sH =
+        (DIV_ROUNDUP(CONFIG_T::pool_height, CONFIG_T::stride_height) - 1) * CONFIG_T::stride_height + CONFIG_T::pool_height;
     // Nearest W without unused pixels on the right
-    constexpr unsigned nW = ((CONFIG_T::in_width - CONFIG_T::pool_width) / CONFIG_T::stride_width) * CONFIG_T::stride_width + CONFIG_T::pool_width;
+    constexpr unsigned nW = ((CONFIG_T::in_width - CONFIG_T::pool_width) / CONFIG_T::stride_width) * CONFIG_T::stride_width +
+                            CONFIG_T::pool_width;
     // Scaled W that behaves like original W
-    constexpr unsigned sW = (DIV_ROUNDUP(CONFIG_T::pool_width, CONFIG_T::stride_width) - 1) * CONFIG_T::stride_width + CONFIG_T::pool_width;
+    constexpr unsigned sW =
+        (DIV_ROUNDUP(CONFIG_T::pool_width, CONFIG_T::stride_width) - 1) * CONFIG_T::stride_width + CONFIG_T::pool_width;
 
 #ifdef __SYNTHESIS__
     bool initialized = false;
     unsigned pool_table_height[CONFIG_T::in_height];
     unsigned pool_table_width[CONFIG_T::in_width];
 #else
     static bool initialized = false;
@@ -79,204 +75,211 @@
 
     typename CONFIG_T::accum_t pool_window[CONFIG_T::pool_height * CONFIG_T::pool_width];
     #pragma HLS ARRAY_PARTITION variable=pool_window complete
 
     const unsigned sh_idx = pool_table_height[h_idx] * CONFIG_T::pool_width;
     const unsigned wp_idx = w_idx * (data_T::size / CONFIG_T::n_filt);
 
-    PixelLoop: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_filt; p++) {
+PixelLoop:
+    for (unsigned p = 0; p < data_T::size / CONFIG_T::n_filt; p++) {
         #pragma HLS PIPELINE
 
-        ap_uint<CONFIG_T::pool_height * CONFIG_T::pool_width> filt_mask = 0;
+        ap_uint<CONFIG_T::pool_height *CONFIG_T::pool_width> filt_mask = 0;
         if ((h_idx < nH) && (wp_idx + p < nW)) {
             filt_mask = sh_idx + pool_table_width[wp_idx + p] + 1;
         }
 
-        CopyDataFilt: for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
-            if (filt_mask > 0) data_window[c * CONFIG_T::pool_height * CONFIG_T::pool_width + filt_mask.to_uint() - 1].write(in_elem[p * CONFIG_T::n_filt + c]);
+    CopyDataFilt:
+        for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
+            if (filt_mask > 0)
+                data_window[c * CONFIG_T::pool_height * CONFIG_T::pool_width + filt_mask.to_uint() - 1].write(
+                    in_elem[p * CONFIG_T::n_filt + c]);
         }
 
         if (filt_mask == CONFIG_T::pool_height * CONFIG_T::pool_width) {
-            FiltLoop: for(unsigned c = 0; c < CONFIG_T::n_filt; c++) {
-                PoolLoop: for(unsigned f = 0; f < CONFIG_T::pool_height * CONFIG_T::pool_width; f++) {
+        FiltLoop:
+            for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
+            PoolLoop:
+                for (unsigned f = 0; f < CONFIG_T::pool_height * CONFIG_T::pool_width; f++) {
                     pool_window[f] = data_window[c * CONFIG_T::pool_height * CONFIG_T::pool_width + f].read();
                 }
-                if (res_T::size / CONFIG_T::n_filt == 1) { // Saves resources if we don't pack output, compiler will remove the else branch
-                    res_pack[c] = reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T>(pool_window);
+                if (res_T::size / CONFIG_T::n_filt ==
+                    1) { // Saves resources if we don't pack output, compiler will remove the else branch
+                    res_pack[c] =
+                        reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T>(
+                            pool_window);
                 } else {
-                    res_pack[outputs_ready * CONFIG_T::n_filt + c] = reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T>(pool_window);
+                    res_pack[outputs_ready * CONFIG_T::n_filt + c] =
+                        reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T>(
+                            pool_window);
                 }
-
             }
-            if (res_T::size / CONFIG_T::n_filt == 1) { // Saves resources if we don't pack output, compiler will remove the else branch
+            if (res_T::size / CONFIG_T::n_filt ==
+                1) { // Saves resources if we don't pack output, compiler will remove the else branch
                 res.write(res_pack);
             } else {
                 if (outputs_ready == (res_T::size / CONFIG_T::n_filt) - 1) {
                     res.write(res_pack);
                     outputs_ready = 0;
                 } else {
                     outputs_ready++;
                 }
             }
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void pooling2d_encoded_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void pooling2d_encoded_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::pool_height == CONFIG_T::stride_height && CONFIG_T::pool_width == CONFIG_T::stride_width);
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
     unsigned outputs_ready = 0;
 
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::pool_height * CONFIG_T::pool_width * CONFIG_T::n_filt];
     constexpr int win_depth = CONFIG_T::pool_height * CONFIG_T::out_width;
     for (unsigned i_out = 0; i_out < CONFIG_T::pool_height * CONFIG_T::pool_width * CONFIG_T::n_filt; i_out++) {
         #pragma HLS STREAM variable=data_window[i_out] depth=win_depth
     }
 
     constexpr int pack_factor = data_T::size / CONFIG_T::n_filt;
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (pack_factor); i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (pack_factor); i_iw++) {
             #pragma HLS LOOP_FLATTEN
             if (res_T::size / CONFIG_T::n_filt == 1) {
                 #pragma HLS PIPELINE II=pack_factor
             }
-            compute_pool_encoded_2d<data_T, res_T, CONFIG_T>(i_ih, i_iw, data.read(), data_window, res, res_pack, outputs_ready);
+            compute_pool_encoded_2d<data_T, res_T, CONFIG_T>(i_ih, i_iw, data.read(), data_window, res, res_pack,
+                                                             outputs_ready);
         }
     }
 }
 
 // *************************************************
 //       Line Buffer Implementation (Phil's)
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_pool_buffer_2d(
-    const data_T& in_elem,
-    ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::pool_height - 1,1)][CONFIG_T::n_filt],
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_pool_buffer_2d(const data_T &in_elem,
+                            ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width>
+                                line_buffer[MAX(CONFIG_T::pool_height - 1, 1)][CONFIG_T::n_filt],
+                            hls::stream<res_T> &res) {
     #pragma HLS INLINE
     const static int lShiftX = CONFIG_T::pool_width - 1;
     const static int lShiftY = CONFIG_T::pool_height - 1;
-    static int pX = 0; // pixel X 
+    static int pX = 0; // pixel X
     static int pY = 0; // pixel Y
     static int sX = 0; // stride X
     static int sY = 0; // stride Y
 
-    typename data_T::value_type pool_window[CONFIG_T::pool_height * CONFIG_T::pool_width];
+    typename CONFIG_T::accum_t pool_window[CONFIG_T::pool_height * CONFIG_T::pool_width];
     #pragma HLS ARRAY_PARTITION variable=pool_window complete
 
     static typename data_T::value_type kernel_data[CONFIG_T::pool_height * CONFIG_T::pool_width * CONFIG_T::n_filt];
     #pragma HLS ARRAY_PARTITION variable = kernel_data complete dim = 0
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
 
     // Add pixel into line buffer, return pooling kernels
     nnet::shift_line_buffer<data_T, CONFIG_T>(in_elem, line_buffer, kernel_data);
 
     // Can compute pooling output
     if ((sX - lShiftX) == 0 && (sY - lShiftY) == 0 && pY > lShiftY - 1 && pX > lShiftX - 1) {
-        FiltLoop: for(unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
-            #pragma HLS PIPELINE
+    FiltLoop:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
+        #pragma HLS PIPELINE
 
-            // Retrieve data for current channel
-            PoolLoop: for(unsigned i_ihw = 0; i_ihw < CONFIG_T::pool_height * CONFIG_T::pool_width; i_ihw++) {
-                pool_window[i_ihw] = kernel_data[i_ihw * CONFIG_T::n_filt + i_ic]; 
+        // Retrieve data for current channel
+        PoolLoop:
+            for (unsigned i_ihw = 0; i_ihw < CONFIG_T::pool_height * CONFIG_T::pool_width; i_ihw++) {
+                pool_window[i_ihw] = kernel_data[i_ihw * CONFIG_T::n_filt + i_ic];
             }
 
             // Compute Pooling
-            res_pack[i_ic] = reduce_pool<typename data_T::value_type, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T>(pool_window);
+            res_pack[i_ic] =
+                reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_height * CONFIG_T::pool_width, CONFIG_T>(pool_window);
         }
 
         // Write to output
         res.write(res_pack);
     }
 
     // Counter Housekeeping
-    if (pX + 1 == CONFIG_T::in_width)  // Includes padding, end of line (padded)
+    if (pX + 1 == CONFIG_T::in_width) // Includes padding, end of line (padded)
     {
         pX = 0;
         sX = 0;
-        if (pY + 1 == CONFIG_T::in_height) {  // Reached bottom of image
+        if (pY + 1 == CONFIG_T::in_height) { // Reached bottom of image
             pY = 0;
             sY = 0;
         } else { // Next line
             pY = pY + 1;
             // Update stride (threshold) ? subtract stride : increment stride
-            sY = ((sY - lShiftY) == 0) ? sY - CONFIG_T::stride_height + 1 : sY + 1; 
+            sY = ((sY - lShiftY) == 0) ? sY - CONFIG_T::stride_height + 1 : sY + 1;
         }
     } else {
         pX = pX + 1;
         // Update stride (threshold) ? subtract stride : increment stride
-        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1; 
+        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void pooling2d_buffer_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void pooling2d_buffer_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::pool_height == CONFIG_T::stride_height && CONFIG_T::pool_width == CONFIG_T::stride_width);
 
-    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::pool_height - 1,1)][CONFIG_T::n_filt];
+    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::pool_height - 1, 1)]
+                                                                                    [CONFIG_T::n_filt];
     #pragma HLS ARRAY_PARTITION variable = line_buffer complete dim = 2
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
             #pragma HLS LOOP_FLATTEN
             #pragma HLS PIPELINE
 
             compute_pool_buffer_2d<data_T, res_T, CONFIG_T>(data.read(), line_buffer, res);
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void pooling2d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
-    #pragma HLS inline region
-    switch(CONFIG_T::implementation){
-        case conv_implementation::linebuffer:
-            pooling2d_buffer_cl<data_T, res_T, CONFIG_T>(data, res);
-            break;
-        case conv_implementation::encoded:
-            pooling2d_encoded_cl<data_T, res_T, CONFIG_T>(data, res);
-            break;
-        } 
+template <class data_T, class res_T, typename CONFIG_T>
+void pooling2d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+    #pragma HLS inline recursive
+    switch (CONFIG_T::implementation) {
+    case conv_implementation::linebuffer:
+        pooling2d_buffer_cl<data_T, res_T, CONFIG_T>(data, res);
+        break;
+    case conv_implementation::encoded:
+        pooling2d_encoded_cl<data_T, res_T, CONFIG_T>(data, res);
+        break;
+    }
 }
 
 // *************************************************
 //                  Pooling 1D
 // *************************************************
 
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_pool_encoded_1d(
-    const unsigned w_idx,
-    const data_T& in_elem,
-    hls::stream<typename data_T::value_type> data_window[CONFIG_T::pool_width * CONFIG_T::n_filt],
-    hls::stream<res_T> &res,
-    res_T &res_pack,
-    unsigned &outputs_ready
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_pool_encoded_1d(const unsigned w_idx, const data_T &in_elem,
+                             hls::stream<typename data_T::value_type> data_window[CONFIG_T::pool_width * CONFIG_T::n_filt],
+                             hls::stream<res_T> &res, res_T &res_pack, unsigned &outputs_ready) {
     // Nearest W without unused pixels on the right
-    constexpr unsigned nW = ((CONFIG_T::n_in - CONFIG_T::pool_width) / CONFIG_T::stride_width) * CONFIG_T::stride_width + CONFIG_T::pool_width;
+    constexpr unsigned nW =
+        ((CONFIG_T::n_in - CONFIG_T::pool_width) / CONFIG_T::stride_width) * CONFIG_T::stride_width + CONFIG_T::pool_width;
     // Scaled W that behaves like original W
-    constexpr unsigned sW = (DIV_ROUNDUP(CONFIG_T::pool_width, CONFIG_T::stride_width) - 1) * CONFIG_T::stride_width + CONFIG_T::pool_width;
+    constexpr unsigned sW =
+        (DIV_ROUNDUP(CONFIG_T::pool_width, CONFIG_T::stride_width) - 1) * CONFIG_T::stride_width + CONFIG_T::pool_width;
 
 #ifdef __SYNTHESIS__
     bool initialized = false;
     unsigned pool_table_width[CONFIG_T::n_in];
 #else
     static bool initialized = false;
     static unsigned pool_table_width[CONFIG_T::n_in];
@@ -293,313 +296,314 @@
     }
 
     typename CONFIG_T::accum_t pool_window[CONFIG_T::pool_width];
     #pragma HLS ARRAY_PARTITION variable=pool_window complete
 
     const unsigned wp_idx = w_idx * (data_T::size / CONFIG_T::n_filt);
 
-    PixelLoop: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_filt; p++) {
+PixelLoop:
+    for (unsigned p = 0; p < data_T::size / CONFIG_T::n_filt; p++) {
         #pragma HLS PIPELINE
 
         ap_uint<CONFIG_T::pool_width> filt_mask = 0;
         if (wp_idx + p < nW) {
             filt_mask = pool_table_width[wp_idx + p] + 1;
         }
 
-        CopyDataFilt: for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
-            if (filt_mask > 0) data_window[c * CONFIG_T::pool_width + filt_mask.to_uint() - 1].write(in_elem[p * CONFIG_T::n_filt + c]);
+    CopyDataFilt:
+        for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
+            if (filt_mask > 0)
+                data_window[c * CONFIG_T::pool_width + filt_mask.to_uint() - 1].write(in_elem[p * CONFIG_T::n_filt + c]);
         }
 
         if (filt_mask == CONFIG_T::pool_width) {
-            FiltLoop: for(unsigned c = 0; c < CONFIG_T::n_filt; c++) {
-                PoolLoop: for(unsigned f = 0; f < CONFIG_T::pool_width; f++) {
+        FiltLoop:
+            for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
+            PoolLoop:
+                for (unsigned f = 0; f < CONFIG_T::pool_width; f++) {
                     pool_window[f] = data_window[c * CONFIG_T::pool_width + f].read();
                 }
-                if (res_T::size / CONFIG_T::n_filt == 1) { // Saves resources if we don't pack output, compiler will remove the else branch
+                if (res_T::size / CONFIG_T::n_filt ==
+                    1) { // Saves resources if we don't pack output, compiler will remove the else branch
                     res_pack[c] = reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_width, CONFIG_T>(pool_window);
                 } else {
-                    res_pack[outputs_ready * CONFIG_T::n_filt + c] = reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_width, CONFIG_T>(pool_window);
+                    res_pack[outputs_ready * CONFIG_T::n_filt + c] =
+                        reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_width, CONFIG_T>(pool_window);
                 }
-
             }
-            if (res_T::size / CONFIG_T::n_filt == 1) { // Saves resources if we don't pack output, compiler will remove the else branch
+            if (res_T::size / CONFIG_T::n_filt ==
+                1) { // Saves resources if we don't pack output, compiler will remove the else branch
                 res.write(res_pack);
             } else {
                 if (outputs_ready == (res_T::size / CONFIG_T::n_filt) - 1) {
                     res.write(res_pack);
                     outputs_ready = 0;
                 } else {
                     outputs_ready++;
                 }
             }
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void pooling1d_encoded_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void pooling1d_encoded_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::pool_width == CONFIG_T::stride_width);
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
     unsigned outputs_ready = 0;
 
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::pool_width * CONFIG_T::n_filt];
     constexpr int win_depth = CONFIG_T::n_out;
     for (unsigned i_out = 0; i_out < CONFIG_T::pool_width * CONFIG_T::n_filt; i_out++) {
         #pragma HLS STREAM variable=data_window[i_out] depth=win_depth
     }
 
     constexpr int pack_factor = data_T::size / CONFIG_T::n_filt;
 
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::n_in / (pack_factor); i_iw++) {
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::n_in / (pack_factor); i_iw++) {
         #pragma HLS LOOP_FLATTEN
         if (res_T::size / CONFIG_T::n_filt == 1) {
             #pragma HLS PIPELINE II=pack_factor
         }
         compute_pool_encoded_1d<data_T, res_T, CONFIG_T>(i_iw, data.read(), data_window, res, res_pack, outputs_ready);
     }
 }
 
 // *************************************************
 //       Line Buffer Implementation (Phil's) 1D
 // *************************************************
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_pool_buffer_1d(
-    const data_T& in_elem,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_pool_buffer_1d(const data_T &in_elem, hls::stream<res_T> &res) {
     #pragma HLS INLINE
     const static int lShiftX = CONFIG_T::pool_width - 1;
     // Counters
     static int pX = 0;
     static int sX = 0;
 
-    typename data_T::value_type pool_window[CONFIG_T::pool_width];
+    typename CONFIG_T::accum_t pool_window[CONFIG_T::pool_width];
     #pragma HLS ARRAY_PARTITION variable=pool_window complete
 
     static typename data_T::value_type kernel_data[CONFIG_T::pool_width * CONFIG_T::n_filt];
     #pragma HLS ARRAY_PARTITION variable = kernel_data complete dim = 0
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
 
     // Add pixel into line buffer, return pooling kernels
     // 1D case line buffer not necessary. Put directly into the kernel_data buffer
     nnet::kernel_shift_1d<data_T, CONFIG_T>(in_elem, kernel_data);
 
     // Can compute pooling output
-    if ( (sX - lShiftX) == 0 && pX > lShiftX - 1) {
-        FiltLoop: for(unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
-            #pragma HLS PIPELINE
+    if ((sX - lShiftX) == 0 && pX > lShiftX - 1) {
+    FiltLoop:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
+        #pragma HLS PIPELINE
 
-            // Retrieve data for current channel
-            PoolLoop: for(unsigned i_iw = 0; i_iw < CONFIG_T::pool_width; i_iw++) {
-                pool_window[i_iw] = kernel_data[i_iw * CONFIG_T::n_filt + i_ic]; 
+        // Retrieve data for current channel
+        PoolLoop:
+            for (unsigned i_iw = 0; i_iw < CONFIG_T::pool_width; i_iw++) {
+                pool_window[i_iw] = kernel_data[i_iw * CONFIG_T::n_filt + i_ic];
             }
 
             // Compute Pooling
-            res_pack[i_ic] = reduce_pool<typename data_T::value_type, CONFIG_T::pool_width, CONFIG_T>(pool_window);
+            res_pack[i_ic] = reduce_pool<typename CONFIG_T::accum_t, CONFIG_T::pool_width, CONFIG_T>(pool_window);
         }
 
         // Write to output
         res.write(res_pack);
     }
 
     // Counter Housekeeping
-    if (pX + 1 == CONFIG_T::n_in)  // Includes padding, end of line (padded)
+    if (pX + 1 == CONFIG_T::n_in) // Includes padding, end of line (padded)
     {
         pX = 0;
         sX = 0;
     } else {
         pX = pX + 1;
         // Update stride (threshold) ? subtract stride : increment stride
-        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1; 
+        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void pooling1d_buffer_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void pooling1d_buffer_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
-    
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::n_in; i_iw++) {
+
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::n_in; i_iw++) {
         #pragma HLS LOOP_FLATTEN
         #pragma HLS PIPELINE
         compute_pool_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res);
     }
 }
 
-
-template<class data_T, class res_T, typename CONFIG_T>
-void pooling1d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
-    #pragma HLS inline region
-    switch(CONFIG_T::implementation){
-        case conv_implementation::linebuffer:
-            pooling1d_buffer_cl<data_T, res_T, CONFIG_T>(data, res);
-            break;
-        case conv_implementation::encoded:
-            pooling1d_encoded_cl<data_T, res_T, CONFIG_T>(data, res);
-            break;
-    } 
+template <class data_T, class res_T, typename CONFIG_T>
+void pooling1d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+    #pragma HLS inline recursive
+    switch (CONFIG_T::implementation) {
+    case conv_implementation::linebuffer:
+        pooling1d_buffer_cl<data_T, res_T, CONFIG_T>(data, res);
+        break;
+    case conv_implementation::encoded:
+        pooling1d_encoded_cl<data_T, res_T, CONFIG_T>(data, res);
+        break;
+    }
 }
 
-
 // *************************************************
 //       Global max/average pooling
 // *************************************************
 
-template <class T, int N, class CONFIG_T>
-T reduce_global_pool(T x, T y[N]) {
+template <class T, int N, class CONFIG_T> T reduce_global_pool(T x, T y[N]) {
     #pragma HLS INLINE
     if (CONFIG_T::pool_op == Max) {
         Op_max<T> op_max;
         T y_max = reduce<T, N, Op_max<T>>(y, op_max);
         return (x > y_max) ? x : y_max;
     } else {
         Op_add<T> op_add;
         T y_sum = reduce<T, N, Op_add<T>>(y, op_add);
         return x + y_sum;
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_global_pool(
-    const data_T& in_elem,
-    typename CONFIG_T::accum_t data_window[CONFIG_T::n_filt]
-) {
-    PoolFilt: for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_global_pool(const data_T &in_elem, typename CONFIG_T::accum_t data_window[CONFIG_T::n_filt]) {
+PoolFilt:
+    for (unsigned c = 0; c < CONFIG_T::n_filt; c++) {
         #pragma HLS UNROLL
 
         typename CONFIG_T::accum_t data_pack[data_T::size / CONFIG_T::n_filt];
         #pragma HLS ARRAY_PARTITION variable=data_pack complete dim=0
 
-        PixelLoop: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_filt; p++) {
+    PixelLoop:
+        for (unsigned p = 0; p < data_T::size / CONFIG_T::n_filt; p++) {
             #pragma HLS UNROLL
             data_pack[p] = in_elem[p * CONFIG_T::n_filt + c];
         }
-        data_window[c] = reduce_global_pool<typename CONFIG_T::accum_t, data_T::size / CONFIG_T::n_filt, CONFIG_T>(data_window[c], data_pack);
+        data_window[c] = reduce_global_pool<typename CONFIG_T::accum_t, data_T::size / CONFIG_T::n_filt, CONFIG_T>(
+            data_window[c], data_pack);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void global_pooling2d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void global_pooling2d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::pool_height == CONFIG_T::stride_height && CONFIG_T::pool_width == CONFIG_T::stride_width);
 
     typename CONFIG_T::accum_t data_window[CONFIG_T::n_filt];
     #pragma HLS ARRAY_PARTITION variable=data_window complete
 
     typename CONFIG_T::accum_t init = 0;
     if (CONFIG_T::pool_op == Max) {
         init = hls::numeric_limits<typename CONFIG_T::accum_t>::min();
     }
 
-    PoolInitLoop: for (unsigned i_init = 0; i_init < CONFIG_T::n_filt; i_init++) {
+PoolInitLoop:
+    for (unsigned i_init = 0; i_init < CONFIG_T::n_filt; i_init++) {
         #pragma HLS UNROLL
         data_window[i_init] = init;
     }
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_filt); i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_filt); i_iw++) {
             #pragma HLS LOOP_FLATTEN
             compute_global_pool<data_T, res_T, CONFIG_T>(data.read(), data_window);
         }
     }
 
     if (CONFIG_T::pool_op == Max) {
-        MaxPoolRes: for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
+    MaxPoolRes:
+        for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
             #pragma HLS PIPELINE
 
             res_T res_pack;
-            #pragma HLS DATA_PACK variable=res_pack
-            MaxPoolPack: for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
+            PRAGMA_DATA_PACK(res_pack)
+        MaxPoolPack:
+            for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
                 #pragma HLS UNROLL
                 res_pack[i_pack] = data_window[i_pack];
             }
             res.write(res_pack);
         }
     } else {
-        AvgPoolRes: for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
+    AvgPoolRes:
+        for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
             #pragma HLS PIPELINE
 
             res_T res_pack;
-            #pragma HLS DATA_PACK variable=res_pack
-            AvgPoolPack: for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
+            PRAGMA_DATA_PACK(res_pack)
+        AvgPoolPack:
+            for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
                 #pragma HLS UNROLL
                 res_pack[i_pack] = data_window[i_pack] / (CONFIG_T::in_height * CONFIG_T::in_width);
             }
             res.write(res_pack);
         }
     }
-
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void global_pooling1d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T> &res
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void global_pooling1d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::pool_width == CONFIG_T::stride_width);
 
     typename CONFIG_T::accum_t data_window[CONFIG_T::n_filt];
     #pragma HLS ARRAY_PARTITION variable=data_window complete
 
     typename CONFIG_T::accum_t init = 0;
     if (CONFIG_T::pool_op == Max) {
         init = hls::numeric_limits<typename CONFIG_T::accum_t>::min();
     }
 
-    PoolInitLoop: for (unsigned i_init = 0; i_init < CONFIG_T::n_filt; i_init++) {
+PoolInitLoop:
+    for (unsigned i_init = 0; i_init < CONFIG_T::n_filt; i_init++) {
         #pragma HLS UNROLL
         data_window[i_init] = init;
     }
 
-    ReadInput: for (unsigned i_iw = 0; i_iw < CONFIG_T::n_in / (data_T::size / CONFIG_T::n_filt); i_iw++) {
+ReadInput:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::n_in / (data_T::size / CONFIG_T::n_filt); i_iw++) {
         #pragma HLS LOOP_FLATTEN
         compute_global_pool<data_T, res_T, CONFIG_T>(data.read(), data_window);
     }
 
     if (CONFIG_T::pool_op == Max) {
-        MaxPoolRes: for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
+    MaxPoolRes:
+        for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
             #pragma HLS PIPELINE
 
             res_T res_pack;
-            #pragma HLS DATA_PACK variable=res_pack
-            MaxPoolPack: for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
+            PRAGMA_DATA_PACK(res_pack)
+        MaxPoolPack:
+            for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
                 #pragma HLS UNROLL
                 res_pack[i_pack] = data_window[i_pack];
             }
             res.write(res_pack);
         }
     } else {
-        AvgPoolRes: for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
+    AvgPoolRes:
+        for (unsigned i_res = 0; i_res < CONFIG_T::n_filt / res_T::size; i_res++) {
             #pragma HLS PIPELINE
 
             res_T res_pack;
-            #pragma HLS DATA_PACK variable=res_pack
-            AvgPoolPack: for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
+            PRAGMA_DATA_PACK(res_pack)
+        AvgPoolPack:
+            for (unsigned i_pack = 0; i_pack < res_T::size; i_pack++) {
                 #pragma HLS UNROLL
-	      res_pack[i_pack] = data_window[i_pack] / CONFIG_T::n_in;
+                res_pack[i_pack] = data_window[i_pack] / CONFIG_T::n_in;
             }
             res.write(res_pack);
         }
     }
-
 }
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_sepconv1d_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_sepconv1d_stream.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,116 +1,112 @@
 #ifndef NNET_SEPARABLE_CONV1D_STREAM_H_
 #define NNET_SEPARABLE_CONV1D_STREAM_H_
 
-#include "nnet_common.h"
 #include "hls_stream.h"
-#include "nnet_sepconv_stream.h"
+#include "nnet_common.h"
 #include "nnet_conv1d_stream.h"
+#include "nnet_sepconv_stream.h"
 
 namespace nnet {
 
-template<class data_T, class res_T, typename CONFIG_T>
-void depthwise_conv_1d_encoded_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_chan])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void depthwise_conv_1d_encoded_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                                  typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan],
+                                  typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
 
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::filt_width * CONFIG_T::n_chan];
     const int win_depth = CONFIG_T::out_width;
     for (unsigned i_out = 0; i_out < CONFIG_T::filt_width * CONFIG_T::n_chan; i_out++) {
         #pragma HLS STREAM variable=data_window[i_out] depth=win_depth
     }
 
     #pragma HLS ARRAY_PARTITION variable=CONFIG_T::pixels complete
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
     unsigned outputs_ready = 0;
 
     ap_uint<CONFIG_T::filt_width> pixel_idx[data_T::size / CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=pixel_idx complete
 
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
         #pragma HLS LOOP_FLATTEN
         if (CONFIG_T::strategy == nnet::latency && data_T::size / CONFIG_T::n_chan == 1) {
             #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
         }
         compute_scaled_indices_1d<data_T, CONFIG_T>(i_iw, pixel_idx);
-        compute_depthwise_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready, weights, biases, pixel_idx);
+        compute_depthwise_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready,
+                                                                  weights, biases, pixel_idx);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void depthwise_conv_1d_buffer_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_chan])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void depthwise_conv_1d_buffer_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                                 typename CONFIG_T::weight_t weights[CONFIG_T::filt_width * CONFIG_T::n_chan],
+                                 typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
 
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
         #pragma HLS LOOP_FLATTEN
         if (CONFIG_T::strategy == nnet::latency) {
             #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
         }
         compute_depthwise_output_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void pointwise_conv_1d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void pointwise_conv_1d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                          typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
+                          typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     assert(CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::filt_width == 1);
 
     #pragma HLS ARRAY_PARTITION variable=weights complete
     #pragma HLS ARRAY_PARTITION variable=biases complete
 
-    ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
+ReadInputWidth:
+    for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
         if (CONFIG_T::strategy == nnet::latency && data_T::size / CONFIG_T::n_chan == 1) {
             #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
         }
         if (i_iw % CONFIG_T::stride_width == 0) {
             pointwise_mult_buffer<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
         } else {
             data.read();
         }
     }
 }
 
-
-template<class data_T, class res_T, typename CONFIG_T>
-void separable_conv_1d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::depthwise_config::weight_t depthwise_weights[CONFIG_T::depthwise_config::filt_width * CONFIG_T::depthwise_config::n_chan],
-    typename CONFIG_T::pointwise_config::weight_t pointwise_weights[CONFIG_T::pointwise_config::n_chan * CONFIG_T::pointwise_config::n_filt],
-    typename CONFIG_T::depthwise_config::bias_t   depthwise_biases[CONFIG_T::depthwise_config::n_chan],
-    typename CONFIG_T::pointwise_config::bias_t   pointwise_biases[CONFIG_T::pointwise_config::n_filt]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void separable_conv_1d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                          typename CONFIG_T::depthwise_config::weight_t
+                              depthwise_weights[CONFIG_T::depthwise_config::filt_width * CONFIG_T::depthwise_config::n_chan],
+                          typename CONFIG_T::pointwise_config::weight_t
+                              pointwise_weights[CONFIG_T::pointwise_config::n_chan * CONFIG_T::pointwise_config::n_filt],
+                          typename CONFIG_T::depthwise_config::bias_t depthwise_biases[CONFIG_T::depthwise_config::n_chan],
+                          typename CONFIG_T::pointwise_config::bias_t pointwise_biases[CONFIG_T::pointwise_config::n_filt]) {
     #pragma HLS DATAFLOW
 
     hls::stream<data_T> depthwise_res;
     unsigned res_depth = CONFIG_T::depthwise_config::out_width;
     #pragma HLS STREAM variable=depthwise_res depth=res_depth
 
-    switch(CONFIG_T::depthwise_config::implementation){
-        case conv_implementation::linebuffer:
-            depthwise_conv_1d_buffer_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(data, depthwise_res, depthwise_weights, depthwise_biases);
-            break;
-        case conv_implementation::encoded:
-            depthwise_conv_1d_encoded_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(data, depthwise_res, depthwise_weights, depthwise_biases);
-            break;
-    } 
-    pointwise_conv_1d_cl<data_T, res_T, typename CONFIG_T::pointwise_config>(depthwise_res, res, pointwise_weights, pointwise_biases);
+    switch (CONFIG_T::depthwise_config::implementation) {
+    case conv_implementation::linebuffer:
+        depthwise_conv_1d_buffer_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(
+            data, depthwise_res, depthwise_weights, depthwise_biases);
+        break;
+    case conv_implementation::encoded:
+        depthwise_conv_1d_encoded_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(
+            data, depthwise_res, depthwise_weights, depthwise_biases);
+        break;
+    }
+    pointwise_conv_1d_cl<data_T, res_T, typename CONFIG_T::pointwise_config>(depthwise_res, res, pointwise_weights,
+                                                                             pointwise_biases);
 }
 
-}
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_sepconv2d_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_sepconv2d_stream.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,132 +1,135 @@
 #ifndef NNET_SEPARABLE_CONV2D_STREAM_H_
 #define NNET_SEPARABLE_CONV2D_STREAM_H_
 
-#include "nnet_common.h"
 #include "hls_stream.h"
-#include "nnet_sepconv_stream.h"
+#include "nnet_common.h"
 #include "nnet_conv2d_stream.h"
+#include "nnet_sepconv_stream.h"
 
 namespace nnet {
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void depthwise_conv_2d_encoded_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_chan])
-{
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::filt_height == CONFIG_T::filt_width);
 
     hls::stream<typename data_T::value_type> data_window[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan];
     const int win_depth = CONFIG_T::filt_height * CONFIG_T::out_width;
     for (unsigned i_out = 0; i_out < CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan; i_out++) {
         #pragma HLS STREAM variable=data_window[i_out] depth=win_depth
     }
 
     #pragma HLS ARRAY_PARTITION variable=CONFIG_T::pixels complete
 
     res_T res_pack;
-    #pragma HLS DATA_PACK variable=res_pack
+    PRAGMA_DATA_PACK(res_pack)
     unsigned outputs_ready = 0;
 
     ap_uint<CONFIG_T::filt_height * CONFIG_T::filt_width> pixel_idx[data_T::size / CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=pixel_idx complete
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
             #pragma HLS LOOP_FLATTEN
             if (CONFIG_T::strategy == nnet::latency && data_T::size / CONFIG_T::n_chan == 1) {
                 #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
             }
             compute_scaled_indices_2d<data_T, CONFIG_T>(i_ih, i_iw, pixel_idx);
-            compute_depthwise_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready, weights, biases, pixel_idx);
+            compute_depthwise_output_encoded<data_T, res_T, CONFIG_T>(data.read(), data_window, res, res_pack, outputs_ready,
+                                                                      weights, biases, pixel_idx);
         }
     }
 }
 
 // Line Buffer Implementation (Phil's)
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void depthwise_conv_2d_buffer_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
+    hls::stream<data_T> &data, hls::stream<res_T> &res,
     typename CONFIG_T::weight_t weights[CONFIG_T::filt_height * CONFIG_T::filt_width * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_chan])
-{
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
 
-    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[CONFIG_T::filt_height - 1][CONFIG_T::n_chan];
+    static ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[CONFIG_T::filt_height - 1]
+                                                                                    [CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable = line_buffer complete dim = 2
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width; i_iw++) {
             #pragma HLS LOOP_FLATTEN
             if (CONFIG_T::strategy == nnet::latency) {
                 #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
             }
             if (CONFIG_T::filt_height > 1) {
                 compute_depthwise_output_buffer_2d<data_T, res_T, CONFIG_T>(data.read(), line_buffer, res, weights, biases);
             } else {
                 compute_depthwise_output_buffer_1d<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
             }
         }
     }
 }
 
-
-template<class data_T, class res_T, typename CONFIG_T>
-void pointwise_conv_2d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t   biases[CONFIG_T::n_filt])
-{
+template <class data_T, class res_T, typename CONFIG_T>
+void pointwise_conv_2d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                          typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
+                          typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     assert(CONFIG_T::pad_top == 0 && CONFIG_T::pad_bottom == 0 && CONFIG_T::pad_left == 0 && CONFIG_T::pad_right == 0);
     assert(CONFIG_T::filt_height == 1 && CONFIG_T::filt_width == 1);
 
     #pragma HLS ARRAY_PARTITION variable=weights complete
     #pragma HLS ARRAY_PARTITION variable=biases complete
 
-    ReadInputHeight: for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
-        ReadInputWidth: for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
+ReadInputHeight:
+    for (unsigned i_ih = 0; i_ih < CONFIG_T::in_height; i_ih++) {
+    ReadInputWidth:
+        for (unsigned i_iw = 0; i_iw < CONFIG_T::in_width / (data_T::size / CONFIG_T::n_chan); i_iw++) {
             if (CONFIG_T::strategy == nnet::latency && data_T::size / CONFIG_T::n_chan == 1) {
                 #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
             }
             if (i_ih % CONFIG_T::stride_height == 0 && i_iw % CONFIG_T::stride_width == 0) {
                 pointwise_mult_buffer<data_T, res_T, CONFIG_T>(data.read(), res, weights, biases);
             } else {
                 data.read();
             }
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void separable_conv_2d_cl(
-    hls::stream<data_T> &data,
-    hls::stream<res_T>  &res,
-    typename CONFIG_T::depthwise_config::weight_t depthwise_weights[CONFIG_T::depthwise_config::filt_height * CONFIG_T::depthwise_config::filt_width * CONFIG_T::depthwise_config::n_chan],
-    typename CONFIG_T::pointwise_config::weight_t pointwise_weights[CONFIG_T::pointwise_config::n_chan * CONFIG_T::pointwise_config::n_filt],
-    typename CONFIG_T::depthwise_config::bias_t   depthwise_biases[CONFIG_T::depthwise_config::n_chan],
-    typename CONFIG_T::pointwise_config::bias_t   pointwise_biases[CONFIG_T::pointwise_config::n_filt]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void separable_conv_2d_cl(hls::stream<data_T> &data, hls::stream<res_T> &res,
+                          typename CONFIG_T::depthwise_config::weight_t
+                              depthwise_weights[CONFIG_T::depthwise_config::filt_height *
+                                                CONFIG_T::depthwise_config::filt_width * CONFIG_T::depthwise_config::n_chan],
+                          typename CONFIG_T::pointwise_config::weight_t
+                              pointwise_weights[CONFIG_T::pointwise_config::n_chan * CONFIG_T::pointwise_config::n_filt],
+                          typename CONFIG_T::depthwise_config::bias_t depthwise_biases[CONFIG_T::depthwise_config::n_chan],
+                          typename CONFIG_T::pointwise_config::bias_t pointwise_biases[CONFIG_T::pointwise_config::n_filt]) {
     #pragma HLS DATAFLOW
 
     hls::stream<data_T> depthwise_res;
     unsigned res_depth = CONFIG_T::depthwise_config::out_height * CONFIG_T::depthwise_config::out_width;
     #pragma HLS STREAM variable=depthwise_res depth=res_depth
 
-    switch(CONFIG_T::depthwise_config::implementation){
-        case conv_implementation::linebuffer:
-            depthwise_conv_2d_buffer_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(data, depthwise_res, depthwise_weights, depthwise_biases);
-            break;
-        case conv_implementation::encoded:
-            depthwise_conv_2d_encoded_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(data, depthwise_res, depthwise_weights, depthwise_biases);
-            break;
-    } 
+    switch (CONFIG_T::depthwise_config::implementation) {
+    case conv_implementation::linebuffer:
+        depthwise_conv_2d_buffer_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(
+            data, depthwise_res, depthwise_weights, depthwise_biases);
+        break;
+    case conv_implementation::encoded:
+        depthwise_conv_2d_encoded_cl<data_T, data_T, typename CONFIG_T::depthwise_config>(
+            data, depthwise_res, depthwise_weights, depthwise_biases);
+        break;
+    }
 
-    pointwise_conv_2d_cl<data_T, res_T, typename CONFIG_T::pointwise_config>(depthwise_res, res, pointwise_weights, pointwise_biases);
+    pointwise_conv_2d_cl<data_T, res_T, typename CONFIG_T::pointwise_config>(depthwise_res, res, pointwise_weights,
+                                                                             pointwise_biases);
 }
 
-}
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_sepconv_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_sepconv_stream.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,94 +1,94 @@
 #ifndef NNET_SEPARABLE_CONV_STREAM_H_
 #define NNET_SEPARABLE_CONV_STREAM_H_
 
-#include "nnet_common.h"
 #include "hls_stream.h"
+#include "nnet_common.h"
 #include "nnet_conv_stream.h"
 
 namespace nnet {
 
-template<class data_T, class res_T, typename CONFIG_T>
-void depthwise_product(
-    data_T    data[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    res_T     res[CONFIG_T::n_chan],
-    typename CONFIG_T::weight_t  weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void depthwise_product(data_T data[CONFIG_T::kernel_size * CONFIG_T::n_chan], res_T res[CONFIG_T::n_chan],
+                       typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                       typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     #pragma HLS INLINE
 
     typename CONFIG_T::accum_t mult[CONFIG_T::kernel_size * CONFIG_T::n_chan];
     typename CONFIG_T::accum_t acc[CONFIG_T::n_chan];
 
     // Use a function_instantiate in case it helps to explicitly optimize unchanging weights/biases
     #pragma HLS function_instantiate variable=weights
 
     #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
 
     #pragma HLS ARRAY_PARTITION variable=mult complete
 
-    int multiplier_limit  = ceil(float(CONFIG_T::kernel_size * CONFIG_T::n_chan) / float(CONFIG_T::reuse_factor)) - floor(float(CONFIG_T::n_zeros) / float(CONFIG_T::reuse_factor));
-    CONFIG_T::mult_config::template product<data_T, typename CONFIG_T::mult_config::weight_t, typename CONFIG_T::mult_config::accum_t>::limit(multiplier_limit);
+    #pragma HLS ALLOCATION operation instances=mul limit=CONFIG_T::multiplier_limit
 
-    // Do the matrix-multiply
-    Product: for(int ii = 0; ii < CONFIG_T::kernel_size * CONFIG_T::n_chan; ii++) {
+// Do the matrix-multiply
+Product:
+    for (int ii = 0; ii < CONFIG_T::kernel_size * CONFIG_T::n_chan; ii++) {
         #pragma HLS UNROLL
-        mult[ii] = CONFIG_T::mult_config::template product<data_T, typename CONFIG_T::mult_config::weight_t, typename CONFIG_T::mult_config::accum_t>::product(data[ii], weights[ii]);
+        mult[ii] = CONFIG_T::mult_config::template product<data_T, typename CONFIG_T::mult_config::weight_t>::product(
+            data[ii], weights[ii]);
     }
 
-    // Initialize accumulator with input biases
-    ResetAccum: for(int iacc = 0; iacc < CONFIG_T::n_chan; iacc++) {
+// Initialize accumulator with input biases
+ResetAccum:
+    for (int iacc = 0; iacc < CONFIG_T::n_chan; iacc++) {
         #pragma HLS UNROLL
-        acc[iacc] = (typename CONFIG_T::accum_t) biases[iacc];
+        acc[iacc] = (typename CONFIG_T::accum_t)biases[iacc];
     }
 
-    // Accumulate multiplication result
-    Accum1: for(int ii = 0; ii < CONFIG_T::kernel_size; ii++) {
-        Accum2: for(int jj = 0; jj < CONFIG_T::n_chan; jj++) {
+// Accumulate multiplication result
+Accum1:
+    for (int ii = 0; ii < CONFIG_T::kernel_size; ii++) {
+    Accum2:
+        for (int jj = 0; jj < CONFIG_T::n_chan; jj++) {
             int index = ii * CONFIG_T::n_chan + jj;
             acc[jj] += mult[index];
         }
     }
 
-    // Cast to "res_t" type
-    Result: for(int ires = 0; ires < CONFIG_T::n_chan; ires++){
+// Cast to "res_t" type
+Result:
+    for (int ires = 0; ires < CONFIG_T::n_chan; ires++) {
         #pragma HLS UNROLL
         res[ires] = cast<data_T, res_T, CONFIG_T>(acc[ires]);
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void depthwise_mult_buffer(
-    hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    res_T& res_pack,
-    hls::stream<res_T>& res_stream,
-    unsigned & outputs_ready,
-    typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void depthwise_mult_buffer(hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                           res_T &res_pack, hls::stream<res_T> &res_stream, unsigned &outputs_ready,
+                           typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                           typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     #pragma HLS INLINE
 
     typename data_T::value_type data[CONFIG_T::kernel_size * CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=data complete
     typename res_T::value_type res[CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=res complete
 
-    InitData: for (int id = 0; id < CONFIG_T::kernel_size * CONFIG_T::n_chan; id++) {
+InitData:
+    for (int id = 0; id < CONFIG_T::kernel_size * CONFIG_T::n_chan; id++) {
         #pragma HLS UNROLL
         data[id] = data_window[id].read();
     }
 
-    #pragma HLS INLINE region
+    #pragma HLS INLINE recursive
     if (CONFIG_T::strategy == nnet::latency) {
         depthwise_product<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(data, res, weights, biases);
     } else {
         assert("Resource strategy for DepthwiseConv2D is not supported." && false);
     }
 
-    CastLoop: for (unsigned jj = 0; jj < CONFIG_T::n_chan; jj++) {
+CastLoop:
+    for (unsigned jj = 0; jj < CONFIG_T::n_chan; jj++) {
         #pragma HLS UNROLL
         if (res_T::size / CONFIG_T::n_chan == 1) {
             res_pack[jj] = res[jj];
         } else {
             res_pack[outputs_ready * CONFIG_T::n_chan + jj] = res[jj];
         }
     }
@@ -101,89 +101,85 @@
             outputs_ready = 0;
         } else {
             outputs_ready++;
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
+template <class data_T, class res_T, typename CONFIG_T>
 void compute_depthwise_output_encoded(
-    const data_T& in_elem,
-    hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    hls::stream<res_T> &res,
-    res_T &res_pack,
-    unsigned &outputs_ready,
+    const data_T &in_elem, hls::stream<typename data_T::value_type> data_window[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+    hls::stream<res_T> &res, res_T &res_pack, unsigned &outputs_ready,
     typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan],
-    ap_uint<CONFIG_T::kernel_size> *pixel_idx
-) {
+    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan], ap_uint<CONFIG_T::kernel_size> *pixel_idx) {
     #pragma HLS INLINE
 
-    MultLoop: for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
-        #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
-        CopyDataFilt: for (unsigned f = 0; f < CONFIG_T::kernel_size; f++) {
-            #pragma HLS UNROLL
-            CopyDataChan: for (unsigned c = 0; c < CONFIG_T::n_chan; c++) {
+MultLoop:
+    for (unsigned p = 0; p < data_T::size / CONFIG_T::n_chan; p++) {
+    #pragma HLS PIPELINE II=CONFIG_T::reuse_factor
+    CopyDataFilt:
+        for (unsigned f = 0; f < CONFIG_T::kernel_size; f++) {
+        #pragma HLS UNROLL
+        CopyDataChan:
+            for (unsigned c = 0; c < CONFIG_T::n_chan; c++) {
                 #pragma HLS UNROLL
-                if (pixel_idx[p][f]) data_window[f * CONFIG_T::n_chan + c].write(in_elem[p * CONFIG_T::n_chan + c]);
+                if (pixel_idx[p][f])
+                    data_window[f * CONFIG_T::n_chan + c].write(in_elem[p * CONFIG_T::n_chan + c]);
             }
         }
         if (pixel_idx[p][CONFIG_T::kernel_size - 1]) {
             depthwise_mult_buffer<data_T, res_T, CONFIG_T>(data_window, res_pack, res, outputs_ready, weights, biases);
         }
     }
 }
 
-
-template<class data_T, class res_T, typename CONFIG_T>
-void pointwise_mult_buffer(
-    const data_T &data_pack,
-    hls::stream<res_T> &res_stream,
-    typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void pointwise_mult_buffer(const data_T &data_pack, hls::stream<res_T> &res_stream,
+                           typename CONFIG_T::weight_t weights[CONFIG_T::n_chan * CONFIG_T::n_filt],
+                           typename CONFIG_T::bias_t biases[CONFIG_T::n_filt]) {
     #pragma HLS INLINE
 
     typename data_T::value_type data[CONFIG_T::n_chan];
     #pragma HLS ARRAY_PARTITION variable=data complete
 
     typename res_T::value_type res[CONFIG_T::n_filt];
     #pragma HLS ARRAY_PARTITION variable=res complete
 
     res_T res_pack;
     #pragma HLS DATA_PACK variable=res_pack
 
-    InitData: for (int id = 0; id < CONFIG_T::n_chan; id++) {
+InitData:
+    for (int id = 0; id < CONFIG_T::n_chan; id++) {
         #pragma HLS UNROLL
         data[id] = data_pack[id];
     }
 
-    #pragma HLS INLINE region
+    #pragma HLS INLINE recursive
     if (CONFIG_T::strategy == nnet::latency) {
-        dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(data, res, weights, biases);
+        dense_latency<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+            data, res, weights, biases);
     } else {
-        dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(data, res, weights, biases);
+        dense_resource<typename data_T::value_type, typename res_T::value_type, typename CONFIG_T::mult_config>(
+            data, res, weights, biases);
     }
 
-    CastLoop: for (unsigned jj = 0; jj < CONFIG_T::n_filt; jj++) {
+CastLoop:
+    for (unsigned jj = 0; jj < CONFIG_T::n_filt; jj++) {
         #pragma HLS UNROLL
         res_pack[jj] = res[jj];
     }
 
     res_stream.write(res_pack);
 }
 
 // Line Buffer Implementation (Phil's)
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_depthwise_output_buffer_1d(
-    const data_T& in_elem,
-    hls::stream<res_T> &res_stream,
-    typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_depthwise_output_buffer_1d(const data_T &in_elem, hls::stream<res_T> &res_stream,
+                                        typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                                        typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     #pragma HLS INLINE
 
     // Thresholds
     const static int lShiftX = CONFIG_T::filt_width - 1;
 
     // Counters
     static int pX = 0;
@@ -198,52 +194,53 @@
     res_T res_pack;
     #pragma HLS DATA_PACK variable=res_pack
 
     // Add pixel to buffer
     nnet::kernel_shift_1d<data_T, CONFIG_T>(in_elem, kernel_data);
 
     // Check to see if we have a full kernel
-    if ((sX - lShiftX) == 0 && pX > lShiftX - 1) { 
-      // Dense multiply
-      #pragma HLS INLINE region
-      if (CONFIG_T::strategy == nnet::latency) {
-        depthwise_product<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(kernel_data, res_out, weights, biases);
-      } else {
-        assert("Resource strategy for DepthwiseConv1D is not supported." && false);
-      }
-
-      // Pack output
-      CastLoop: for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
-          #pragma HLS UNROLL
-          res_pack[i_ic] = res_out[i_ic];
-      }
+    if ((sX - lShiftX) == 0 && pX > lShiftX - 1) {
+        // Dense multiply
+        #pragma HLS INLINE recursive
+        if (CONFIG_T::strategy == nnet::latency) {
+            depthwise_product<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(kernel_data, res_out,
+                                                                                                 weights, biases);
+        } else {
+            assert("Resource strategy for DepthwiseConv1D is not supported." && false);
+        }
+
+    // Pack output
+    CastLoop:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
+            #pragma HLS UNROLL
+            res_pack[i_ic] = res_out[i_ic];
+        }
 
-      // Write output to stream when output ready
-      res_stream.write(res_pack);
+        // Write output to stream when output ready
+        res_stream.write(res_pack);
     }
 
     // Pointer Housekeeping
-    if (pX + 1 == CONFIG_T::in_width)  // Includes padding, end of line (padded)
+    if (pX + 1 == CONFIG_T::in_width) // Includes padding, end of line (padded)
     {
         pX = 0;
         sX = 0;
     } else {
         pX = pX + 1;
-        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1; 
+        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1;
     }
-  }
+}
 
-template<class data_T, class res_T, typename CONFIG_T>
-void compute_depthwise_output_buffer_2d(
-    const data_T& in_elem,
-    ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width> line_buffer[MAX(CONFIG_T::filt_height - 1,1)][CONFIG_T::n_chan],
-    hls::stream<res_T> &res_stream,
-    typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
-    typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]
-) {
+template <class data_T, class res_T, typename CONFIG_T>
+void compute_depthwise_output_buffer_2d(const data_T &in_elem,
+                                        ap_shift_reg<typename data_T::value_type, CONFIG_T::in_width>
+                                            line_buffer[MAX(CONFIG_T::filt_height - 1, 1)][CONFIG_T::n_chan],
+                                        hls::stream<res_T> &res_stream,
+                                        typename CONFIG_T::weight_t weights[CONFIG_T::kernel_size * CONFIG_T::n_chan],
+                                        typename CONFIG_T::bias_t biases[CONFIG_T::n_chan]) {
     #pragma HLS INLINE
 
     // Thresholds
     const static int lShiftX = CONFIG_T::filt_width - 1;
     const static int lShiftY = CONFIG_T::filt_height - 1;
 
     // counters
@@ -262,46 +259,48 @@
     res_T res_pack;
     #pragma HLS DATA_PACK variable=res_pack
 
     // Add pixel to buffer
     nnet::shift_line_buffer<data_T, CONFIG_T>(in_elem, line_buffer, kernel_data);
 
     // Check to see if we have a full kernel
-    if ((sX - lShiftX) == 0 && (sY - lShiftY) == 0 && pY > lShiftY - 1 && pX > lShiftX - 1) { 
-      // Dense multiply
-      #pragma HLS INLINE region
-      if (CONFIG_T::strategy == nnet::latency) {
-        depthwise_product<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(kernel_data, res_out, weights, biases);
-      } else {
-        assert("Resource strategy for DepthwiseConv2D is not supported." && false);
-      }
+    if ((sX - lShiftX) == 0 && (sY - lShiftY) == 0 && pY > lShiftY - 1 && pX > lShiftX - 1) {
+        // Dense multiply
+        #pragma HLS INLINE recursive
+        if (CONFIG_T::strategy == nnet::latency) {
+            depthwise_product<typename data_T::value_type, typename res_T::value_type, CONFIG_T>(kernel_data, res_out,
+                                                                                                 weights, biases);
+        } else {
+            assert("Resource strategy for DepthwiseConv2D is not supported." && false);
+        }
 
-      // Pack output
-      CastLoop: for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
-          #pragma HLS UNROLL
-          res_pack[i_ic] = res_out[i_ic];
-      }
+    // Pack output
+    CastLoop:
+        for (unsigned i_ic = 0; i_ic < CONFIG_T::n_filt; i_ic++) {
+            #pragma HLS UNROLL
+            res_pack[i_ic] = res_out[i_ic];
+        }
 
-      // Write output to stream when output ready
-      res_stream.write(res_pack);
+        // Write output to stream when output ready
+        res_stream.write(res_pack);
     }
 
     // Pointer Housekeeping
-    if (pX + 1 == CONFIG_T::in_width)  // Includes padding, end of line (padded)
+    if (pX + 1 == CONFIG_T::in_width) // Includes padding, end of line (padded)
     {
         pX = 0;
         sX = 0;
-        if (pY + 1 == CONFIG_T::in_height) {  // Reached bottom of image
+        if (pY + 1 == CONFIG_T::in_height) { // Reached bottom of image
             pY = 0;
             sY = 0;
         } else {
             pY = pY + 1;
-            sY = ((sY - lShiftY) == 0) ? sY - CONFIG_T::stride_height + 1 : sY + 1; 
+            sY = ((sY - lShiftY) == 0) ? sY - CONFIG_T::stride_height + 1 : sY + 1;
         }
     } else {
         pX = pX + 1;
-        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1; 
+        sX = ((sX - lShiftX) == 0) ? sX - CONFIG_T::stride_width + 1 : sX + 1;
     }
-  }
-
 }
+
+} // namespace nnet
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_stream.h` & `hls4ml-0.7.0rc1/hls4ml/templates/quartus/firmware/nnet_utils/nnet_stream.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,120 +1,121 @@
+#ifndef NNET_CLONE_H
+#define NNET_CLONE_H
 
-#ifndef NNET_STREAM_H
-#define NNET_STREAM_H
-
-#include "hls_stream.h"
+#include "nnet_common.h"
 
 namespace nnet {
 
-struct broadcast_config
-{
-  static const unsigned in_height = 10;
-  static const unsigned in_width = 10;
-  static const unsigned n_chan = 1;
-  static const unsigned n_dupl = 2;
+struct broadcast_config {
+    static const unsigned in_height = 10;
+    static const unsigned in_width = 10;
+    static const unsigned n_chan = 1;
+    static const unsigned n_dupl = 2;
 };
 
-template<class data_T, class res_T, int N>
-void clone_stream(hls::stream<data_T> &data, hls::stream<res_T> &res1, hls::stream<res_T> &res2) {
-    CloneLoop: for (int i = 0; i < N / data_T::size; i++) {
-        #pragma HLS PIPELINE
+template <class data_T, class res_T, int N>
+void clone_stream(stream<data_T> &data, stream<res_T> &res1, stream<res_T> &res2) {
+CloneLoop:
+    #pragma ii 1
+    for (int i = 0; i < N / data_T::size; i++) {
+        data_T in_data = data.read();
+        res_T out_data1;
+        res_T out_data2;
+
+    ClonePack:
+        #pragma unroll
+        for (int j = 0; j < data_T::size; j++) {
+            out_data1[j] = in_data[j];
+            out_data2[j] = in_data[j];
+        }
+
+        res1.write(out_data1);
+        res2.write(out_data2);
+    }
+}
 
+template <class data_T, class res_T, int N>
+void clone_stream(stream<data_T> &data, stream<res_T> &res1, stream<res_T> &res2, stream<res_T> &res3) {
+CloneLoop:
+    #pragma ii 1
+    for (int i = 0; i < N / data_T::size; i++) {
         data_T in_data = data.read();
         res_T out_data1;
         res_T out_data2;
-        #pragma HLS DATA_PACK variable=out_data1
-        #pragma HLS DATA_PACK variable=out_data2
+        res_T out_data3;
 
-        ClonePack: for (int j = 0; j < data_T::size; j++) {
-            #pragma HLS UNROLL
+    ClonePack:
+        #pragma unroll
+        for (int j = 0; j < data_T::size; j++) {
             out_data1[j] = in_data[j];
             out_data2[j] = in_data[j];
+            out_data3[j] = in_data[j];
         }
 
         res1.write(out_data1);
         res2.write(out_data2);
+        res3.write(out_data3);
     }
 }
 
-template<class data_T, class res_T, int N>
-void repack_stream(hls::stream<data_T> &data, hls::stream<res_T> &res) {
+template <class data_T, class res_T, int N> void repack_stream(stream<data_T> &data, stream<res_T> &res) {
     if (data_T::size == res_T::size) {
+        #pragma ii 1
         for (int i = 0; i < N / data_T::size; i++) {
-            #pragma HLS PIPELINE
 
             data_T in_data = data.read();
             res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
 
+            #pragma unroll
             for (int j = 0; j < data_T::size; j++) {
-                #pragma HLS UNROLL
                 out_data[j] = in_data[j];
             }
 
             res.write(out_data);
         }
     } else if (data_T::size > res_T::size) {
         constexpr unsigned pack_diff = data_T::size / res_T::size;
+
         for (int i = 0; i < N / data_T::size; i++) {
-            if (N / data_T::size > 1) {
-                #pragma HLS PIPELINE
-            }
 
             data_T in_data = data.read();
             res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
 
+            #pragma ii 1
             for (int j = 0; j < pack_diff; j++) {
-                #pragma HLS PIPELINE
 
                 res_T out_data;
+
+                #pragma unroll
                 for (int k = 0; k < res_T::size; k++) {
-                    #pragma HLS UNROLL
                     out_data[k] = in_data[j * res_T::size + k];
                 }
                 res.write(out_data);
             }
         }
     } else { // data_T::size < res_T::size
         res_T out_data;
         constexpr unsigned pack_diff = res_T::size / data_T::size;
         unsigned pack_cnt = 0;
+        #pragma ii 1
         for (int i = 0; i < N / data_T::size; i++) {
-            #pragma HLS PIPELINE
 
             data_T in_data = data.read();
+
+            #pragma unroll
             for (int j = 0; j < data_T::size; j++) {
-                #pragma HLS UNROLL
                 out_data[pack_cnt * data_T::size + j] = in_data[j];
             }
 
             if (pack_cnt == pack_diff - 1) {
                 res.write(out_data);
                 pack_cnt = 0;
             } else {
                 pack_cnt++;
             }
         }
     }
 }
 
-template<class data_T, class res_T, typename CONFIG_T>
-void broadcast_stream(hls::stream<data_T> &data, hls::stream<res_T> &res) {
-    BroadcastLoop: for (int i = 0; i < CONFIG_T::in_height * CONFIG_T::in_width * CONFIG_T::n_chan / data_T::size; i++) {
-        #pragma HLS PIPELINE
-        data_T in_data = data.read();
-        for (int j = 0; j < CONFIG_T::n_dupl; j++) {
-            #pragma HLS PIPELINE
-            res_T out_data;
-            #pragma HLS DATA_PACK variable=out_data
-            for (int k = 0; k < res_T::size; k++) {
-                #pragma HLS UNROLL
-                out_data[k] = in_data[k];
-            }
-            res.write(out_data);
-        }
-    }
-}
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado/nnet_utils/nnet_types.h` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado/nnet_utils/nnet_types.h`

 * *Files 21% similar despite different names*

```diff
@@ -4,39 +4,34 @@
 #include <assert.h>
 #include <cstddef>
 #include <cstdio>
 
 namespace nnet {
 
 // Fixed-size array
-template<typename T, unsigned N>
-struct array {
+template <typename T, unsigned N> struct array {
     typedef T value_type;
     static const unsigned size = N;
 
     T data[N];
 
-    T& operator[](size_t pos) {
-        return data[pos];
-    }
-    
-    const T& operator[](size_t pos) const {
-        return data[pos];
-    }
+    T &operator[](size_t pos) { return data[pos]; }
+
+    const T &operator[](size_t pos) const { return data[pos]; }
 
-    array& operator=(const array &other) {
-        if(&other == this)
+    array &operator=(const array &other) {
+        if (&other == this)
             return *this;
 
         assert(N == other.size && "Array sizes must match.");
-        
+
         for (unsigned i = 0; i < N; i++) {
             #pragma HLS UNROLL
             data[i] = other[i];
         }
         return *this;
-    }  
+    }
 };
 
-}
+} // namespace nnet
 
 #endif
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/build_lib.sh` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/build_lib.sh`

 * *Files 0% similar despite different names*

```diff
@@ -11,8 +11,7 @@
 LIB_STAMP=mystamp
 
 ${CC} ${CFLAGS} ${INCFLAGS} -c firmware/${PROJECT}.cpp -o ${PROJECT}.o
 ${CC} ${CFLAGS} ${INCFLAGS} -c firmware/${PROJECT}_axi.cpp -o ${PROJECT}_axi.o
 ${CC} ${CFLAGS} ${INCFLAGS} -c ${PROJECT}_bridge.cpp -o ${PROJECT}_bridge.o
 ${CC} ${CFLAGS} ${INCFLAGS} -shared ${PROJECT}.o ${PROJECT}_axi.o ${PROJECT}_bridge.o -o firmware/${PROJECT}-${LIB_STAMP}.so
 rm -f *.o
-
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/python_drivers/axi_stream_driver.py` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/python_drivers/axi_stream_driver.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-from pynq import DefaultHierarchy, DefaultIP, allocate
-from pynq import Overlay
 from datetime import datetime
-import pynq.lib.dma
+
 import numpy as np
+from pynq import Overlay, allocate
 
 
 class NeuralNetworkOverlay(Overlay):
-    def __init__(self, bitfile_name, x_shape, y_shape, dtype=np.float32, dtbo=None, download=True, ignore_version=False,
-                 device=None):
+    def __init__(
+        self, bitfile_name, x_shape, y_shape, dtype=np.float32, dtbo=None, download=True, ignore_version=False, device=None
+    ):
         super().__init__(bitfile_name, dtbo=None, download=True, ignore_version=False, device=None)
         self.sendchannel = self.hier_0.axi_dma_0.sendchannel
         self.recvchannel = self.hier_0.axi_dma_0.recvchannel
         self.input_buffer = allocate(shape=x_shape, dtype=dtype)
         self.output_buffer = allocate(shape=y_shape, dtype=dtype)
 
     def _print_dt(self, timea, timeb, N):
-        dt = (timeb - timea)
-        dts = dt.seconds + dt.microseconds * 10 ** -6
+        dt = timeb - timea
+        dts = dt.seconds + dt.microseconds * 10**-6
         rate = N / dts
-        print("Classified {} samples in {} seconds ({} inferences / s)".format(N, dts, rate))
+        print(f"Classified {N} samples in {dts} seconds ({rate} inferences / s)")
         return dts, rate
 
     def predict(self, X, debug=False, profile=False, encode=None, decode=None):
         """
         Obtain the predictions of the NN implemented in the FPGA.
         Parameters:
         - X : the input vector. Should be numpy ndarray.
@@ -68,8 +68,8 @@
             self.output_buffer = decode(self.output_buffer)
 
         if profile:
             timeb = datetime.now()
             dts, rate = self._print_dt(timea, timeb, len(X))
             return self.output_buffer, dts, rate
         else:
-            return self.output_buffer
+            return self.output_buffer
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_lite_design.tcl` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_lite_design.tcl`

 * *Files 10% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 set tcldir [file dirname [info script]]
 source [file join $tcldir project.tcl]
 
-create_project project_1 ${myproject}_vivado_accelerator -part xc7z020clg400-1 -force
+create_project project_1 ${project_name}_vivado_accelerator -part xc7z020clg400-1 -force
 
 set_property board_part tul.com.tw:pynq-z2:part0:1.0 [current_project]
-set_property  ip_repo_paths  ${myproject}_prj [current_project]
+set_property  ip_repo_paths  ${project_name}_prj [current_project]
 update_ip_catalog
 
 # Create Block Designer design
 create_bd_design "design_1"
 create_bd_cell -type ip -vlnv xilinx.com:ip:processing_system7:5.5 processing_system7_0
 apply_bd_automation -rule xilinx.com:bd_rule:processing_system7 -config {make_external "FIXED_IO, DDR" apply_board_preset "1" Master "Disable" Slave "Disable" }  [get_bd_cells processing_system7_0]
-create_bd_cell -type ip -vlnv xilinx.com:hls:${myproject}_axi:1.0 ${myproject}_axi_0
-apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {Auto} Clk_xbar {Auto} Master {/processing_system7_0/M_AXI_GP0} Slave {/${myproject}_axi_0/s_axi_AXILiteS} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins ${myproject}_axi_0/s_axi_AXILiteS]
+create_bd_cell -type ip -vlnv xilinx.com:hls:${project_name}_axi:1.0 ${project_name}_axi_0
+apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {Auto} Clk_xbar {Auto} Master {/processing_system7_0/M_AXI_GP0} Slave {/${project_name}_axi_0/s_axi_AXILiteS} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins ${project_name}_axi_0/s_axi_AXILiteS]
 
-make_wrapper -files [get_files ./${myproject}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd] -top
-add_files -norecurse ./${myproject}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hdl/design_1_wrapper.v
+make_wrapper -files [get_files ./${project_name}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd] -top
+add_files -norecurse ./${project_name}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hdl/design_1_wrapper.v
 
 reset_run impl_1
 reset_run synth_1
 launch_runs impl_1 -to_step write_bitstream -jobs 6
 wait_on_run -timeout 360 impl_1
 
 open_run impl_1
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_stream_design.tcl` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/pynq-z2/tcl_scripts/axi_stream_design.tcl`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #@todo: try to remove startgroup and endgroup and see if it work
 set tcldir [file dirname [info script]]
 source [file join $tcldir project.tcl]
 
-create_project project_1 ${myproject}_vivado_accelerator -part xc7z020clg400-1 -force
+create_project project_1 ${project_name}_vivado_accelerator -part xc7z020clg400-1 -force
 
 set_property board_part tul.com.tw:pynq-z2:part0:1.0 [current_project]
-set_property  ip_repo_paths  ${myproject}_prj [current_project]
+set_property  ip_repo_paths  ${project_name}_prj [current_project]
 update_ip_catalog
 
 create_bd_design "design_1"
 
 startgroup
 create_bd_cell -type ip -vlnv xilinx.com:ip:processing_system7:5.5 processing_system7_0
 endgroup
@@ -32,27 +32,27 @@
 
 apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {Auto} Clk_xbar {Auto} Master {/axi_dma_0/M_AXI_MM2S} Slave {/processing_system7_0/S_AXI_HP0} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins processing_system7_0/S_AXI_HP0]
 endgroup
 
 apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {/processing_system7_0/FCLK_CLK0 (100 MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK0 (100 MHz)} Master {/axi_dma_0/M_AXI_S2MM} Slave {/processing_system7_0/S_AXI_HP0} ddr_seg {Auto} intc_ip {/axi_mem_intercon} master_apm {0}}  [get_bd_intf_pins axi_dma_0/M_AXI_S2MM]
 
 startgroup
-create_bd_cell -type ip -vlnv xilinx.com:hls:${myproject}_axi:1.0 ${myproject}_axi_0
+create_bd_cell -type ip -vlnv xilinx.com:hls:${project_name}_axi:1.0 ${project_name}_axi_0
 endgroup
 
-connect_bd_intf_net [get_bd_intf_pins axi_dma_0/M_AXIS_MM2S] [get_bd_intf_pins ${myproject}_axi_0/in_r]
-connect_bd_intf_net [get_bd_intf_pins ${myproject}_axi_0/out_r] [get_bd_intf_pins axi_dma_0/S_AXIS_S2MM]
+connect_bd_intf_net [get_bd_intf_pins axi_dma_0/M_AXIS_MM2S] [get_bd_intf_pins ${project_name}_axi_0/in_r]
+connect_bd_intf_net [get_bd_intf_pins ${project_name}_axi_0/out_r] [get_bd_intf_pins axi_dma_0/S_AXIS_S2MM]
 
-apply_bd_automation -rule xilinx.com:bd_rule:clkrst -config { Clk {/processing_system7_0/FCLK_CLK0 (100 MHz)} Freq {100} Ref_Clk0 {} Ref_Clk1 {} Ref_Clk2 {}}  [get_bd_pins ${myproject}_axi_0/ap_clk]
+apply_bd_automation -rule xilinx.com:bd_rule:clkrst -config { Clk {/processing_system7_0/FCLK_CLK0 (100 MHz)} Freq {100} Ref_Clk0 {} Ref_Clk1 {} Ref_Clk2 {}}  [get_bd_pins ${project_name}_axi_0/ap_clk]
 
-group_bd_cells hier_0 [get_bd_cells axi_dma_0] [get_bd_cells ${myproject}_axi_0]
+group_bd_cells hier_0 [get_bd_cells axi_dma_0] [get_bd_cells ${project_name}_axi_0]
 
-make_wrapper -files [get_files ./${myproject}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd] -top
+make_wrapper -files [get_files ./${project_name}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd] -top
 
-add_files -norecurse ./${myproject}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hdl/design_1_wrapper.v
+add_files -norecurse ./${project_name}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hdl/design_1_wrapper.v
 
 reset_run impl_1
 reset_run synth_1
 launch_runs impl_1 -to_step write_bitstream -jobs 6
 wait_on_run -timeout 360 impl_1
 
 open_run impl_1
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/python_drivers/axi_stream_driver.py` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/python_drivers/axi_stream_driver.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-from pynq import DefaultHierarchy, DefaultIP, allocate
-from pynq import Overlay
 from datetime import datetime
-import pynq.lib.dma
+
 import numpy as np
+from pynq import Overlay, allocate
 
 
 class NeuralNetworkOverlay(Overlay):
-    def __init__(self, bitfile_name, x_shape, y_shape, dtype=np.float32, dtbo=None, download=True, ignore_version=False,
-                 device=None):
+    def __init__(
+        self, bitfile_name, x_shape, y_shape, dtype=np.float32, dtbo=None, download=True, ignore_version=False, device=None
+    ):
         super().__init__(bitfile_name, dtbo=None, download=True, ignore_version=False, device=None)
         self.sendchannel = self.hier_0.axi_dma_0.sendchannel
         self.recvchannel = self.hier_0.axi_dma_0.recvchannel
         self.input_buffer = allocate(shape=x_shape, dtype=dtype)
         self.output_buffer = allocate(shape=y_shape, dtype=dtype)
 
     def _print_dt(self, timea, timeb, N):
-        dt = (timeb - timea)
-        dts = dt.seconds + dt.microseconds * 10 ** -6
+        dt = timeb - timea
+        dts = dt.seconds + dt.microseconds * 10**-6
         rate = N / dts
-        print("Classified {} samples in {} seconds ({} inferences / s)".format(N, dts, rate))
+        print(f"Classified {N} samples in {dts} seconds ({rate} inferences / s)")
         return dts, rate
 
     def predict(self, X, debug=False, profile=False, encode=None, decode=None):
         """
         Obtain the predictions of the NN implemented in the FPGA.
         Parameters:
         - X : the input vector. Should be numpy ndarray.
@@ -68,8 +68,8 @@
             self.output_buffer = decode(self.output_buffer)
 
         if profile:
             timeb = datetime.now()
             dts, rate = self._print_dt(timea, timeb, len(X))
             return self.output_buffer, dts, rate
         else:
-            return self.output_buffer
+            return self.output_buffer
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator/zcu102/tcl_scripts/axi_stream_design.tcl` & `hls4ml-0.7.0rc1/hls4ml/templates/vivado_accelerator/zcu102/tcl_scripts/axi_stream_design.tcl`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 #@todo: try to remove startgroup and endgroup and see if it work
 set tcldir [file dirname [info script]]
 source [file join $tcldir project.tcl]
 
-create_project project_1 ${myproject}_vivado_accelerator -part xczu9eg-ffvb1156-2-e -force
+create_project project_1 ${project_name}_vivado_accelerator -part xczu9eg-ffvb1156-2-e -force
 
 set_property board_part xilinx.com:zcu102:part0:3.3 [current_project]
-set_property  ip_repo_paths  ${myproject}_prj [current_project]
+set_property  ip_repo_paths  ${project_name}_prj [current_project]
 update_ip_catalog
 
 create_bd_design "design_1"
-set_property  ip_repo_paths ${myproject}_prj/solution1/impl/ip [current_project]
+set_property  ip_repo_paths ${project_name}_prj/solution1/impl/ip [current_project]
 update_ip_catalog
 
 startgroup
 create_bd_cell -type ip -vlnv xilinx.com:ip:zynq_ultra_ps_e:3.3 zynq_ultra_ps_e_0
 endgroup
 
 apply_bd_automation -rule xilinx.com:bd_rule:zynq_ultra_ps_e -config {apply_board_preset "1" }  [get_bd_cells zynq_ultra_ps_e_0]
@@ -33,25 +33,25 @@
 
 startgroup
 apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {/zynq_ultra_ps_e_0/pl_clk0 (99 MHz)} Clk_xbar {/zynq_ultra_ps_e_0/pl_clk0 (99 MHz)} Master {/axi_dma_0/M_AXI_S2MM} Slave {/zynq_ultra_ps_e_0/S_AXI_HPC0_FPD} ddr_seg {Auto} intc_ip {/axi_smc} master_apm {0}}  [get_bd_intf_pins axi_dma_0/M_AXI_S2MM]
 apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {/zynq_ultra_ps_e_0/pl_clk0 (99 MHz)} Clk_xbar {/zynq_ultra_ps_e_0/pl_clk0 (99 MHz)} Master {/zynq_ultra_ps_e_0/M_AXI_HPM1_FPD} Slave {/axi_dma_0/S_AXI_LITE} ddr_seg {Auto} intc_ip {/ps8_0_axi_periph} master_apm {0}}  [get_bd_intf_pins zynq_ultra_ps_e_0/M_AXI_HPM1_FPD]
 endgroup
 
 startgroup
-create_bd_cell -type ip -vlnv xilinx.com:hls:${myproject}_axi:1.0 ${myproject}_axi_0
+create_bd_cell -type ip -vlnv xilinx.com:hls:${project_name}_axi:1.0 ${project_name}_axi_0
 endgroup
-connect_bd_intf_net [get_bd_intf_pins axi_dma_0/M_AXIS_MM2S] [get_bd_intf_pins ${myproject}_axi_0/in_r]
-connect_bd_intf_net [get_bd_intf_pins axi_dma_0/S_AXIS_S2MM] [get_bd_intf_pins ${myproject}_axi_0/out_r]
+connect_bd_intf_net [get_bd_intf_pins axi_dma_0/M_AXIS_MM2S] [get_bd_intf_pins ${project_name}_axi_0/in_r]
+connect_bd_intf_net [get_bd_intf_pins axi_dma_0/S_AXIS_S2MM] [get_bd_intf_pins ${project_name}_axi_0/out_r]
 
-apply_bd_automation -rule xilinx.com:bd_rule:clkrst -config { Clk {/zynq_ultra_ps_e_0/pl_clk0 (99 MHz)} Freq {100} Ref_Clk0 {} Ref_Clk1 {} Ref_Clk2 {}}  [get_bd_pins ${myproject}_axi_0/ap_clk]
-group_bd_cells hier_0 [get_bd_cells axi_dma_0] [get_bd_cells ${myproject}_axi_0]
+apply_bd_automation -rule xilinx.com:bd_rule:clkrst -config { Clk {/zynq_ultra_ps_e_0/pl_clk0 (99 MHz)} Freq {100} Ref_Clk0 {} Ref_Clk1 {} Ref_Clk2 {}}  [get_bd_pins ${project_name}_axi_0/ap_clk]
+group_bd_cells hier_0 [get_bd_cells axi_dma_0] [get_bd_cells ${project_name}_axi_0]
 
-make_wrapper -files [get_files ./${myproject}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd] -top
+make_wrapper -files [get_files ./${project_name}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd] -top
 
-add_files -norecurse ./${myproject}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hdl/design_1_wrapper.v
+add_files -norecurse ./${project_name}_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hdl/design_1_wrapper.v
 
 reset_run impl_1
 reset_run synth_1
 launch_runs impl_1 -to_step write_bitstream -jobs 6
 wait_on_run -timeout 360 impl_1
 
 open_run impl_1
```

### Comparing `hls4ml-0.6.0/hls4ml/templates/vivado_accelerator_config.py` & `hls4ml-0.7.0rc1/hls4ml/backends/vivado_accelerator/vivado_accelerator_config.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,143 +1,162 @@
+import json
+import os
+
 import numpy as np
 
-from hls4ml.model.hls_layers import FixedPrecisionType, IntegerPrecisionType
-from hls4ml.templates import get_supported_boards_dict
+from hls4ml.model.layers import FixedPrecisionType, IntegerPrecisionType
 
 
-class VivadoAcceleratorConfig(object):
+class VivadoAcceleratorConfig:
     def __init__(self, config, model_inputs, model_outputs):
         self.config = config.config
-        self.board = self.config.get('Board', 'pynq-z2')
-        boards = get_supported_boards_dict()
-        if self.board in boards.keys():
-            board_info = boards[self.board]
+        self.board = self.config.get('AcceleratorConfig', {}).get('Board', 'pynq-z2')
+        self.supported_boards = json.load(open(os.path.dirname(__file__) + '/supported_boards.json'))
+        if self.board in self.supported_boards.keys():
+            board_info = self.supported_boards[self.board]
             self.part = board_info['part']
         else:
             raise Exception('The board does not appear in supported_boards.json file')
-        
-        if self.config.get('XilinxPart') is not None:
-            if self.config.get('XilinxPart') != self.part:
-                print('WARNING: You set a XilinxPart that does not correspond to the Board you specified. The correct '
-                      'XilinxPart is now set.')
-                self.config['XilinxPart'] = self.part
+
+        if self.config.get('Part') is not None:
+            if self.config.get('Part') != self.part:
+                print(
+                    'WARNING: You set a Part that does not correspond to the Board you specified. The correct '
+                    'Part is now set.'
+                )
+                self.config['Part'] = self.part
         accel_config = self.config.get('AcceleratorConfig', None)
         if accel_config is not None:
             prec = accel_config.get('Precision')
             if prec is None:
                 raise Exception('Precision must be provided in the AcceleratorConfig')
             else:
                 if prec.get('Input') is None or prec.get('Output') is None:
                     raise Exception('Input and Output fields must be provided in the AcceleratorConfig->Precision')
         else:
-            accel_config = {'Precision': 
-                                {
-                                    'Input': 'float',
-                                    'Output': 'float'
-                                },
-                            'Driver': 'python',
-                            'Interface': 'axi_stream'
-                            }
+            accel_config = {
+                'Precision': {'Input': 'float', 'Output': 'float'},
+                'Driver': 'python',
+                'Interface': 'axi_stream',
+            }
             config.config['AcceleratorConfig'] = accel_config
 
-        self.interface = self.config['AcceleratorConfig'].get('Interface',
-                                                              'axi_stream')  # axi_stream, axi_master, axi_lite
+        self.interface = self.config['AcceleratorConfig'].get('Interface', 'axi_stream')  # axi_stream, axi_master, axi_lite
         self.driver = self.config['AcceleratorConfig'].get('Driver', 'python')  # python or c
-        self.input_type = self.config['AcceleratorConfig']['Precision'].get('Input',
-                                                                            'float')  # float, double or ap_fixed<a,b>
-        self.output_type = self.config['AcceleratorConfig']['Precision'].get('Output',
-                                                                             'float')  # float, double or ap_fixed<a,b>
-
-        assert len(
-            model_inputs) == 1, "Only models with one input tensor are currently supported by VivadoAcceleratorBackend"
-        assert len(
-            model_outputs) == 1, "Only models with one output tensor are currently supported by VivadoAcceleratorBackend"
+        self.input_type = self.config['AcceleratorConfig']['Precision'].get(
+            'Input', 'float'
+        )  # float, double or ap_fixed<a,b>
+        self.output_type = self.config['AcceleratorConfig']['Precision'].get(
+            'Output', 'float'
+        )  # float, double or ap_fixed<a,b>
+        self.platform = self.config['AcceleratorConfig'].get(
+            'Platform', 'xilinx_u250_xdma_201830_2'
+        )  # Get platform folder name
+
+        assert (
+            len(model_inputs) == 1
+        ), "Only models with one input tensor are currently supported by VivadoAcceleratorBackend"
+        assert (
+            len(model_outputs) == 1
+        ), "Only models with one output tensor are currently supported by VivadoAcceleratorBackend"
         self.inp = model_inputs[0]
         self.out = model_outputs[0]
         inp_axi_t = self.input_type
         out_axi_t = self.output_type
 
         if inp_axi_t not in ['float', 'double']:
             self.input_type = self._next_factor8_type(config.backend.convert_precision_string(inp_axi_t))
         if out_axi_t not in ['float', 'double']:
             self.output_type = self._next_factor8_type(config.backend.convert_precision_string(out_axi_t))
 
-        if self.input_type is 'float':
+        if self.input_type == 'float':
             self.input_bitwidth = 32
-        elif self.input_type is 'double':
+        elif self.input_type == 'double':
             self.input_bitwidth = 64
         else:
             self.input_bitwidth = config.backend.convert_precision_string(inp_axi_t).width
 
-        if out_axi_t is 'float':
+        if out_axi_t == 'float':
             self.output_bitwidth = 32
-        elif out_axi_t is 'double':
+        elif out_axi_t == 'double':
             self.output_bitwidth = 64
         else:
             self.output_bitwidth = config.backend.convert_precision_string(out_axi_t).width
 
     def _next_factor8_type(self, p):
-        ''' Return a new type with the width rounded to the next factor of 8 up to p's width
-            Args:
-                p : IntegerPrecisionType or FixedPrecisionType
-            Returns:
-                An IntegerPrecisionType or FixedPrecisionType with the width rounder up to the next factor of 8
-                of p's width. Other parameters (fractional bits, extra modes) stay the same.
+        '''Return a new type with the width rounded to the next factor of 8 up to p's width
+        Args:
+            p : IntegerPrecisionType or FixedPrecisionType
+        Returns:
+            An IntegerPrecisionType or FixedPrecisionType with the width rounder up to the next factor of 8
+            of p's width. Other parameters (fractional bits, extra modes) stay the same.
         '''
         W = p.width
         newW = int(np.ceil(W / 8) * 8)
         if isinstance(p, FixedPrecisionType):
-            return FixedPrecisionType(newW, p.integer, p.signed, p.rounding_mode, p.saturation_mode,
-                                      p.saturation_bits)
+            return FixedPrecisionType(newW, p.integer, p.signed, p.rounding_mode, p.saturation_mode, p.saturation_bits)
         elif isinstance(p, IntegerPrecisionType):
             return IntegerPrecisionType(newW, p.signed)
 
     def get_io_bitwidth(self):
         return self.input_bitwidth, self.output_bitwidth
 
     def get_corrected_types(self):
         return self.input_type, self.output_type, self.inp, self.out
 
     def get_interface(self):
         return self.interface
 
     def get_board_info(self, board=None):
-        boards = get_supported_boards_dict()
         if board is None:
             board = self.board
-        if board in boards.keys():
-            return boards[board]
+        if board in self.supported_boards.keys():
+            return self.supported_boards[board]
         else:
             raise Exception('The board is still not supported')
 
     def get_part(self):
         return self.part
 
     def get_driver(self):
         return self.driver
 
     def get_board(self):
         return self.board
 
+    def get_platform(self):
+        return self.platform
+
+    def get_clock_period(self):
+        return self.clock_period
+
     def get_driver_path(self):
-        return '../templates/vivado_accelerator/' + self.board + '/' + self.driver + '_drivers/' + \
-               self.get_driver_file()
+        if self.board.startswith('alveo'):
+            return '../templates/vivado_accelerator/' + 'alveo/' + self.driver + '_drivers/' + self.get_driver_file()
+        else:
+            return '../templates/vivado_accelerator/' + self.board + '/' + self.driver + '_drivers/' + self.get_driver_file()
 
     def get_driver_file(self):
         driver_ext = '.py' if self.driver == 'python' else '.h'
         return self.interface + '_driver' + driver_ext
 
+    def get_krnl_rtl_src_dir(self):
+        return '../templates/vivado_accelerator/' + 'alveo/' + '/krnl_rtl_src'
+
     def get_input_type(self):
         return self.input_type
 
     def get_output_type(self):
         return self.output_type
 
     def get_tcl_file_path(self):
         board_info = self.get_board_info(self.board)
         tcl_scripts = board_info.get('tcl_scripts', None)
         if tcl_scripts is None:
             raise Exception('No tcl scripts definition available for the board in supported_board.json')
         tcl_script = tcl_scripts.get(self.interface, None)
         if tcl_script is None:
             raise Exception('No tcl script definition available for the desired interface in supported_board.json')
-        return '../templates/vivado_accelerator/' + self.board + '/tcl_scripts/' + tcl_script
+        if self.board.startswith('alveo'):
+            return '../templates/vivado_accelerator/' + 'alveo/' + '/tcl_scripts/' + tcl_script
+        else:
+            return '../templates/vivado_accelerator/' + self.board + '/tcl_scripts/' + tcl_script
```

### Comparing `hls4ml-0.6.0/hls4ml/utils/config.py` & `hls4ml-0.7.0rc1/hls4ml/utils/config.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,345 +1,329 @@
-from __future__ import print_function
 import json
 
+import qkeras
+
 import hls4ml
 
 
-def create_config(output_dir='my-hls-test', project_name='myproject',
-    backend='Vivado', **kwargs):
+def create_config(output_dir='my-hls-test', project_name='myproject', backend='Vivado', **kwargs):
 
-    backend_list = hls4ml.templates.get_available_backends()
-    if backend not in backend_list:
-        raise Exception('Unknown backend: {}'.format(backend))
+    backend_list = hls4ml.backends.get_available_backends()
+    if backend.lower() not in backend_list:
+        raise Exception(f'Unknown backend: {backend}')
 
-    backend = hls4ml.templates.get_backend(backend)
+    backend = hls4ml.backends.get_backend(backend)
 
     backend_config = backend.create_initial_config(**kwargs)
 
     config = {}
     config['OutputDir'] = output_dir
     config['ProjectName'] = project_name
     config['Backend'] = backend.name
     config.update(backend_config)
 
     return config
 
+
 def _get_precision_from_quantizer(quantizer):
-    import qkeras
+
     if isinstance(quantizer, str):
         quantizer_obj = qkeras.get_quantizer(quantizer)
         quantizer = {}
         # Some activations are classes with get_config method
         if hasattr(quantizer_obj, 'get_config'):
             quantizer['class_name'] = quantizer_obj.__class__.__name__
             quantizer['config'] = quantizer_obj.get_config()
         # Some activations are just functions
-        else: 
+        else:
             quantizer['class_name'] = quantizer_obj.__name__
 
-    supported_quantizers = ['quantized_bits', 'quantized_relu', 'quantized_tanh', 'quantized_po2', 'quantized_relu_po2']
+    supported_quantizers = [
+        'quantized_bits',
+        'quantized_relu',
+        'quantized_tanh',
+        'quantized_sigmoid',
+        'quantized_po2',
+        'quantized_relu_po2',
+        'linear',
+    ]
     signed = True
+    rnd = "AP_TRN"
+    overflow = "AP_WRAP"
+
     if quantizer['class_name'] in supported_quantizers:
         bits = int(quantizer['config']['bits'])
         # if integer isn't specified, it should be the same as bits
-        integer = int(quantizer['config'].get('integer', bits-1)) + 1
-        if quantizer['class_name'] == 'quantized_relu':
+        integer = int(quantizer['config'].get('integer', bits - 1)) + 1
+        # for quantizers use the following default rounding and overflow
+        rnd = "AP_RND_CONV"
+        overflow = "AP_SAT"
+        if quantizer['class_name'] in ('quantized_relu', 'quantized_relu_po2'):
             signed = False
             integer -= 1
+        elif quantizer['class_name'] == 'quantized_tanh':
+            overflow = "AP_SAT_SYM" if quantizer['config']['symmetric'] else "AP_SAT"
+            integer = 1
+        elif quantizer['class_name'] == 'quantized_sigmoid':
+            integer = 0
+            signed = False
+
     elif quantizer['class_name'] in ['binary', 'stochastic_binary', 'binary_tanh']:
         bits = 2
         integer = 2
-    
+
     elif quantizer['class_name'] in ['ternary', 'stochastic_ternary', 'ternary_tanh']:
         bits = 2
         integer = 2
     else:
         raise Exception('ERROR: Unsupported quantizer: {}'.format(quantizer['class_name']))
 
     decimal = bits - integer
-    signed = '' if signed else 'u'
+
     if decimal > 0:
-        return 'ap_{}fixed<{},{}>'.format(signed, bits, integer)
+        return hls4ml.model.types.FixedPrecisionType(
+            width=bits, integer=integer, signed=signed, rounding_mode=rnd, saturation_mode=overflow
+        )
     else:
-        return 'ap_{}int<{}>'.format(signed, bits)
+        return hls4ml.model.types.IntegerPrecisionType(width=integer, signed=signed)
 
-def config_from_keras_model(model, granularity='model', default_precision='ap_fixed<16,6>', default_reuse_factor=1):
+
+def config_from_keras_model(
+    model, granularity='model', backend=None, default_precision='fixed<16,6>', default_reuse_factor=1
+):
     """Create an HLS conversion config given the Keras model.
 
     This function serves as the initial step in creating the custom conversion configuration.
     Users are advised to inspect the returned object to tweak the conversion configuration.
     The return object can be passed as `hls_config` parameter to `convert_from_keras_model`.
 
     Args:
         model: Keras model
         granularity (str, optional): Granularity of the created config. Defaults to 'model'.
             Can be set to 'model', 'type' and 'layer'.
 
             Granularity can be used to generate a more verbose config that can be fine-tuned.
-            The default granulrity ('model') will generate config keys that apply to the whole
+            The default granularity ('model') will generate config keys that apply to the whole
             model, so changes to the keys will affect the entire model. 'type' granularity will
             generate config keys that affect all layers of a given type, while the 'name' granularity
             will generate config keys for every layer separately, allowing for highly specific
             configuration tweaks.
-        default_precision (str, optional): Default precision to use. Defaults to 'ap_fixed<16,6>'.
+        backend(str, optional): Name of the backend to use
+        default_precision (str, optional): Default precision to use. Defaults to 'fixed<16,6>'.
         default_reuse_factor (int, optional): Default reuse factor. Defaults to 1.
 
     Raises:
         Exception: If Keras model has layers not supported by hls4ml.
 
     Returns:
         [dict]: The created config.
     """
     if granularity.lower() not in ['model', 'type', 'name']:
-        raise Exception('Invalid configuration granularity specified, expected "model", "type" or "name" got "{}"'.format(granularity))
+        raise Exception(
+            f'Invalid configuration granularity specified, expected "model", "type" or "name" got "{granularity}"'
+        )
+
+    if backend is not None:
+        backend = hls4ml.backends.get_backend(backend)
 
-    #This is a list of dictionaries to hold all the layer info we need to generate HLS
+    # This is a list of dictionaries to hold all the layer info we need to generate HLS
     layer_list = []
 
     if isinstance(model, dict):
         model_arch = model
     else:
         model_arch = json.loads(model.to_json())
 
-    #Define supported layers
-    core_layers = ['InputLayer', 'Dropout', 'Flatten', 'Reshape', 'Permute']
-    dense_layers = ['Dense', 'BinaryDense', 'TernaryDense']
-    conv_layers = ['Conv1D', 'Conv2D', 'BinaryConv2D']
-    pooling_layers = ['MaxPooling1D', 'MaxPooling2D', 'GlobalMaxPooling1D', 'GlobalMaxPooling2D', 'AveragePooling1D', 'AveragePooling2D', 'GlobalAveragePooling1D', 'GlobalAveragePooling2D']
-    norm_layers = ['BatchNormalization']
-    activation_layers = ['Activation', 'LeakyReLU', 'ThresholdedReLU', 'ELU', 'PReLU', 'Softmax', 'ReLU']
-    merge_layers = ['Add', 'Subtract', 'Multiply', 'Average', 'Maximum', 'Minimum', 'Concatenate', 'Dot']
-    qkeras_layers = ['QDense', 'QActivation', 'QConv1D', 'QConv2D', 'QBatchNormalization', 'QConv2DBatchnorm']
-    #Define layers to skip because they're not configurable or not converted to HLS
-    skip_layers = ['Dropout', 'Flatten', 'Reshape', 'Permute']
-    #All supported layers
-    supported_layers = core_layers + dense_layers + conv_layers + pooling_layers + norm_layers + activation_layers + merge_layers + qkeras_layers + skip_layers
-
-    keras_layer_config = None
-    if model_arch['class_name'] == 'Sequential':
-        print('Interpreting Sequential')
-        keras_layer_config = model_arch['config']
-        if 'layers' in keras_layer_config: # Newer Keras versions have 'layers' in 'config' key
-            keras_layer_config = keras_layer_config['layers']
-        # Sequential doesn't have InputLayer in TF < 2.3 (Keras 2.4.0)
-        if keras_layer_config[0]['class_name'] != 'InputLayer':
-            input_layer = {}
-            input_layer['name'] = 'input1'
-            input_layer['class_name'] = 'Input'
-            layer_list.append(input_layer)
-    elif model_arch['class_name'] in ['Model', 'Functional']:
-        print('Interpreting Model')
-        keras_layer_config = model_arch['config']['layers']
-
-    print('Topology:')
-    for keras_layer in keras_layer_config:
-        if keras_layer['class_name'] not in supported_layers:
-            raise Exception('ERROR: Unsupported layer type: {}'.format(keras_layer['class_name']))
-        if keras_layer['class_name'] in skip_layers:
-            continue
-
-        #Dictionary to fill in and append to layer_list
-        layer = {}
-
-        #Extract name for finding weights and biases
-        layer['name'] = keras_layer['config']['name']
-        layer['class_name'] = keras_layer['class_name']
-        layer['config'] = keras_layer['config']
-
-        if layer['class_name'] == 'InputLayer':
-            layer['class_name'] = 'Input'
-
-        if layer['class_name'] in qkeras_layers:
-            layer['precision'] = {}
-            for qname, qclass in layer['config'].items():
-                if 'quantizer' in qname.lower():
-                    pname = qname.split('_quantizer')[0]
-                    if pname == 'kernel': pname = 'weight'
-                    if qclass is not None:
-                        precision = _get_precision_from_quantizer(qclass)
-                        layer['precision'][pname] = precision
-                elif qname == 'activation' and layer['class_name'] == 'QActivation':
-                    precision = _get_precision_from_quantizer(qclass)
-                    layer['precision']['result'] = precision
-
-        print('Layer name: {}, layer type: {}'.format(layer['name'], layer['class_name']))
-        layer_list.append( layer )
-        if 'activation' in layer['config'] and layer['class_name'] not in activation_layers + qkeras_layers:
-            act_layer = {}
-            act_layer['name'] = layer['name'] + '_' + layer['config']['activation']
-            act_layer['class_name'] = 'Activation'
-            print('  -> Activation ({}), layer name: {}'.format(layer['config']['activation'], layer['name']))
-            layer_list.append(act_layer)
+    reader = hls4ml.converters.KerasModelReader(model)
 
+    layer_list, _, _ = hls4ml.converters.parse_keras_model(model_arch, reader)
 
     def make_layer_config(layer):
+        cls_name = layer['class_name']
+        if 'config' in layer.keys():
+            if 'activation' in layer['config'].keys():
+                if layer['config']['activation'] == 'softmax':
+                    cls_name = 'Softmax'
+
+        layer_cls = hls4ml.model.layers.layer_map[cls_name]
+        if backend is not None:
+            layer_cls = backend.create_layer_class(layer_cls)
+
         layer_config = {}
-        if layer['class_name'] in dense_layers + conv_layers:
-            layer_config['Precision'] = {}
-            layer_config['Precision']['weight'] = default_precision
-            layer_config['Precision']['bias'] = default_precision
-            layer_config['Precision']['result'] = default_precision
-            layer_config['ReuseFactor'] = default_reuse_factor
 
-        elif layer['class_name'] in activation_layers:
-            layer_config['Precision'] = default_precision
-            layer_config['ReuseFactor'] = default_reuse_factor
-            layer_config['table_size'] = 1024
-            is_softmax = layer['class_name'] == 'Softmax'
-            if 'config' in layer.keys():
-                if 'activation' in layer['config'].keys():
-                    is_softmax = is_softmax or (layer['config']['activation'] == 'softmax')
-            if is_softmax:
-               layer_config['exp_table_t'] = 'ap_fixed<18,8,AP_RND,AP_SAT>'
-               layer_config['inv_table_t'] = 'ap_fixed<18,8,AP_RND,AP_SAT>'
+        config_attrs = [a for a in layer_cls.expected_attributes if a.configurable]
+        for attr in config_attrs:
+            if isinstance(attr, hls4ml.model.attributes.TypeAttribute):
+                precision_cfg = layer_config.setdefault('Precision', {})
+                name = attr.name
+                if name.endswith('_t'):
+                    name = name[:-2]
+                if attr.default is None:
+                    precision_cfg[name] = default_precision
+                else:
+                    precision_cfg[name] = str(attr.default)
             else:
-                layer_config['table_t'] = 'ap_fixed<18,8>'
-        
-        elif layer['class_name'] in norm_layers:
-            layer_config['Precision'] = {}
-            layer_config['Precision']['scale'] = default_precision
-            layer_config['Precision']['bias'] = default_precision
-            layer_config['ReuseFactor'] = default_reuse_factor
-        
-        elif layer['class_name'] in qkeras_layers:
-            if 'precision' in layer:
-                layer_config['Precision'] = {}
-                for name, precision in layer['precision'].items():
-                    layer_config['Precision'][name] = precision
+                if attr.default is not None:
+                    layer_config[attr.config_name] = attr.default
+
+        quantizers = {qname: qclass for qname, qclass in layer.items() if 'quantizer' in qname and qclass is not None}
+        for qname, qclass in quantizers.items():
+            pname = qname.lower().split('_quantizer')[0]
+            if pname == 'activation':
+                pname = 'result'
+            if isinstance(qclass, dict):
+                precision = _get_precision_from_quantizer(qclass)
             else:
-                print('WARNING: Found no precision information in QKeras layer {} ({})'.format(layer['name'], layer['class_name']))
-                layer_config['Precision'] = default_precision
+                precision = qclass.hls_type
+            # TODO In the next version of this function, these should not be exposed to user to tweak
+            layer_config['Precision'][pname] = str(precision)
+
+        if layer['class_name'] in ['GarNet', 'GarNetStack']:
+            # Define default precisions for various internal arrays (can be overridden from the config file)
+            import math
+
+            log2_reuse = int(math.log(default_reuse_factor, 2.0))
+            n_vertices_width = int(math.log(layer['n_vertices'], 2.0))
+
+            # We always give 10 digits for the subintegral part
+            fwidth = 10
+            # Integral precision for aggr_t depends on how large the temporary sum for weighed feature mean will be
+            aggr_intw = max(log2_reuse, n_vertices_width - log2_reuse) + 3  # safety factor 2**3
+            aggr_w = aggr_intw + fwidth
+            # edge_weight_aggr_t does not need the safety factor
+            ew_aggr_intw = aggr_intw - 3
+            ew_aggr_w = ew_aggr_intw + fwidth
+
+            layer_config['Precision'] = {}
+            layer_config['Precision']['edge_weight'] = 'ap_ufixed<10,0,AP_TRN,AP_SAT>'
+            layer_config['Precision']['edge_weight_aggr'] = f'ap_ufixed<{ew_aggr_w},{ew_aggr_intw},AP_TRN,AP_SAT>'
+            layer_config['Precision']['aggr'] = f'ap_fixed<{aggr_w},{aggr_intw},AP_TRN,AP_SAT>'
+            layer_config['Precision']['norm'] = 'ap_ufixed<14,4,AP_TRN,AP_SAT>'
+
             layer_config['ReuseFactor'] = default_reuse_factor
 
         elif layer['class_name'] == 'Input':
-            layer_config['Precision'] = {}
-            layer_config['Precision']['result'] = default_precision
+            dtype = layer['config']['dtype']
+            if dtype.startswith('int') or dtype.startswith('uint'):
+                typename = dtype[: dtype.index('int') + 3]
+                width = int(dtype[dtype.index('int') + 3 :])
+                layer_config['Precision']['result'] = f'ap_{typename}<{width}>'
+            # elif bool, q[u]int, ...
 
-        else:
-            layer_config['Precision'] = default_precision
-        
         return layer_config
 
     config = {}
 
     model_config = {}
     model_config['Precision'] = default_precision
     model_config['ReuseFactor'] = default_reuse_factor
     model_config['Strategy'] = 'Latency'
-    #model_config['Compression'] = False
-    #model_config['Trace'] = False
+    model_config['BramFactor'] = 1_000_000_000
+    model_config['TraceOutput'] = False
 
     config['Model'] = model_config
-    
+
     if granularity.lower() == 'type':
         type_config = {}
         for layer in layer_list:
             if layer['class_name'] in type_config:
                 continue
             layer_config = make_layer_config(layer)
             type_config[layer['class_name']] = layer_config
-        
+
         config['LayerType'] = type_config
 
     elif granularity.lower() == 'name':
         name_config = {}
         for layer in layer_list:
             layer_config = make_layer_config(layer)
             name_config[layer['name']] = layer_config
-        
+
         config['LayerName'] = name_config
 
     return config
 
 
-def config_from_pytorch_model(model, granularity='model', default_precision='ap_fixed<16,6>', default_reuse_factor=1):
-    """Generate configuration dictionary from a Pytorch model.
-    
-    Parameters
-    ----------
-    model : Pytorch model object.
-        Model to be converted to hls model object.
-    granularity : string, optional
-        How granular you want the configuration to be.
-    default_precision : string, optional
-        Defines the precsion of your inputs, outputs, weights and biases.
-        It is denoted by ap_fixed<X,Y>, where Y is the number of bits representing 
-        the signed number above the binary point (i.e. the integer part),
-        and X is the total number of bits. Additionally, integers in fixed precision 
-        data type (ap_int<N>, where N is a bit-size from 1 to 1024) can also be used. 
-    default_reuse_factor : int, optional
-        Reuse factor for hls model
-        
-    Returns
-    -------
-    config : dict
-        configuration dictionary to be used in Pytorch converter.
-        
-    See Also
-    --------
-    hls4ml.config_from_keras_model, hls4ml.convert_from_onnx_model
-    
-    Examples
-    --------
-    >>> import hls4ml
-    >>> config = hls4ml.utils.config_from_keras_model(model, granularity='model')
-    >>> hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config)
+def config_from_pytorch_model(
+    model, granularity='model', backend=None, default_precision='ap_fixed<16,6>', default_reuse_factor=1
+):
+    """Create an HLS conversion config given the PyTorch model.
 
+    This function serves as the initial step in creating the custom conversion configuration.
+    Users are advised to inspect the returned object to tweak the conversion configuration.
+    The return object can be passed as `hls_config` parameter to `convert_from_pytorch_model`.
+
+    Args:
+        model: PyTorch model
+        granularity (str, optional): Granularity of the created config. Defaults to 'model'.
+            Can be set to 'model', 'type' and 'layer'.
+
+            Granularity can be used to generate a more verbose config that can be fine-tuned.
+            The default granularity ('model') will generate config keys that apply to the whole
+            model, so changes to the keys will affect the entire model. 'type' granularity will
+            generate config keys that affect all layers of a given type, while the 'name' granularity
+            will generate config keys for every layer separately, allowing for highly specific
+            configuration tweaks.
+        backend(str, optional): Name of the backend to use
+        default_precision (str, optional): Default precision to use. Defaults to 'fixed<16,6>'.
+        default_reuse_factor (int, optional): Default reuse factor. Defaults to 1.
+
+    Raises:
+        Exception: If PyTorch model has layers not supported by hls4ml.
+
+    Returns:
+        [dict]: The created config.
     """
-    
+
     config = {}
 
     model_config = {}
     model_config['Precision'] = default_precision
     model_config['ReuseFactor'] = default_reuse_factor
     model_config['Strategy'] = 'Latency'
 
     config['Model'] = model_config
-    
+
     return config
 
 
-def config_from_onnx_model(model, granularity='model', default_precision='ap_fixed<16,6>', default_reuse_factor=1):
-    """Generate configuration dictionary from an ONNX model.
-    
-    Parameters
-    ----------
-    model : ONNX model object.
-        Model to be converted to hls model object.
-    granularity : string, optional
-        How granular you want the configuration to be.
-    default_precision : string, optional
-        Defines the precsion of your inputs, outputs, weights and biases.
-        It is denoted by ap_fixed<X,Y>, where Y is the number of bits representing 
-        the signed number above the binary point (i.e. the integer part),
-        and X is the total number of bits. Additionally, integers in fixed precision 
-        data type (ap_int<N>, where N is a bit-size from 1 to 1024) can also be used. 
-    default_reuse_factor : int, optional
-        Reuse factor for hls model
-        
-    Returns
-    -------
-    config : dict
-        configuration dictionary to be used in ONNX converter.
-        
-    See Also
-    --------
-    hls4ml.config_from_keras_model, hls4ml.convert_from_pytorch_model
-    
-    Examples
-    --------
-    >>> import hls4ml
-    >>> config = hls4ml.utils.config_from_keras_model(model, granularity='model')
-    >>> hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config)
+def config_from_onnx_model(
+    model, granularity='model', backend=None, default_precision='ap_fixed<16,6>', default_reuse_factor=1
+):
+    """Create an HLS conversion config given the ONNX model.
+
+    This function serves as the initial step in creating the custom conversion configuration.
+    Users are advised to inspect the returned object to tweak the conversion configuration.
+    The return object can be passed as `hls_config` parameter to `convert_from_onnx_model`.
 
+    Args:
+        model: ONNX model
+        granularity (str, optional): Granularity of the created config. Defaults to 'model'.
+            Can be set to 'model', 'type' and 'layer'.
+
+            Granularity can be used to generate a more verbose config that can be fine-tuned.
+            The default granularity ('model') will generate config keys that apply to the whole
+            model, so changes to the keys will affect the entire model. 'type' granularity will
+            generate config keys that affect all layers of a given type, while the 'name' granularity
+            will generate config keys for every layer separately, allowing for highly specific
+            configuration tweaks.
+        backend(str, optional): Name of the backend to use
+        default_precision (str, optional): Default precision to use. Defaults to 'fixed<16,6>'.
+        default_reuse_factor (int, optional): Default reuse factor. Defaults to 1.
+
+    Raises:
+        Exception: If ONNX model has layers not supported by hls4ml.
+
+    Returns:
+        [dict]: The created config.
     """
-    
+
     config = {}
 
     model_config = {}
     model_config['Precision'] = default_precision
     model_config['ReuseFactor'] = default_reuse_factor
     model_config['Strategy'] = 'Latency'
 
     config['Model'] = model_config
-    
-    return config
+
+    return config
```

### Comparing `hls4ml-0.6.0/hls4ml/utils/example_models.py` & `hls4ml-0.7.0rc1/hls4ml/utils/example_models.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,169 +1,192 @@
-from urllib.request import urlretrieve
-from .config import create_config
-import pprint
 import json
+import pprint
+from urllib.request import urlretrieve
+
 import yaml
 
+from .config import create_config
+
+
 def _load_data_config_avai(model_name):
     """
     Check data and configuration availability for each model from this file:
 
     https://github.com/hls-fpga-machine-learning/example-models/blob/master/available_data_config.json
     """
 
-    link_to_list = 'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/available_data_config.json'
-    
+    link_to_list = (
+        'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/available_data_config.json'
+    )
+
     temp_file, _ = urlretrieve(link_to_list)
-    
+
     # Read data from file:
     data = json.load(open(temp_file))
 
     return data[model_name]
 
+
 def _data_is_available(model_name):
 
     data = _load_data_config_avai(model_name)
 
     return data['example_data']
 
+
 def _config_is_available(model_name):
 
     data = _load_data_config_avai(model_name)
 
     return data['example_config']
 
+
 def _create_default_config(model_name, model_config, backend):
 
-    #Initiate the configuration file
+    # Initiate the configuration file
     config = create_config(backend=backend)
 
-    #Additional configuration parameters
+    # Additional configuration parameters
     config[model_config] = model_name
     config['HLSConfig']['Model'] = {}
     config['HLSConfig']['Model']['Precision'] = 'ap_fixed<16,6>'
-    config['HLSConfig']['Model']['ReuseFactor'] = '1'
+    config['HLSConfig']['Model']['ReuseFactor'] = 1
 
     return config
 
+
 def _filter_name(model_name):
     """
     Need to get "garnet_1layer" from "garnet_1layer.json" for loading of data and configuration files
     """
     filtered_name = None
 
     if model_name.endswith('.json') or model_name.endswith('.onnx'):
         filtered_name = model_name[:-5]
     elif model_name.endswith('.pt') or model_name.endswith('.pb'):
         filtered_name = model_name[:-3]
 
     return filtered_name
 
+
 def _load_example_data(model_name):
 
     print("Downloading input & output example files ...")
 
     filtered_name = _filter_name(model_name)
 
     input_file_name = filtered_name + "_input.dat"
     output_file_name = filtered_name + "_output.dat"
 
-    link_to_input = 'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/data/' + input_file_name
-    link_to_output = 'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/data/' + output_file_name
+    link_to_input = (
+        'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/data/' + input_file_name
+    )
+    link_to_output = (
+        'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/data/' + output_file_name
+    )
 
     urlretrieve(link_to_input, input_file_name)
     urlretrieve(link_to_output, output_file_name)
 
+
 def _load_example_config(model_name):
 
     print("Downloading configuration files ...")
 
     filtered_name = _filter_name(model_name)
 
-    config_name =  filtered_name + "_config.yml"
+    config_name = filtered_name + "_config.yml"
 
-    link_to_config = 'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/config-files/' + config_name
+    link_to_config = (
+        'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/config-files/' + config_name
+    )
 
-    #Load the configuration as dictionary from file
+    # Load the configuration as dictionary from file
     urlretrieve(link_to_config, config_name)
 
-    #Load the configuration from local yml file
-    with open(config_name, 'r') as ymlfile:
+    # Load the configuration from local yml file
+    with open(config_name) as ymlfile:
         config = yaml.safe_load(ymlfile)
 
     return config
 
+
 def fetch_example_model(model_name, backend='Vivado'):
     """
-    Download an example model (and example data & configuration if available) from github repo to working directory, and return the corresponding configuration:
+    Download an example model (and example data & configuration if available) from github repo to working directory,
+    and return the corresponding configuration:
 
     https://github.com/hls-fpga-machine-learning/example-models
 
     Use fetch_example_list() to see all the available models.
 
     Args:
-        - model_name: string, name of the example model in the repo. Example: fetch_example_model('KERAS_3layer.json')
-    
+        model_name (str): Name of the example model in the repo. Example: fetch_example_model('KERAS_3layer.json')
+        backend (str, optional): Name of the backend to use for model conversion.
+
     Return:
-        - Dictionary that stores the configuration to the model
+        dict: Dictionary that stores the configuration to the model
     """
 
-    #Initilize the download link and model type
+    # Initilize the download link and model type
     download_link = 'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/'
     model_type = None
     model_config = None
 
-    #Check for model's type to update link
+    # Check for model's type to update link
     if '.json' in model_name:
         model_type = 'keras'
         model_config = 'KerasJson'
     elif '.pt' in model_name:
         model_type = 'pytorch'
         model_config = 'PytorchModel'
     elif '.onnx' in model_name:
         model_type = 'onnx'
-        model_config ='OnnxModel'
+        model_config = 'OnnxModel'
     elif '.pb' in model_name:
         model_type = 'tensorflow'
         model_config = 'TensorFlowModel'
     else:
         raise TypeError('Model type is not supported in hls4ml.')
-    
 
     download_link_model = download_link + model_type + '/' + model_name
 
-    #Download the example model
+    # Download the example model
     print("Downloading example model files ...")
-    urlretrieve(download_link_model, model_name)
+    urlretrieve(
+        download_link_model,
+        model_name,
+    )
 
-    #Check if the example data and configuration for the model are available
+    # Check if the example data and configuration for the model are available
     if _data_is_available(model_name):
         _load_example_data(model_name)
 
     if _config_is_available(model_name):
         config = _load_example_config(model_name)
     else:
         config = _create_default_config(model_name, model_config, backend)
 
-    #If the model is a keras model then have to download its weight file as well
+    # If the model is a keras model then have to download its weight file as well
     if model_type == 'keras':
         model_weight_name = model_name[:-5] + "_weights.h5"
 
         download_link_weight = download_link + model_type + '/' + model_weight_name
         urlretrieve(download_link_weight, model_weight_name)
 
-        config['KerasH5'] =  model_weight_name #Set configuration for the weight file
-    
+        config['KerasH5'] = model_weight_name  # Set configuration for the weight file
+
     return config
 
+
 def fetch_example_list():
-    
+
     link_to_list = 'https://raw.githubusercontent.com/hls-fpga-machine-learning/example-models/master/available_models.json'
-    
+
     temp_file, _ = urlretrieve(link_to_list)
-    
+
     # Read data from file:
     data = json.load(open(temp_file))
-    
+
     # Print in fancy format
     pp = pprint.PrettyPrinter(indent=4)
     pp.pprint(data)
```

### Comparing `hls4ml-0.6.0/hls4ml/utils/plot.py` & `hls4ml-0.7.0rc1/hls4ml/utils/plot.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,9 @@
 # Heavily inspired by Keras's plot_model
 """Utilities related to model visualization."""
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
 
 import os
 import sys
 
 try:
     import pydot
 except ImportError:
@@ -27,21 +24,17 @@
 
 
 def add_edge(dot, src, dst):
     if not dot.get_edge(src, dst):
         dot.add_edge(pydot.Edge(src, dst))
 
 
-def model_to_dot(model,
-                 show_shapes=False,
-                 show_layer_names=True,
-                 show_precision=False,
-                 rankdir='TB',
-                 dpi=96,
-                 subgraph=False):
+def model_to_dot(
+    model, show_shapes=False, show_layer_names=True, show_precision=False, rankdir='TB', dpi=96, subgraph=False
+):
     """Convert a HLS model to dot format.
 
     Arguments:
         model: A HLS model instance.
         show_shapes: whether to display shape information.
         show_layer_names: whether to display layer names.
         show_precision: whether to display precision of layer's variables.
@@ -61,20 +54,18 @@
         ImportError: if graphviz or pydot are not available.
     """
 
     if not check_pydot():
         if 'IPython.core.magics.namespace' in sys.modules:
             # We don't raise an exception here in order to avoid crashing notebook
             # tests where graphviz is not available.
-            print('Failed to import pydot. You must install pydot'
-                  ' and graphviz for `pydotprint` to work.')
+            print('Failed to import pydot. You must install pydot' ' and graphviz for `pydotprint` to work.')
             return
         else:
-            raise ImportError('Failed to import pydot. You must install pydot'
-                            ' and graphviz for `pydotprint` to work.')
+            raise ImportError('Failed to import pydot. You must install pydot' ' and graphviz for `pydotprint` to work.')
 
     if subgraph:
         dot = pydot.Cluster(style='dashed', graph_name=model.name)
         dot.set('label', model.name)
         dot.set('labeljust', 'l')
     else:
         dot = pydot.Dot()
@@ -83,35 +74,36 @@
         dot.set('dpi', dpi)
         dot.set_node_defaults(shape='record')
 
     layers = model.get_layers()
 
     # Create graph nodes.
     for i, layer in enumerate(layers):
-        #layer_id = str(id(layer))
+        # layer_id = str(id(layer))
         layer_id = str(layer.index)
 
         # Append a wrapped layer's label to node's label, if it exists.
         layer_name = layer.name
-        class_name = layer.__class__.__name__
+        class_name = layer.class_name
 
         # Create node's label.
         if show_layer_names:
-            #label = '{}: {}'.format(class_name, layer_name)
-            #label = '{}\\l{}\\l'.format(class_name, layer_name)
-            label = '<b>{}</b><br align="left" />{}'.format(class_name, layer_name)
+            # label = '{}: {}'.format(class_name, layer_name)
+            # label = '{}\\l{}\\l'.format(class_name, layer_name)
+            label = f'<b>{class_name}</b><br align="left" />{layer_name}'
         else:
             label = class_name
 
         # Rebuild the label as a table including input/output shapes.
         if show_shapes:
+
             def format_shape(shape):
                 return str(tuple(shape)).replace(str(None), '?')
-            
-            input_labels = '?' 
+
+            input_labels = '?'
             try:
                 output_labels = format_shape(layer.get_output_variable().shape)
             except AttributeError:
                 output_labels = '?'
             if class_name != 'Input':
                 if len(layer.inputs) > 1:
                     input_shapes = []
@@ -123,113 +115,110 @@
                             input_shapes.append('?')
                     formatted_shapes = [format_shape(ishape) for ishape in input_shapes]
                     input_labels = ', '.join(formatted_shapes)
                 else:
                     input_layer = layer.get_input_variable()
                     if input_layer is not None:
                         input_labels = format_shape(input_layer.shape)
-            label = '%s\n|{input: %s|output: %s}' % (label,
-                                                     input_labels,
-                                                     output_labels)
+            label = f'{label}\n|{{input: {input_labels}|output: {output_labels}}}'
 
         # Rebuild the label as a table including tensor precision.
         if show_precision:
+
             def format_precision(precision):
                 return str(precision).replace('<', '&lt;').replace('>', '&gt;')
 
             precision_labels = []
             tensors = {}
             tensors.update(layer.weights)
             if len(layer.variables) == 1:
                 # A bit cleaner output
                 tensors['output'] = layer.get_output_variable()
             else:
                 tensors.update(layer.variables)
             for tensor_name, var in tensors.items():
                 if show_shapes:
-                    #tensor_label = '{} {}: {}'.format(tensor_name,
+                    # tensor_label = '{} {}: {}'.format(tensor_name,
                     tensor_label = '<tr><td align="left">{} {}:</td><td align="left">{}</td></tr>'.format(
-                        tensor_name,
-                        format_shape(var.shape),
-                        format_precision(var.type.precision))
+                        tensor_name, format_shape(var.shape), format_precision(var.type.precision)
+                    )
                 else:
-                    #tensor_label = '{}: {}'.format(tensor_name,
+                    # tensor_label = '{}: {}'.format(tensor_name,
                     tensor_label = '<tr><td align="left">{}:</td><td align="left">{}</td></tr>'.format(
-                        tensor_name,
-                        format_precision(var.type.precision))
+                        tensor_name, format_precision(var.type.precision)
+                    )
                 precision_labels.append(tensor_label)
-            #precision_label = '<br align="left" />'.join(precision_labels)
+            # precision_label = '<br align="left" />'.join(precision_labels)
             precision_label = ''.join(precision_labels)
             precision_label = '<table border="0" cellspacing="0">' + precision_label + '</table>'
-            label = '%s|{%s}' % (label, precision_label)
+            label = f'{label}|{{{precision_label}}}'
 
         label = '<' + label + '>'
         node = pydot.Node(layer_id, label=label)
         dot.add_node(node)
 
     # Connect nodes with edges.
     for layer in layers:
         layer_id = str(layer.index)
-        for i, input_name in enumerate(layer.inputs):
+        for input_name in layer.inputs:
             input_layer = layer.get_input_node(input_name)
             if input_layer is not None:
                 input_layer_id = str(input_layer.index)
                 add_edge(dot, input_layer_id, layer_id)
 
     return dot
 
 
-def plot_model(model,
-               to_file='model.png',
-               show_shapes=False,
-               show_layer_names=True,
-               show_precision=False,
-               rankdir='TB',
-               dpi=96):
+def plot_model(
+    model, to_file='model.png', show_shapes=False, show_layer_names=True, show_precision=False, rankdir='TB', dpi=96
+):
     """Converts a HLS model to dot format and save to a file.
-  
+
     Arguments:
         model: A HLS model instance
         to_file: File name of the plot image.
         show_shapes: whether to display shape information.
         show_layer_names: whether to display layer names.
         show_precision: whether to display precision of layer's variables.
         rankdir: `rankdir` argument passed to PyDot,
             a string specifying the format of the plot:
             'TB' creates a vertical plot;
             'LR' creates a horizontal plot.
         dpi: Dots per inch.
-  
+
     Returns:
         A Jupyter notebook Image object if Jupyter is installed.
         This enables in-line display of the model plots in notebooks.
     """
-    dot = model_to_dot(model,
-                       show_shapes=show_shapes,
-                       show_layer_names=show_layer_names,
-                       show_precision=show_precision,
-                       rankdir=rankdir,
-                       dpi=dpi)
+    dot = model_to_dot(
+        model,
+        show_shapes=show_shapes,
+        show_layer_names=show_layer_names,
+        show_precision=show_precision,
+        rankdir=rankdir,
+        dpi=dpi,
+    )
     if dot is None:
         return
-    
+
     if to_file is not None:
         _, extension = os.path.splitext(to_file)
         if not extension:
             extension = 'png'
         else:
             extension = extension[1:]
         # Save image to disk.
         dot.write(to_file, format=extension)
     else:
         # Return the image as a Jupyter Image object, to be displayed in-line.
         # Note that we cannot easily detect whether the code is running in a
         # notebook, and thus we always return the Image if Jupyter is available.
         try:
-            from IPython import display
             import tempfile
 
+            from IPython import display
+
             temp = tempfile.NamedTemporaryFile(suffix='.png')
             dot.write(temp.name, format='png')
             return display.Image(filename=temp.name)
         except ImportError:
             pass
```

### Comparing `hls4ml-0.6.0/hls4ml/writer/vivado_accelerator_writer.py` & `hls4ml-0.7.0rc1/hls4ml/writer/vivado_accelerator_writer.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,131 +1,161 @@
 import os
+from distutils.dir_util import copy_tree
 from shutil import copyfile
 
-from hls4ml.templates.vivado_accelerator_config import VivadoAcceleratorConfig
 from hls4ml.writer.vivado_writer import VivadoWriter
 
-class VivadoAcceleratorWriter(VivadoWriter):
 
+class VivadoAcceleratorWriter(VivadoWriter):
     def __init__(self):
         super().__init__()
         self.vivado_accelerator_config = None
 
     def write_axi_wrapper(self, model):
-        ''' Write a top level HLS C++ file to wrap the hls4ml project with AXI interfaces
-            Args:
-                model : The HLSModel to write the wrapper for
+        '''Write a top level HLS C++ file to wrap the hls4ml project with AXI interfaces
+        Args:
+            model : The ModelGraph to write the wrapper for
         '''
         inp_axi_t, out_axi_t, inp, out = self.vivado_accelerator_config.get_corrected_types()
         indent = '    '
 
         #######################
-        ## myproject_axi.h
+        # myproject_axi.h
         #######################
 
         filedir = os.path.dirname(os.path.abspath(__file__))
-        f = open(os.path.join(filedir, '../templates/vivado_accelerator/myproject_axi.h'), 'r')
-        fout = open('{}/firmware/{}_axi.h'.format(model.config.get_output_dir(), model.config.get_project_name()), 'w')
+        f = open(os.path.join(filedir, '../templates/vivado_accelerator/myproject_axi.h'))
+        fout = open(f'{model.config.get_output_dir()}/firmware/{model.config.get_project_name()}_axi.h', 'w')
 
         for line in f.readlines():
             if 'MYPROJECT' in line:
                 newline = line.replace('MYPROJECT', format(model.config.get_project_name().upper()))
-            elif '//hls-fpga-machine-learning insert include' in line:
-                newline = '#include "{}.h"\n'.format(model.config.get_project_name())
-            elif 'void myproject(' in line:
-                newline = 'void {}_axi(\n'.format(model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert definitions' in line:
+            elif '// hls-fpga-machine-learning insert include' in line:
+                newline = f'#include "{model.config.get_project_name()}.h"\n'
+            elif 'myproject' in line:
+                newline = line.replace('myproject', model.config.get_project_name())
+            elif '// hls-fpga-machine-learning insert definitions' in line:
                 newline = ''
-                newline += 'static const unsigned N_IN = {};\n'.format(inp.size())
-                newline += 'static const unsigned N_OUT = {};\n'.format(out.size())
+                newline += f'static const unsigned N_IN = {inp.size()};\n'
+                newline += f'static const unsigned N_OUT = {out.size()};\n'
                 if self.vivado_accelerator_config.get_interface() == 'axi_stream':
-                    newline += 'typedef {} T_in;\n'.format(inp_axi_t)
-                    newline += 'typedef {} T_out;\n'.format(out_axi_t)
-                    newline += 'typedef struct in_struct {\n' + \
-                               indent + 'T_in data;\n' + \
-                               indent + 'ap_uint<1> last;\n' + \
-                               indent + 'in_struct(const T_in& data, const ap_uint<1>& last){this->data = data; this->last = last;};\n' + \
-                               indent + 'in_struct(){this->data = 0; this->last = 0;};\n' + \
-                               indent + 'friend std::ostream& operator<<(std::ostream& stream, const in_struct& in)\n' + \
-                               indent + '{ return stream << "{ data: " << in.data << ", last: " << in.last << " }" << std::endl; }\n' + \
-                               indent + 'operator float() const {return this->data;}\n' + \
-                               indent + 'operator double() const {return this->data;}\n' + \
-                               indent + 'in_struct(float data) {this->data = data; this->last = 0;}\n' + \
-                               indent + 'in_struct(double data) {this->data = data; this->last = 0;}\n' + \
-                               '} input_axi_t;\n'
-                    newline += 'typedef struct out_struct {\n' + \
-                               indent + 'T_out data;\n' + \
-                               indent + 'ap_uint<1> last;\n' + \
-                               indent + 'out_struct(const T_out& data, const ap_uint<1>& last){this->data = data; this->last = last;};\n' + \
-                               indent + 'out_struct(){this->data = 0; this->last = 0;};\n' + \
-                               indent + 'friend std::ostream& operator<<(std::ostream& stream, const out_struct& out)\n' + \
-                               indent + '{ return stream << "{ data: " << out.data << ", last: " << out.last << " }" << std::endl; }\n' + \
-                               indent + 'operator float() const {return this->data;}\n' + \
-                               indent + 'operator double() const {return this->data;}\n' + \
-                               indent + 'out_struct(float data) {this->data = data; this->last = 0;}\n' + \
-                               indent + 'out_struct(double data) {this->data = data; this->last = 0;}\n' + \
-                               '} output_axi_t;\n'
+                    newline += f'typedef {inp_axi_t} T_in;\n'
+                    newline += f'typedef {out_axi_t} T_out;\n'
+                    newline += (
+                        'typedef struct in_struct {\n'
+                        + indent
+                        + 'T_in data;\n'
+                        + indent
+                        + 'ap_uint<1> last;\n'
+                        + indent
+                        + 'in_struct(const T_in& data, const ap_uint<1>& last){this->data = data; this->last = last;};\n'
+                        + indent
+                        + 'in_struct(){this->data = 0; this->last = 0;};\n'
+                        + indent
+                        + 'friend std::ostream& operator<<(std::ostream& stream, const in_struct& in)\n'
+                        + indent
+                        + '{ return stream << "{ data: " << in.data << ", last: " << in.last << " }" << std::endl; }\n'
+                        + indent
+                        + 'operator float() const {return this->data;}\n'
+                        + indent
+                        + 'operator double() const {return this->data;}\n'
+                        + indent
+                        + 'in_struct(float data) {this->data = data; this->last = 0;}\n'
+                        + indent
+                        + 'in_struct(double data) {this->data = data; this->last = 0;}\n'
+                        + '} input_axi_t;\n'
+                    )
+                    newline += (
+                        'typedef struct out_struct {\n'
+                        + indent
+                        + 'T_out data;\n'
+                        + indent
+                        + 'ap_uint<1> last;\n'
+                        + indent
+                        + 'out_struct(const T_out& data, const ap_uint<1>& last){this->data = data; this->last = last;};\n'
+                        + indent
+                        + 'out_struct(){this->data = 0; this->last = 0;};\n'
+                        + indent
+                        + 'friend std::ostream& operator<<(std::ostream& stream, const out_struct& out)\n'
+                        + indent
+                        + '{ return stream << "{ data: " << out.data << ", last: " << out.last << " }" << std::endl; }\n'
+                        + indent
+                        + 'operator float() const {return this->data;}\n'
+                        + indent
+                        + 'operator double() const {return this->data;}\n'
+                        + indent
+                        + 'out_struct(float data) {this->data = data; this->last = 0;}\n'
+                        + indent
+                        + 'out_struct(double data) {this->data = data; this->last = 0;}\n'
+                        + '} output_axi_t;\n'
+                    )
                 else:
-                    newline += 'typedef {} input_axi_t;\n'.format(inp_axi_t)
-                    newline += 'typedef {} output_axi_t;\n'.format(out_axi_t)
+                    newline += f'typedef {inp_axi_t} input_axi_t;\n'
+                    newline += f'typedef {out_axi_t} output_axi_t;\n'
             else:
                 newline = line
             fout.write(newline)
         f.close()
         fout.close()
 
         #######################
-        ## myproject_axi.cpp
+        # myproject_axi.cpp
         #######################
 
-        f = open(os.path.join(filedir, '../templates/vivado_accelerator/myproject_axi.cpp'), 'r')
-        fout = open('{}/firmware/{}_axi.cpp'.format(model.config.get_output_dir(), model.config.get_project_name()),
-                    'w')
+        f = open(os.path.join(filedir, '../templates/vivado_accelerator/myproject_axi.cpp'))
+        fout = open(f'{model.config.get_output_dir()}/firmware/{model.config.get_project_name()}_axi.cpp', 'w')
 
         io_type = model.config.get_config_value("IOType")
 
         for line in f.readlines():
-            if 'void myproject(' in line:
-                newline = 'void {}_axi(\n'.format(model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert include' in line:
-                newline = '#include "{}_axi.h"\n'.format(model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert local vars' in line:
+            if 'myproject' in line:
+                newline = line.replace('myproject', model.config.get_project_name())
+            elif '// hls-fpga-machine-learning insert include' in line:
+                newline = f'#include "{model.config.get_project_name()}_axi.h"\n'
+            elif '// hls-fpga-machine-learning insert local vars' in line:
                 newline = ''
                 if self.vivado_accelerator_config.get_interface() == 'axi_stream':
                     newline += indent + 'bool is_last = false;\n'
                 if io_type == 'io_parallel':
                     newline += indent + inp.type.name + ' in_local[N_IN];\n'
                     newline += indent + out.type.name + ' out_local[N_OUT];\n'
                 elif io_type == 'io_stream':
                     newline += indent + 'hls::stream<' + inp.type.name + '> in_local("input_1");\n'
                     newline += indent + 'hls::stream<' + out.type.name + '> out_local("output_1");\n\n'
-                    newline += indent + '#pragma HLS STREAM variable=in_local depth=N_IN\n'
-                    newline += indent + '#pragma HLS STREAM variable=out_local depth=N_OUT\n'
-            elif '//hls-fpga-machine-learning insert call' in line:
-                newline = indent + '{}(in_local, out_local, in_size, out_size);\n'.format(
-                    model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert interface' in line:
+                    newline += indent + '#pragma HLS STREAM variable=in_local depth={}\n'.format(
+                        model.get_input_variables()[0].pragma[1]
+                    )
+                    newline += indent + '#pragma HLS STREAM variable=out_local depth={}\n'.format(
+                        model.get_output_variables()[0].pragma[1]
+                    )
+            elif '// hls-fpga-machine-learning insert call' in line:
+                newline = indent + f'{model.config.get_project_name()}(in_local, out_local);\n'
+            elif '// hls-fpga-machine-learning insert interface' in line:
                 if self.vivado_accelerator_config.get_interface() == 'axi_lite':
                     newline = ''
                     newline += indent + '#pragma HLS INTERFACE ap_ctrl_none port=return\n'
                     newline += indent + '#pragma HLS INTERFACE s_axilite port=in\n'
                     newline += indent + '#pragma HLS INTERFACE s_axilite port=out\n'
                 elif self.vivado_accelerator_config.get_interface() == 'axi_master':
                     newline = ''
                     newline += indent + '#pragma HLS INTERFACE s_axilite port=return bundle=CTRL_BUS\n'
-                    newline += indent + '#pragma HLS INTERFACE m_axi depth=N_IN port=in offset=slave bundle=IN_BUS\n'
-                    newline += indent + '#pragma HLS INTERFACE m_axi depth=N_OUT port=out offset=slave bundle=OUT_BUS\n'
+                    newline += indent + '#pragma HLS INTERFACE m_axi depth={} port=in offset=slave bundle=IN_BUS\n'.format(
+                        model.get_input_variables()[0].pragma[1]
+                    )
+                    newline += indent + '#pragma HLS INTERFACE m_axi depth={} port=out offset=slave bundle=OUT_BUS\n'.format(
+                        model.get_output_variables()[0].pragma[1]
+                    )
                 elif self.vivado_accelerator_config.get_interface() == 'axi_stream':
                     newline = ''
                     newline += indent + '#pragma HLS INTERFACE axis port=in\n'
                     newline += indent + '#pragma HLS INTERFACE axis port=out\n'
                     newline += indent + '#pragma HLS INTERFACE ap_ctrl_none port=return\n'
                     if model.config.get_config_value("IOType") == 'io_stream':
                         newline += indent + '#pragma HLS DATAFLOW\n'
-            elif '//hls-fpga-machine-learning insert enqueue' in line:
+            elif '// hls-fpga-machine-learning insert enqueue' in line:
                 io_type = model.config.get_config_value("IOType")
                 if io_type == 'io_parallel':
                     newline = ''
                     newline += indent + 'for(unsigned i = 0; i < N_IN; i++){\n'
                     if self.vivado_accelerator_config.get_interface() == 'axi_stream':
                         newline += indent + indent + '#pragma HLS PIPELINE\n'
                         newline += indent + indent + 'in_local[i] = in[i].data; // Read input with cast\n'
@@ -139,23 +169,35 @@
                     newline += indent + 'for(unsigned i = 0; i < N_IN / {input_t}::size; ++i) {{\n'
                     # newline += indent + indent + '#pragma HLS PIPELINE\n'
                     newline += indent + indent + '{input_t} ctype;\n'
                     newline += indent + indent + '#pragma HLS DATA_PACK variable=ctype\n'
                     newline += indent + indent + 'for(unsigned j = 0; j < {input_t}::size; j++) {{\n'
                     # newline += indent + indent + indent + '#pragma HLS UNROLL\n'
                     if self.vivado_accelerator_config.get_interface() == 'axi_stream':
-                        newline += indent + indent + indent + 'ctype[j] = typename {input_t}::value_type(in[i * {input_t}::size + j].data);\n'
-                        newline += indent + indent + indent + 'is_last |= (in[i * input_t::size + j].last == 1)? true : false;\n'
+                        newline += (
+                            indent
+                            + indent
+                            + indent
+                            + 'ctype[j] = typename {input_t}::value_type(in[i * {input_t}::size + j].data);\n'
+                        )
+                        newline += (
+                            indent + indent + indent + 'is_last |= (in[i * input_t::size + j].last == 1)? true : false;\n'
+                        )
                     else:
-                        newline += indent + indent + indent + 'ctype[j] = typename {input_t}::value_type(in[i * {input_t}::size + j]);\n'
+                        newline += (
+                            indent
+                            + indent
+                            + indent
+                            + 'ctype[j] = typename {input_t}::value_type(in[i * {input_t}::size + j]);\n'
+                        )
                     newline += indent + indent + '}}\n'
                     newline += indent + indent + 'in_local.write(ctype);\n'
                     newline += indent + '}}\n'
                     newline = newline.format(input_t=inp.type.name)
-            elif '//hls-fpga-machine-learning insert dequeue' in line:
+            elif '// hls-fpga-machine-learning insert dequeue' in line:
                 io_type = model.config.get_config_value("IOType")
                 if io_type == 'io_parallel':
                     newline = ''
                     newline += indent + 'for(unsigned i = 0; i < N_OUT; i++){\n'
                     if self.vivado_accelerator_config.get_interface() == 'axi_stream':
                         newline += indent + indent + '#pragma HLS PIPELINE\n'
                         newline += indent + indent + 'out[i].data = out_local[i]; // Write output with cast\n'
@@ -168,16 +210,23 @@
                     newline = ''
                     newline += indent + 'for(unsigned i = 0; i < N_OUT / {result_t}::size; ++i) {{\n'
                     # newline += indent + indent + '#pragma HLS PIPELINE\n'
                     newline += indent + indent + '{result_t} ctype = out_local.read();\n'
                     newline += indent + indent + 'for(unsigned j = 0; j < {result_t}::size; j++) {{\n'
                     # newline += indent + indent + indent + '#pragma HLS UNROLL\n'
                     if self.vivado_accelerator_config.get_interface() == 'axi_stream':
-                        newline += indent + indent + indent + 'bool last = (is_last && (i * {result_t}::size + j == N_OUT - 1)) ? true : false;\n'
-                        newline += indent + indent + indent + 'out[i * {result_t}::size + j] = output_axi_t(ctype[j], last);\n'
+                        newline += (
+                            indent
+                            + indent
+                            + indent
+                            + 'bool last = (is_last && (i * {result_t}::size + j == N_OUT - 1)) ? true : false;\n'
+                        )
+                        newline += (
+                            indent + indent + indent + 'out[i * {result_t}::size + j] = output_axi_t(ctype[j], last);\n'
+                        )
                     else:
                         newline += indent + indent + indent + 'out[i * {result_t}::size + j] = output_axi_t(ctype[j]);\n'
                     newline += indent + indent + '}}\n'
                     newline += indent + '}}\n'
                     newline = newline.format(result_t=out.type.name)
             else:
                 newline = line
@@ -186,82 +235,89 @@
         fout.close()
 
     def modify_build_script(self, model):
         '''
         Modify the build_prj.tcl and build_lib.sh scripts to add the extra wrapper files and set the top function
         '''
         filedir = os.path.dirname(os.path.abspath(__file__))
-        oldfile = '{}/build_prj.tcl'.format(model.config.get_output_dir())
-        newfile = '{}/build_prj_axi.tcl'.format(model.config.get_output_dir())
-        f = open(oldfile, 'r')
+        oldfile = f'{model.config.get_output_dir()}/build_prj.tcl'
+        newfile = f'{model.config.get_output_dir()}/build_prj_axi.tcl'
+        f = open(oldfile)
         fout = open(newfile, 'w')
 
         for line in f.readlines():
             if 'set_top' in line:
                 newline = line[:-1] + '_axi\n'  # remove the newline from the line end and append _axi for the new top
-                newline += 'add_files firmware/{}_axi.cpp -cflags "-std=c++0x"\n'.format(
-                    model.config.get_project_name())
-            elif 'myproject_cosim' in line:
-                newline = line.replace('myproject_cosim', 'myproject_axi_cosim')
+                newline += f'add_files firmware/{model.config.get_project_name()}_axi.cpp -cflags "-std=c++0x"\n'
+            elif f'{model.config.get_project_name()}_cosim' in line:
+                newline = line.replace(
+                    f'{model.config.get_project_name()}_cosim',
+                    f'{model.config.get_project_name()}_axi_cosim',
+                )
+            elif '${project_name}.tcl' in line:
+                newline = line.replace('${project_name}.tcl', '${project_name}_axi.tcl')
             else:
                 newline = line
             fout.write(newline)
 
         f.close()
         fout.close()
         os.rename(newfile, oldfile)
 
         ###################
         # build_lib.sh
         ###################
 
-        f = open(os.path.join(filedir, '../templates/vivado_accelerator/build_lib.sh'), 'r')
-        fout = open('{}/build_lib.sh'.format(model.config.get_output_dir()), 'w')
+        f = open(os.path.join(filedir, '../templates/vivado_accelerator/build_lib.sh'))
+        fout = open(f'{model.config.get_output_dir()}/build_lib.sh', 'w')
 
         for line in f.readlines():
             line = line.replace('myproject', model.config.get_project_name())
             line = line.replace('mystamp', model.config.get_config_value('Stamp'))
 
             fout.write(line)
         f.close()
         fout.close()
 
     def write_wrapper_test(self, model):
 
         ###################
         # write myproject_test_wrapper.cpp
         ###################
-        oldfile = '{}/{}_test.cpp'.format(model.config.get_output_dir(), model.config.get_project_name())
-        newfile = '{}/{}_test_wrapper.cpp'.format(model.config.get_output_dir(), model.config.get_project_name())
+        oldfile = f'{model.config.get_output_dir()}/{model.config.get_project_name()}_test.cpp'
+        newfile = f'{model.config.get_output_dir()}/{model.config.get_project_name()}_test_wrapper.cpp'
 
-        f = open(oldfile, 'r')
+        f = open(oldfile)
         fout = open(newfile, 'w')
 
         inp = model.get_input_variables()[0]
         out = model.get_output_variables()[0]
 
         for line in f.readlines():
-            if '{}.h'.format(model.config.get_project_name()) in line:
-                newline = line.replace('{}.h'.format(model.config.get_project_name()),
-                                       '{}_axi.h'.format(model.config.get_project_name()))
-            elif self.variable_definition_cpp(model, inp) in line:
-                newline = line.replace(self.variable_definition_cpp(model, inp), 'input_axi_t inputs[N_IN]')
-            elif self.variable_definition_cpp(model, out) in line:
-                newline = line.replace(self.variable_definition_cpp(model, out), 'output_axi_t outputs[N_OUT]')
+            if f'{model.config.get_project_name()}.h' in line:
+                newline = line.replace(f'{model.config.get_project_name()}.h', f'{model.config.get_project_name()}_axi.h')
+            elif inp.definition_cpp() in line:
+                newline = line.replace(
+                    inp.definition_cpp(), 'input_axi_t inputs[N_IN]'
+                )  # TODO instead of replacing strings, how about we use proper variables and their definition?
+            elif out.definition_cpp() in line:
+                newline = line.replace(out.definition_cpp(), 'output_axi_t outputs[N_OUT]')
             elif 'unsigned short' in line:
                 newline = ''
-            elif '{}('.format(model.config.get_project_name()) in line:
+            elif f'{model.config.get_project_name()}(' in line:
                 indent_amount = line.split(model.config.get_project_name())[0]
-                newline = indent_amount + '{}_axi(inputs,outputs);\n'.format(model.config.get_project_name())
-            elif inp.size_cpp() in line or inp.cppname in line or inp.type.name in line:
-                newline = line.replace(inp.size_cpp(), 'N_IN').replace(inp.cppname, 'inputs').replace(inp.type.name,
-                                                                                                      'input_axi_t')
-            elif out.size_cpp() in line or out.cppname in line or out.type.name in line:
-                newline = line.replace(out.size_cpp(), 'N_OUT').replace(out.cppname, 'outputs').replace(out.type.name,
-                                                                                                        'output_axi_t')
+                newline = indent_amount + f'{model.config.get_project_name()}_axi(inputs,outputs);\n'
+            elif inp.size_cpp() in line or inp.name in line or inp.type.name in line:
+                newline = (
+                    line.replace(inp.size_cpp(), 'N_IN').replace(inp.name, 'inputs').replace(inp.type.name, 'input_axi_t')
+                )
+            elif out.size_cpp() in line or out.name in line or out.type.name in line:
+                newline = (
+                    line.replace(out.size_cpp(), 'N_OUT').replace(out.name, 'outputs').replace(out.type.name, 'output_axi_t')
+                )
             else:
                 newline = line
             if self.vivado_accelerator_config.get_interface() == 'axi_stream':
                 if 'nnet::fill_zero' in line:
                     indent = line.split('n')[0]
                     newline = indent + 'inputs[N_IN-1].last = 1;\n'
                 if 'copy_data' in line:
@@ -271,81 +327,103 @@
         f.close()
         fout.close()
         os.rename(newfile, oldfile)
 
         ###################
         # write myproject_bridge_wrapper.cpp
         ###################
-        oldfile = '{}/{}_bridge.cpp'.format(model.config.get_output_dir(), model.config.get_project_name())
-        newfile = '{}/{}_bridge_wrapper.cpp'.format(model.config.get_output_dir(), model.config.get_project_name())
+        oldfile = f'{model.config.get_output_dir()}/{model.config.get_project_name()}_bridge.cpp'
+        newfile = f'{model.config.get_output_dir()}/{model.config.get_project_name()}_bridge_wrapper.cpp'
 
-        f = open(oldfile, 'r')
+        f = open(oldfile)
         fout = open(newfile, 'w')
 
         inp = model.get_input_variables()[0]
         out = model.get_output_variables()[0]
 
         for line in f.readlines():
-            if '{}.h'.format(model.config.get_project_name()) in line:
-                newline = line.replace('{}.h'.format(model.config.get_project_name()),
-                                       '{}_axi.h'.format(model.config.get_project_name()))
-            elif self.variable_definition_cpp(model, inp, name_suffix='_ap') in line:
-                newline = line.replace(self.variable_definition_cpp(model, inp, name_suffix='_ap'),
-                                       'input_axi_t {}_ap[N_IN]'.format(inp.cppname))
-            elif self.variable_definition_cpp(model, out, name_suffix='_ap') in line:
-                newline = line.replace(self.variable_definition_cpp(model, out, name_suffix='_ap'),
-                                       'output_axi_t {}_ap[N_OUT]'.format(out.cppname))
-            elif '{}('.format(model.config.get_project_name()) in line:
+            if f'{model.config.get_project_name()}.h' in line:
+                newline = line.replace(f'{model.config.get_project_name()}.h', f'{model.config.get_project_name()}_axi.h')
+            elif inp.definition_cpp(name_suffix='_ap') in line:
+                newline = line.replace(inp.definition_cpp(name_suffix='_ap'), f'input_axi_t {inp.name}_ap[N_IN]')
+            elif out.definition_cpp(name_suffix='_ap') in line:
+                newline = line.replace(out.definition_cpp(name_suffix='_ap'), f'output_axi_t {out.name}_ap[N_OUT]')
+            elif f'{model.config.get_project_name()}(' in line:
                 indent_amount = line.split(model.config.get_project_name())[0]
-                newline = indent_amount + '{}_axi({}_ap,{}_ap);\n'.format(model.config.get_project_name(), inp.cppname,
-                                                                          out.cppname)
-            elif inp.size_cpp() in line or inp.cppname in line or inp.type.name in line:
+                newline = indent_amount + '{}_axi({}_ap,{}_ap);\n'.format(
+                    model.config.get_project_name(), inp.name, out.name
+                )
+            elif inp.size_cpp() in line or inp.name in line or inp.type.name in line:
                 newline = line.replace(inp.size_cpp(), 'N_IN').replace(inp.type.name, 'input_axi_t')
-            elif out.size_cpp() in line or out.cppname in line or out.type.name in line:
+            elif out.size_cpp() in line or out.name in line or out.type.name in line:
                 newline = line.replace(out.size_cpp(), 'N_OUT').replace(out.type.name, 'output_axi_t')
             else:
                 newline = line
             fout.write(newline)
 
         f.close()
         fout.close()
         os.rename(newfile, oldfile)
 
     def write_board_script(self, model):
         '''
-        Write the tcl scripts to create a Vivado IPI project for the VivadoAccelerator
+        Write the tcl scripts and kernel sources to create a Vivado IPI project for the VivadoAccelerator
         '''
         filedir = os.path.dirname(os.path.abspath(__file__))
-        copyfile(os.path.join(filedir, self.vivado_accelerator_config.get_tcl_file_path()),
-                 '{}/design.tcl'.format(model.config.get_output_dir()))
-        f = open('{}/project.tcl'.format(model.config.get_output_dir()), 'w')
-        f.write('variable myproject\n')
-        f.write('set myproject "{}"\n'.format(model.config.get_project_name()))
+        copyfile(
+            os.path.join(filedir, self.vivado_accelerator_config.get_tcl_file_path()),
+            f'{model.config.get_output_dir()}/design.tcl',
+        )
+        # Generic alveo board
+        if self.vivado_accelerator_config.get_board().startswith('alveo'):
+            src_dir = os.path.join(filedir, self.vivado_accelerator_config.get_krnl_rtl_src_dir())
+            dst_dir = os.path.abspath(model.config.get_output_dir()) + '/src'
+            copy_tree(src_dir, dst_dir)
+
+        ###################
+        # project.tcl
+        ###################
+        f = open(f'{model.config.get_output_dir()}/project.tcl', 'w')
+        f.write('variable project_name\n')
+        f.write(f'set project_name "{model.config.get_project_name()}"\n')
+        f.write('variable backend\n')
+        f.write('set backend "vivadoaccelerator"\n')
+        f.write('variable part\n')
+        f.write(f'set part "{self.vivado_accelerator_config.get_part()}"\n')
+        f.write('variable clock_period\n')
+        f.write('set clock_period {}\n'.format(model.config.get_config_value('ClockPeriod')))
+        f.write('variable clock_uncertainty\n')
+        f.write('set clock_uncertainty {}\n'.format(model.config.get_config_value('ClockUncertainty', '12.5%')))
         if self.vivado_accelerator_config.get_interface() == 'axi_stream':
             in_bit, out_bit = self.vivado_accelerator_config.get_io_bitwidth()
-            f.write('set bit_width_hls_output {}\n'.format(in_bit))
-            f.write('set bit_width_hls_input {}\n'.format(out_bit))
+            f.write(f'set bit_width_hls_output {in_bit}\n')
+            f.write(f'set bit_width_hls_input {out_bit}\n')
         f.close()
 
     def write_driver(self, model):
         filedir = os.path.dirname(os.path.abspath(__file__))
-        copyfile(os.path.join(filedir, self.vivado_accelerator_config.get_driver_path()),
-                 ('{}/' + self.vivado_accelerator_config.get_driver_file()).format(model.config.get_output_dir()))
-        
+        copyfile(
+            os.path.join(filedir, self.vivado_accelerator_config.get_driver_path()),
+            ('{}/' + self.vivado_accelerator_config.get_driver_file()).format(model.config.get_output_dir()),
+        )
+
     def write_new_tar(self, model):
         os.remove(model.config.get_output_dir() + '.tar.gz')
-        super(VivadoAcceleratorWriter, self).write_tar(model)
-        
+        super().write_tar(model)
+
     def write_hls(self, model):
         """
         Write the HLS project. Calls the VivadoBackend writer, and extra steps for VivadoAccelerator/AXI interface
         """
-        self.vivado_accelerator_config = VivadoAcceleratorConfig(model.config, model.get_input_variables(),
-                                                                 model.get_output_variables())
-        super(VivadoAcceleratorWriter, self).write_hls(model)
+        # TODO temporarily move config import here to avoid cyclic dependency, until config is moved to its own package
+        from hls4ml.backends import VivadoAcceleratorConfig
+
+        self.vivado_accelerator_config = VivadoAcceleratorConfig(
+            model.config, model.get_input_variables(), model.get_output_variables()
+        )
+        super().write_hls(model)
         self.write_board_script(model)
         self.write_driver(model)
         self.write_wrapper_test(model)
         self.write_axi_wrapper(model)
         self.modify_build_script(model)
         self.write_new_tar(model)
-
```

### Comparing `hls4ml-0.6.0/hls4ml/writer/vivado_writer.py` & `hls4ml-0.7.0rc1/hls4ml/writer/vivado_writer.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,116 +1,86 @@
-from __future__ import print_function
+import glob
+import os
 import tarfile
-import yaml
+from collections import OrderedDict
 from shutil import copyfile, copytree, rmtree
+
 import numpy as np
-import os
-import re
-import glob
-from collections import OrderedDict
+import yaml
 
 from hls4ml.writer.writers import Writer
-from hls4ml.model.hls_layers import XnorPrecisionType
 
 config_filename = 'hls4ml_config.yml'
 
-class VivadoWriter(Writer):
-
-    def type_definition_cpp(self, model, atype):
-        type_class = atype.__class__.__name__
-        if type_class == 'HLSType':
-            return 'typedef {precision} {name};\n'.format(name=atype.name, precision=atype.precision)
-        elif type_class == 'CompressedType':
-            cpp_fmt = ('typedef struct {name} {{ '
-               '{index} row_index;'
-               '{index} col_index;'
-               '{precision} weight; }} {name};\n')
-            return cpp_fmt.format(name=atype.name, index=atype.index_precision, precision=atype.precision)
-        elif type_class == 'PackedType':
-            n_elem_expr = '/' if atype.unpack else '*'
-            return 'typedef nnet::array<{precision}, {n_elem}> {name};\n'.format(name=atype.name, precision=atype.precision, n_elem=str(atype.n_elem) + n_elem_expr + str(atype.n_pack))
-        elif type_class == 'ExponentType':
-            cpp_fmt = ('typedef struct {name} {{ '
-                       '{sign} sign; '
-                       '{precision} weight; }} {name};\n')
-            return cpp_fmt.format(name=atype.name, precision=atype.precision, sign=str(XnorPrecisionType()))
-        else:
-            raise Exception('Unknown data type class "{}"'.format(type_class))
-
-    def variable_definition_cpp(self, model, var, name_suffix='', as_reference=False):
-        var_class = var.__class__.__name__
-        if var_class == 'ArrayVariable':
-            return '{type} {name}{suffix}[{shape}]'.format(type=var.type.name, name=var.cppname, suffix=name_suffix, shape=var.size_cpp())
-        elif var_class == 'StreamVariable':
-            if as_reference: # Function parameter
-                return 'hls::stream<{type}> &{name}{suffix}'.format(type=var.type.name, name=var.cppname, suffix=name_suffix)
-            else: # Declaration
-                return 'hls::stream<{type}> {name}{suffix}("{name}")'.format(type=var.type.name, name=var.cppname, suffix=name_suffix)
-        elif var_class == 'WeightVariable':
-            return '{type} {name}{suffix}[{size}]'.format(type=var.type.name, name=var.cppname, suffix=name_suffix, size=var.data_length)
-        elif var_class == 'InplaceVariable':
-            return None
-        else:
-            raise Exception('Unknown variable class "{}"'.format(var_class))
 
+class VivadoWriter(Writer):
     def print_array_to_cpp(self, var, odir, write_txt_file=True):
-        #######################################
-        ## Print weight array to C++
-        #######################################
+        """Write a weights array to C++ header files.
+
+        Args:
+            var (WeightVariable): Weight to write
+            odir (str): Output directory
+            write_txt_file (bool, optional): Write txt files in addition to .h files. Defaults to True.
+        """
 
-        h_file = open("{}/firmware/weights/{}.h".format(odir,var.name),"w")
+        h_file = open(f"{odir}/firmware/weights/{var.name}.h", "w")
         if write_txt_file:
-            txt_file = open("{}/firmware/weights/{}.txt".format(odir,var.name),"w")
+            txt_file = open(f"{odir}/firmware/weights/{var.name}.txt", "w")
 
-        #meta data
-        h_file.write("//Numpy array shape {}\n".format(var.shape))
-        h_file.write("//Min {:.12f}\n".format(np.min(var.min)))
-        h_file.write("//Max {:.12f}\n".format(np.max(var.max)))
-        h_file.write("//Number of zeros {}\n".format(var.nzeros))
+        # meta data
+        h_file.write(f"//Numpy array shape {var.shape}\n")
+        h_file.write(f"//Min {np.min(var.min):.12f}\n")
+        h_file.write(f"//Max {np.max(var.max):.12f}\n")
+        h_file.write(f"//Number of zeros {var.nzeros}\n")
         h_file.write("\n")
 
-        h_file.write("#ifndef {}_H_\n".format(var.name.upper()))
-        h_file.write("#define {}_H_\n".format(var.name.upper()))
+        h_file.write(f"#ifndef {var.name.upper()}_H_\n")
+        h_file.write(f"#define {var.name.upper()}_H_\n")
         h_file.write("\n")
 
         if write_txt_file:
             h_file.write("#ifndef __SYNTHESIS__\n")
             h_file.write(var.definition_cpp() + ";\n")
             h_file.write("#else\n")
 
         h_file.write(var.definition_cpp() + " = {")
 
-        #fill c++ array.
-        #not including internal brackets for multidimensional case
+        # fill c++ array.
+        # not including internal brackets for multidimensional case
         sep = ''
         for x in var:
             h_file.write(sep + x)
             if write_txt_file:
                 txt_file.write(sep + x)
             sep = ", "
         h_file.write("};\n")
         if write_txt_file:
             h_file.write("#endif\n")
             txt_file.close()
         h_file.write("\n#endif\n")
         h_file.close()
 
     def write_project_dir(self, model):
-        if not os.path.isdir("{}/firmware/weights".format(model.config.get_output_dir())):
-            os.makedirs("{}/firmware/weights".format(model.config.get_output_dir()))
+        """Write the base project directory
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
+        if not os.path.isdir(f"{model.config.get_output_dir()}/firmware/weights"):
+            os.makedirs(f"{model.config.get_output_dir()}/firmware/weights")
 
     @staticmethod
     def _make_array_pragma(variable):
         """
         Layers in hls_model.py can specify output array partitioning through the `pragma` attribute.
         If `pragma` is a string: options are 'partition', 'reshape', or 'stream'.
         If `pragma` is a tuple: (mode, type, factor) where mode is 'partition' or 'reshape', type is
         'complete', 'cyclic', or 'block', and factor is an integer only used when the type is not 'complete'.
         """
-        
+
         config = variable.pragma
         if type(config) is tuple:
             mode = config[0]
             if mode in ['partition', 'reshape']:
                 typ = config[1]
                 if typ != 'complete':
                     factor = config[2]
@@ -126,547 +96,621 @@
                 template = '#pragma HLS ARRAY_{mode} variable={name} {type} dim={dim}'
             else:
                 template = '#pragma HLS ARRAY_{mode} variable={name} {type} factor={factor} dim={dim}'
 
             return template.format(mode=mode.upper(), name=variable.name, type=typ, factor=factor, dim=0)
 
         elif mode == 'stream':
-            return '#pragma HLS STREAM variable={name} depth={depth}'.format(name=variable.name, depth=depth)
+            return f'#pragma HLS STREAM variable={variable.name} depth={depth}'
 
     def write_project_cpp(self, model):
-        ###################
-        ## myproject.cpp
-        ###################
+        """Write the main architecture source file (myproject.cpp)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
         filedir = os.path.dirname(os.path.abspath(__file__))
-        f = open(os.path.join(filedir,'../templates/vivado/firmware/myproject.cpp'),'r')
-        fout = open('{}/firmware/{}.cpp'.format(model.config.get_output_dir(), model.config.get_project_name()),'w')
+
+        f = open(os.path.join(filedir, '../templates/vivado/firmware/myproject.cpp'))
+        fout = open(f'{model.config.get_output_dir()}/firmware/{model.config.get_project_name()}.cpp', 'w')
 
         model_inputs = model.get_input_variables()
         model_outputs = model.get_output_variables()
-        model_brams   = model.get_bram_variables()
+        model_brams = [var for var in model.get_weight_variables() if var.storage.lower() == 'bram']
 
         indent = '    '
 
         for line in f.readlines():
-            #Add headers to weights and biases
+            # Add headers to weights and biases
             if 'myproject' in line:
                 newline = line.replace('myproject', model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert header' in line:
-                inputs_str = ', '.join([self.variable_definition_cpp(model, i, as_reference=True) for i in model_inputs])
-                outputs_str = ', '.join([self.variable_definition_cpp(model, o, as_reference=True) for o in model_outputs])
-                brams_str  = ', \n'.join([indent + self.variable_definition_cpp(model, b, as_reference=False) for b in model_brams])
-                insize_str = ', '.join(['unsigned short &const_size_in_{}'.format(i) for i in range(1, len(model_inputs) + 1)])
-                outsize_str = ', '.join(['unsigned short &const_size_out_{}'.format(i) for i in range(1, len(model_outputs) + 1)])
+            elif '// hls-fpga-machine-learning insert header' in line:
+                inputs_str = ', '.join([i.definition_cpp(as_reference=True) for i in model_inputs])
+                outputs_str = ', '.join([o.definition_cpp(as_reference=True) for o in model_outputs])
+                brams_str = ', \n'.join([indent + b.definition_cpp(as_reference=False) for b in model_brams])
 
                 newline = ''
                 newline += indent + inputs_str + ',\n'
-                newline += indent + outputs_str + ',\n'
-                if len(model_brams) > 0: 
-                    newline += brams_str + ',\n'
-                newline += indent + insize_str + ',\n'
-                newline += indent + outsize_str + '\n'
+                newline += indent + outputs_str
+                if len(model_brams) > 0:
+                    newline += ',\n' + brams_str
+                newline += '\n'
 
-            elif '//hls-fpga-machine-learning insert load weights' in line:
+            elif '// hls-fpga-machine-learning insert load weights' in line:
                 newline = line
                 for layer in model.get_layers():
                     for w in layer.get_weights():
-                        if w.__class__.__name__ == 'CompressedWeightVariable':
-                            newline += indent + '    nnet::load_compressed_weights_from_txt<{}, {}>({}, "{}.txt");\n'.format(w.type.name, w.nonzeros, w.name, w.name)
-                        elif w.__class__.__name__ == 'ExponentWeightVariable':
-                            newline += indent + '    nnet::load_exponent_weights_from_txt<{}, {}>({}, "{}.txt");\n'.format(w.type.name, w.data_length, w.name, w.name)
+                        if w.weight_class == 'CompressedWeightVariable':
+                            newline += indent + '    nnet::load_compressed_weights_from_txt<{}, {}>({}, "{}.txt");\n'.format(
+                                w.type.name, w.nonzeros, w.name, w.name
+                            )
+                        elif w.weight_class == 'ExponentWeightVariable':
+                            newline += indent + '    nnet::load_exponent_weights_from_txt<{}, {}>({}, "{}.txt");\n'.format(
+                                w.type.name, w.data_length, w.name, w.name
+                            )
                         else:
-                            newline += indent + '    nnet::load_weights_from_txt<{}, {}>({}, "{}.txt");\n'.format(w.type.name, w.data_length, w.name, w.name)
-
-            #Add input/output type
-            elif '//hls-fpga-machine-learning insert IO' in line:
-                newline = line
-                all_inputs = [i.cppname for i in model_inputs]
-                all_outputs = [o.cppname for o in model_outputs]
-                all_brams = [b.cppname for b in model_brams] 
+                            newline += indent + '    nnet::load_weights_from_txt<{}, {}>({}, "{}.txt");\n'.format(
+                                w.type.name, w.data_length, w.name, w.name
+                            )
+
+            # Add input/output type
+            elif '// hls-fpga-machine-learning insert IO' in line:
+                newline = line
+                all_inputs = [i.name for i in model_inputs]
+                all_outputs = [o.name for o in model_outputs]
+                all_brams = [b.name for b in model_brams]
                 io_type = model.config.get_config_value("IOType")
 
                 if io_type == 'io_parallel':
-                    for i in model_inputs: newline += indent + self._make_array_pragma(i) + '\n'
-                    for o in model_outputs: newline += indent + self._make_array_pragma(o) + '\n'
-                    # TODO discussed adding a handle for setting the interface mode for individual input and output arrays (16.03.2020)
+                    for i in model_inputs:
+                        newline += indent + self._make_array_pragma(i) + '\n'
+                    for o in model_outputs:
+                        newline += indent + self._make_array_pragma(o) + '\n'
+                    # TODO discussed adding a handle for setting the interface mode for individual input and output arrays
                     # Probably the handle doesn't need to be exposed to the user but should be just set in hls_model.py
-                    newline += indent + '#pragma HLS INTERFACE ap_vld port={},{} \n'.format(','.join(all_inputs), ','.join(all_outputs))
+                    newline += indent + '#pragma HLS INTERFACE ap_vld port={},{} \n'.format(
+                        ','.join(all_inputs), ','.join(all_outputs)
+                    )
                     if model.config.model_strategy.lower() == 'resource':
                         newline += indent + '#pragma HLS DATAFLOW \n'
                     else:
                         newline += indent + '#pragma HLS PIPELINE \n'
-                if io_type == 'io_serial' or io_type == 'io_stream':
-                    newline += indent + '#pragma HLS INTERFACE axis port={},{} \n'.format(','.join(all_inputs), ','.join(all_outputs))
-                    if all_brams: # No BRAM ports
+                if io_type == 'io_stream':
+                    newline += indent + '#pragma HLS INTERFACE axis port={},{} \n'.format(
+                        ','.join(all_inputs), ','.join(all_outputs)
+                    )
+                    if all_brams:
                         newline += indent + '#pragma HLS INTERFACE bram port={} \n'.format(','.join(all_brams))
                     newline += indent + '#pragma HLS DATAFLOW \n'
 
-                inval_str = '\n    '.join(['const_size_in_{} = {};'.format(i, inp.size_cpp()) for i, inp in enumerate(model_inputs, 1)])
-                outval_str = '\n    '.join(['const_size_out_{} = {};'.format(i, out.size_cpp()) for i, out in enumerate(model_outputs, 1)])
-                newline += '\n' + indent + inval_str
-                newline += '\n' + indent + outval_str
-                newline += '\n'
-
-            elif '//hls-fpga-machine-learning insert layers' in line:
+            elif '// hls-fpga-machine-learning insert layers' in line:
                 newline = line + '\n'
-                inputs = model.get_input_variables()
-                outputs = model.get_output_variables()
                 for layer in model.get_layers():
                     vars = layer.get_variables()
                     for var in vars:
-                        if var not in inputs and var not in outputs:
-                            def_cpp = self.variable_definition_cpp(model, var)
+                        if var not in model_inputs and var not in model_outputs:
+                            def_cpp = var.definition_cpp()
                             if def_cpp is not None:
                                 newline += '    ' + def_cpp + ';\n'
                                 if var.pragma:
                                     newline += '    ' + self._make_array_pragma(var) + '\n'
-                    func = layer.function_cpp()
+                    func = layer.get_attr('function_cpp', None)
                     if func:
+                        func = [func]
                         if len(func) == 1:
                             newline += '    ' + func[0] + ' // ' + layer.name + '\n'
                         else:
                             newline += '// ' + layer.name + '\n'
                             for line in func:
                                 newline += '    ' + line + '\n'
-                        if model.config.trace_output and layer.get_attr('Trace', False):
+                        if model.config.trace_output and layer.get_attr('trace', False):
                             newline += '#ifndef __SYNTHESIS__\n'
                             for var in vars:
-                                newline += '    nnet::save_layer_output<{}>({}, "{}", {});\n'.format(var.type.name, var.name, layer.name, var.size_cpp())
+                                newline += '    nnet::save_layer_output<{}>({}, "{}", {});\n'.format(
+                                    var.type.name, var.name, layer.name, var.size_cpp()
+                                )
                             newline += '#endif\n'
                         newline += '\n'
 
-            #Just copy line
+            # Just copy line
             else:
                 newline = line
 
             fout.write(newline)
 
         f.close()
         fout.close()
 
     def write_project_header(self, model):
-        #######################
-        ## myproject.h
-        #######################
+        """Write the main architecture header file (myproject.h)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
         filedir = os.path.dirname(os.path.abspath(__file__))
-        f = open(os.path.join(filedir,'../templates/vivado/firmware/myproject.h'),'r')
-        fout = open('{}/firmware/{}.h'.format(model.config.get_output_dir(), model.config.get_project_name()),'w')
+        f = open(os.path.join(filedir, '../templates/vivado/firmware/myproject.h'))
+        fout = open(f'{model.config.get_output_dir()}/firmware/{model.config.get_project_name()}.h', 'w')
 
         model_inputs = model.get_input_variables()
         model_outputs = model.get_output_variables()
-        model_brams   = model.get_bram_variables()
+        model_brams = [var for var in model.get_weight_variables() if var.storage.lower() == 'bram']
 
         indent = '    '
 
         for line in f.readlines():
 
             if 'MYPROJECT' in line:
-                newline = line.replace('MYPROJECT',format(model.config.get_project_name().upper()))
-            elif 'void myproject(' in line:
-                newline = 'void {}(\n'.format(model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert header' in line:
-                inputs_str = ', '.join([self.variable_definition_cpp(model, i, as_reference=True) for i in model_inputs])
-                outputs_str = ', '.join([self.variable_definition_cpp(model, o, as_reference=True) for o in model_outputs])
-                brams_str  = ', \n'.join([indent + self.variable_definition_cpp(model, b, as_reference=False) for b in model_brams])
-                insize_str = ', '.join(['unsigned short &const_size_in_{}'.format(i) for i in range(1, len(model_inputs) + 1)])
-                outsize_str = ', '.join(['unsigned short &const_size_out_{}'.format(o) for o in range(1, len(model_outputs) + 1)])
+                newline = line.replace('MYPROJECT', format(model.config.get_project_name().upper()))
+            elif 'myproject' in line:
+                newline = line.replace('myproject', model.config.get_project_name())
+            elif '// hls-fpga-machine-learning insert header' in line:
+                inputs_str = ', '.join([i.definition_cpp(as_reference=True) for i in model_inputs])
+                outputs_str = ', '.join([o.definition_cpp(as_reference=True) for o in model_outputs])
+                brams_str = ', \n'.join([indent + b.definition_cpp(as_reference=False) for b in model_brams])
 
                 newline = ''
                 newline += indent + inputs_str + ',\n'
-                newline += indent + outputs_str + ',\n'
-                if len(model_brams) > 0: 
-                    newline += brams_str + ',\n'
-                newline += indent + insize_str + ',\n'
-                newline += indent + outsize_str + '\n'
+                newline += indent + outputs_str
+                if len(model_brams) > 0:
+                    newline += ',\n' + brams_str
+                newline += '\n'
             else:
                 newline = line
             fout.write(newline)
 
         f.close()
         fout.close()
 
     def write_defines(self, model):
+        """Write the C++ type definitions file (defines.h)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
         filedir = os.path.dirname(os.path.abspath(__file__))
-        f = open(os.path.join(filedir,'../templates/vivado/firmware/defines.h'),'r')
-        fout = open('{}/firmware/defines.h'.format(model.config.get_output_dir()),'w')
+        f = open(os.path.join(filedir, '../templates/vivado/firmware/defines.h'))
+        fout = open(f'{model.config.get_output_dir()}/firmware/defines.h', 'w')
 
         for line in f.readlines():
 
-            #Insert numbers
-            if '//hls-fpga-machine-learning insert numbers' in line:
+            # Insert numbers
+            if '// hls-fpga-machine-learning insert numbers' in line:
                 newline = line
-                numbers = OrderedDict.fromkeys([layer.get_numbers_cpp() for layer in model.get_layers()])
-                newline += ''.join(numbers)
 
-            elif '//hls-fpga-machine-learning insert layer-precision' in line:
+                defines_list = []
+                for layer in model.get_layers():
+                    defines = ''
+                    for k, v in layer.get_output_variable().get_shape():
+                        defines += f'#define {k} {v}\n'
+
+                    defines_list.append(defines)
+
+                newline += ''.join(defines_list)
+
+            elif '// hls-fpga-machine-learning insert layer-precision' in line:
                 newline = line
                 all_precision = OrderedDict()
                 for layer in model.get_layers():
                     layer_precision = layer.get_layer_precision()
-                    all_precision.update(layer_precision)
+                    for type_name, type_var in layer_precision.items():
+                        # Ensure that layer's types doesn't override existing types
+                        # This can happen in case of InplaceVariable types
+                        if type_name not in all_precision:
+                            all_precision[type_name] = type_var
                 for used_type in all_precision.values():
-                    newline += self.type_definition_cpp(model, used_type)
+                    newline += used_type.definition_cpp()
+
             else:
                 newline = line
             fout.write(newline)
         f.close()
         fout.close()
 
     def write_parameters(self, model):
+        """Write the C++ layer config file (parameters.h)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
         filedir = os.path.dirname(os.path.abspath(__file__))
-        f = open(os.path.join(filedir,'../templates/vivado/firmware/parameters.h'),'r')
-        fout = open('{}/firmware/parameters.h'.format(model.config.get_output_dir()),'w')
+        f = open(os.path.join(filedir, '../templates/vivado/firmware/parameters.h'))
+        fout = open(f'{model.config.get_output_dir()}/firmware/parameters.h', 'w')
 
         for line in f.readlines():
 
-            if '//hls-fpga-machine-learning insert includes' in line:
+            if '// hls-fpga-machine-learning insert includes' in line:
                 newline = line
-                for include in sorted(set(sum((layer.include_list for layer in model.get_layers()), []))):
+                for include in sorted(set(sum((layer.get_attr('include_header', []) for layer in model.get_layers()), []))):
                     newline += '#include "%s"\n' % include
 
-            elif '//hls-fpga-machine-learning insert weights' in line:
+            elif '// hls-fpga-machine-learning insert weights' in line:
                 newline = line
                 for layer in model.get_layers():
                     for w in layer.get_weights():
-                        if w.name not in model.bram_vars.keys():
-                            newline += '#include "weights/{}.h"\n'.format(w.name)
-                        else:
-                            newline += '// #include "weights/{}.h"\n'.format(w.name)
+                        if w.storage.lower() != 'bram':
+                            newline += f'#include "weights/{w.name}.h"\n'
 
-            elif "//hls-fpga-machine-learning insert layer-config" in line:
+            elif "// hls-fpga-machine-learning insert layer-config" in line:
                 newline = line
                 for layer in model.get_layers():
-                    config = layer.config_cpp()
+                    config = layer.get_attr('config_cpp', None)
                     if config:
                         newline += '// ' + layer.name + '\n'
                         newline += config + '\n'
             else:
                 newline = line
             fout.write(newline)
         f.close()
         fout.close()
 
     def write_weights(self, model):
+        """Write the weights into header files
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
         for layer in model.get_layers():
             for weights in layer.get_weights():
                 self.print_array_to_cpp(weights, model.config.get_output_dir())
-    
-    def __make_dat_file(self, original_path, project_path): 
+
+    def __make_dat_file(self, original_path, project_path):
         """
         Convert other input/output data types into a dat file, which is
         a text file with the falttened matrix printed out. Note that ' ' is
-        assumed to be the delimiter. 
+        assumed to be the delimiter.
         """
 
-        #Take in data from current supported data files
+        # Take in data from current supported data files
         if original_path[-3:] == "npy":
             data = np.load(original_path)
         else:
             raise Exception("Unsupported input/output data files.")
 
-        #Faltten data, just keep first dimension
+        # Faltten data, just keep first dimension
         data = data.reshape(data.shape[0], -1)
 
         def print_data(f):
             for i in range(data.shape[0]):
                 for j in range(data.shape[1]):
                     f.write(str(data[i][j]) + " ")
                 f.write("\n")
 
-        #Print out in dat file
-        with open(project_path, "w" ) as f:
+        # Print out in dat file
+        with open(project_path, "w") as f:
             print_data(f)
 
     def write_test_bench(self, model):
-        ###################
-        ## test bench
-        ###################
+        """Write the testbench files (myproject_test.cpp and input/output .dat files)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
         filedir = os.path.dirname(os.path.abspath(__file__))
 
-        if not os.path.exists('{}/tb_data/'.format(model.config.get_output_dir())):
-            os.mkdir('{}/tb_data/'.format(model.config.get_output_dir()))
-        
+        if not os.path.exists(f'{model.config.get_output_dir()}/tb_data/'):
+            os.mkdir(f'{model.config.get_output_dir()}/tb_data/')
+
         input_data = model.config.get_config_value('InputData')
         output_predictions = model.config.get_config_value('OutputPredictions')
-        
+
         if input_data:
             if input_data[-3:] == "dat":
-                copyfile(input_data, '{}/tb_data/tb_input_features.dat'.format(model.config.get_output_dir()))
+                copyfile(input_data, f'{model.config.get_output_dir()}/tb_data/tb_input_features.dat')
             else:
-                self.__make_dat_file(input_data,'{}/tb_data/tb_input_features.dat'.format(model.config.get_output_dir()))
-        
+                self.__make_dat_file(input_data, f'{model.config.get_output_dir()}/tb_data/tb_input_features.dat')
+
         if output_predictions:
             if output_predictions[-3:] == "dat":
-                copyfile(output_predictions, '{}/tb_data/tb_output_predictions.dat'.format(model.config.get_output_dir()))
+                copyfile(output_predictions, f'{model.config.get_output_dir()}/tb_data/tb_output_predictions.dat')
             else:
-                self.__make_dat_file(output_predictions,'{}/tb_data/tb_output_predictions.dat'.format(model.config.get_output_dir()))
+                self.__make_dat_file(
+                    output_predictions, f'{model.config.get_output_dir()}/tb_data/tb_output_predictions.dat'
+                )
 
-        f = open(os.path.join(filedir,'../templates/vivado/myproject_test.cpp'),'r')
-        fout = open('{}/{}_test.cpp'.format(model.config.get_output_dir(), model.config.get_project_name()),'w')
+        f = open(os.path.join(filedir, '../templates/vivado/myproject_test.cpp'))
+        fout = open(f'{model.config.get_output_dir()}/{model.config.get_project_name()}_test.cpp', 'w')
+
+        model_inputs = model.get_input_variables()
+        model_outputs = model.get_output_variables()
+        model_brams = [var for var in model.get_weight_variables() if var.storage.lower() == 'bram']
 
         for line in f.readlines():
             indent = ' ' * (len(line) - len(line.lstrip(' ')))
 
-            #Insert numbers
+            # Insert numbers
             if 'myproject' in line:
                 newline = line.replace('myproject', model.config.get_project_name())
-            elif '//hls-fpga-machine-learning insert bram' in line:
+            elif '// hls-fpga-machine-learning insert bram' in line:
                 newline = line
-                for bram in model.get_bram_variables():
-                    newline += '#include \"firmware/weights/{}.h\"\n'.format(bram.cppname)
-            elif '//hls-fpga-machine-learning insert data' in line:
+                for bram in model_brams:
+                    newline += f'#include \"firmware/weights/{bram.name}.h\"\n'
+            elif '// hls-fpga-machine-learning insert data' in line:
                 newline = line
                 offset = 0
-                # for bram in model.get_bram_variables():
-                #     newline += bram.definition_cpp()+';\n'
-                for inp in model.get_input_variables():
-                    newline += '      ' + self.variable_definition_cpp(model, inp) + ';\n'
-                    newline += '      nnet::copy_data<float, {}, {}, {}>(in, {});\n'.format(inp.type.name, offset, inp.size_cpp(), inp.cppname)
+                for inp in model_inputs:
+                    newline += '      ' + inp.definition_cpp() + ';\n'
+                    newline += '      nnet::copy_data<float, {}, {}, {}>(in, {});\n'.format(
+                        inp.type.name, offset, inp.size_cpp(), inp.name
+                    )
                     offset += inp.size()
-                for out in model.get_output_variables():
-                    newline += '      ' + self.variable_definition_cpp(model, out) + ';\n'
-            elif '//hls-fpga-machine-learning insert zero' in line:
-                newline = line
-                for inp in model.get_input_variables():
-                    newline += '    ' + self.variable_definition_cpp(model, inp) + ';\n'
-                    newline += '    nnet::fill_zero<{}, {}>({});\n'.format(inp.type.name, inp.size_cpp(), inp.cppname)
-                for out in model.get_output_variables():
-                    newline += '    ' + self.variable_definition_cpp(model, out) + ';\n'
-            elif '//hls-fpga-machine-learning insert top-level-function' in line:
-                newline = line
-
-                size_str = indent + 'unsigned short {},{};\n'
-                input_size_vars = ','.join(['size_in{}'.format(i) for i in range(1, len(model.get_input_variables()) + 1)])
-                output_size_vars = ','.join(['size_out{}'.format(o) for o in range(1, len(model.get_output_variables()) + 1)])
-                newline += size_str.format(input_size_vars, output_size_vars)
-
-                input_vars = ','.join([i.cppname for i in model.get_input_variables()])
-                output_vars = ','.join([o.cppname for o in model.get_output_variables()])
-                bram_vars   =','.join([b.cppname for b in model.get_bram_variables()]) 
+                for out in model_outputs:
+                    newline += '      ' + out.definition_cpp() + ';\n'
+            elif '// hls-fpga-machine-learning insert zero' in line:
+                newline = line
+                for inp in model_inputs:
+                    newline += '    ' + inp.definition_cpp() + ';\n'
+                    newline += f'    nnet::fill_zero<{inp.type.name}, {inp.size_cpp()}>({inp.name});\n'
+                for out in model_outputs:
+                    newline += '    ' + out.definition_cpp() + ';\n'
+            elif '// hls-fpga-machine-learning insert top-level-function' in line:
+                newline = line
+
+                input_vars = ','.join([i.name for i in model_inputs])
+                output_vars = ','.join([o.name for o in model_outputs])
+                bram_vars = ','.join([b.name for b in model_brams])
 
                 # Concatenate the input, output, and bram variables. Filter out empty/null values
                 all_vars = ','.join(filter(None, [input_vars, output_vars, bram_vars]))
 
-                top_level = indent + '{}({},{},{});\n'.format(model.config.get_project_name(), all_vars, input_size_vars, output_size_vars)
-                
+                top_level = indent + f'{model.config.get_project_name()}({all_vars});\n'
+
                 newline += top_level
-            elif '//hls-fpga-machine-learning insert predictions' in line:
+            elif '// hls-fpga-machine-learning insert predictions' in line:
                 newline = line
-                for out in model.get_output_variables():
-                    newline += indent + 'for(int i = 0; i < {}; i++) {{\n'.format(out.size_cpp())
+                for out in model_outputs:
+                    newline += indent + f'for(int i = 0; i < {out.size_cpp()}; i++) {{\n'
                     newline += indent + '  std::cout << pr[i] << " ";\n'
                     newline += indent + '}\n'
                     newline += indent + 'std::cout << std::endl;\n'
-            elif '//hls-fpga-machine-learning insert tb-output' in line:
-                newline = line
-                for out in model.get_output_variables():
-                    newline += indent + 'nnet::print_result<{}, {}>({}, fout);\n'.format(out.type.name, out.size_cpp(), out.cppname) #TODO enable this
-            elif '//hls-fpga-machine-learning insert output' in line or '//hls-fpga-machine-learning insert quantized' in line:
+            elif '// hls-fpga-machine-learning insert tb-output' in line:
                 newline = line
-                for out in model.get_output_variables():
-                    newline += indent + 'nnet::print_result<{}, {}>({}, std::cout, true);\n'.format(out.type.name, out.size_cpp(), out.cppname)
+                for out in model_outputs:
+                    newline += indent + 'nnet::print_result<{}, {}>({}, fout);\n'.format(
+                        out.type.name, out.size_cpp(), out.name
+                    )  # TODO enable this
+            elif (
+                '// hls-fpga-machine-learning insert output' in line
+                or '// hls-fpga-machine-learning insert quantized' in line
+            ):
+                newline = line
+                for out in model_outputs:
+                    newline += indent + 'nnet::print_result<{}, {}>({}, std::cout, true);\n'.format(
+                        out.type.name, out.size_cpp(), out.name
+                    )
             else:
                 newline = line
             fout.write(newline)
         f.close()
         fout.close()
 
     def write_bridge(self, model):
-        ###################
-        # c++-python bridge
-        ###################
+        """Write the Python-C++ bridge (myproject_bridge.cpp)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
         filedir = os.path.dirname(os.path.abspath(__file__))
-        f = open(os.path.join(filedir,'../templates/vivado/myproject_bridge.cpp'),'r')
-        fout = open('{}/{}_bridge.cpp'.format(model.config.get_output_dir(), model.config.get_project_name()),'w')
+        f = open(os.path.join(filedir, '../templates/vivado/myproject_bridge.cpp'))
+        fout = open(f'{model.config.get_output_dir()}/{model.config.get_project_name()}_bridge.cpp', 'w')
 
         model_inputs = model.get_input_variables()
         model_outputs = model.get_output_variables()
-        model_brams   = model.get_bram_variables()
+        model_brams = [var for var in model.get_weight_variables() if var.storage.lower() == 'bram']
 
         indent = '    '
 
         for line in f.readlines():
 
             if 'MYPROJECT' in line:
                 newline = line.replace('MYPROJECT', format(model.config.get_project_name().upper()))
             elif 'myproject' in line:
                 newline = line.replace('myproject', format(model.config.get_project_name()))
-            elif '//hls-fpga-machine-learning insert bram' in line:
+            elif '// hls-fpga-machine-learning insert bram' in line:
                 newline = line
                 for bram in model_brams:
-                    newline += '#include \"firmware/weights/{}.h\"\n'.format(bram.cppname)
-            elif '//hls-fpga-machine-learning insert header' in line:
+                    newline += f'#include \"firmware/weights/{bram.name}.h\"\n'
+            elif '// hls-fpga-machine-learning insert header' in line:
                 dtype = line.split('#', 1)[1].strip()
-                inputs_str = ', '.join(['{type} {name}[{shape}]'.format(type=dtype, name=i.cppname, shape=i.size_cpp()) for i in model_inputs])
-                outputs_str = ', '.join(['{type} {name}[{shape}]'.format(type=dtype, name=o.cppname, shape=o.size_cpp()) for o in model_outputs])
-                insize_str = ', '.join(['unsigned short &const_size_in_{}'.format(i) for i in range(1, len(model_inputs) + 1)])
-                outsize_str = ', '.join(['unsigned short &const_size_out_{}'.format(o) for o in range(1, len(model_outputs) + 1)])
+                inputs_str = ', '.join([f'{dtype} {i.name}[{i.size_cpp()}]' for i in model_inputs])
+                outputs_str = ', '.join([f'{dtype} {o.name}[{o.size_cpp()}]' for o in model_outputs])
 
                 newline = ''
                 newline += indent + inputs_str + ',\n'
-                newline += indent + outputs_str + ',\n'
-                newline += indent + insize_str + ',\n'
-                newline += indent + outsize_str + '\n'
-            elif '//hls-fpga-machine-learning insert wrapper' in line:
+                newline += indent + outputs_str + '\n'
+            elif '// hls-fpga-machine-learning insert wrapper' in line:
                 dtype = line.split('#', 1)[1].strip()
                 newline = ''
                 for i in model_inputs:
-                    newline += indent + '{var};\n'.format(var=self.variable_definition_cpp(model, i, name_suffix='_ap'))
-                    newline += indent + 'nnet::convert_data<{}, {}, {}>({}, {}_ap);\n'.format(dtype, i.type.name, i.size_cpp(), i.cppname, i.cppname)
+                    newline += indent + '{var};\n'.format(var=i.definition_cpp(name_suffix='_ap'))
+                    newline += indent + 'nnet::convert_data<{}, {}, {}>({}, {}_ap);\n'.format(
+                        dtype, i.type.name, i.size_cpp(), i.name, i.name
+                    )
                 newline += '\n'
-                
+
                 for o in model_outputs:
-                    newline += indent + '{var};\n'.format(var=self.variable_definition_cpp(model, o, name_suffix='_ap'))
-                
+                    newline += indent + '{var};\n'.format(var=o.definition_cpp(name_suffix='_ap'))
+
                 newline += '\n'
 
-                input_size_vars = ','.join(['const_size_in_{}'.format(i) for i in range(1, len(model.get_input_variables()) + 1)])
-                output_size_vars = ','.join(['const_size_out_{}'.format(o) for o in range(1, len(model.get_output_variables()) + 1)])
-                input_vars = ','.join([i.cppname + '_ap' for i in model.get_input_variables()])
-                bram_vars   =','.join([b.cppname for b in model.get_bram_variables()]) 
-                output_vars = ','.join([o.cppname + '_ap' for o in model.get_output_variables()])
-                
+                input_vars = ','.join([i.name + '_ap' for i in model_inputs])
+                bram_vars = ','.join([b.name for b in model_brams])
+                output_vars = ','.join([o.name + '_ap' for o in model_outputs])
+
                 # Concatenate the input, output, and bram variables. Filter out empty/null values
                 all_vars = ','.join(filter(None, [input_vars, output_vars, bram_vars]))
 
-                top_level = indent + '{}({},{},{});\n'.format(model.config.get_project_name(), all_vars, input_size_vars, output_size_vars)
+                top_level = indent + f'{model.config.get_project_name()}({all_vars});\n'
                 newline += top_level
 
                 newline += '\n'
 
                 for o in model_outputs:
-                    newline += indent + 'nnet::convert_data<{}, {}, {}>({}_ap, {});\n'.format(o.type.name, dtype, o.size_cpp(), o.cppname, o.cppname)
-            elif '//hls-fpga-machine-learning insert trace_outputs' in line:
+                    newline += indent + 'nnet::convert_data<{}, {}, {}>({}_ap, {});\n'.format(
+                        o.type.name, dtype, o.size_cpp(), o.name, o.name
+                    )
+            elif '// hls-fpga-machine-learning insert trace_outputs' in line:
                 newline = ''
                 for layer in model.get_layers():
-                    if layer.function_cpp() and model.config.trace_output and layer.get_attr('Trace', False):
-                            vars = layer.get_variables()
-                            for var in vars:
-                                newline += indent + 'nnet::trace_outputs->insert(std::pair<std::string, void *>("{}", (void *) malloc({} * element_size)));\n'.format(layer.name, var.size_cpp())
-                    
+                    func = layer.get_attr('function_cpp', None)
+                    if func and model.config.trace_output and layer.get_attr('trace', False):
+                        vars = layer.get_variables()
+                        for var in vars:
+                            newline += (
+                                indent
+                                + 'nnet::trace_outputs->insert(std::pair<std::string, void *>('
+                                + f'"{layer.name}", (void *) malloc({var.size_cpp()} * element_size)));\n'
+                            )
+
             else:
                 newline = line
             fout.write(newline)
 
         f.close()
         fout.close()
 
     def write_build_script(self, model):
-        ###################
-        # build_prj.tcl
-        ###################
+        """Write the TCL/Shell build scripts (project.tcl, build_prj.tcl, vivado_synth.tcl, build_lib.sh)
 
-        filedir = os.path.dirname(os.path.abspath(__file__))
-
-        f = open(os.path.join(filedir,'../templates/vivado/build_prj.tcl'),'r')
-        fout = open('{}/build_prj.tcl'.format(model.config.get_output_dir()),'w')
-
-        for line in f.readlines():
-
-            line = line.replace('myproject',model.config.get_project_name())
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
-            if 'set_part {xcku115-flvb2104-2-i}' in line:
-                line = 'set_part {{{}}}\n'.format(model.config.get_config_value('XilinxPart'))
-            elif 'create_clock -period 5 -name default' in line:
-                line = 'create_clock -period {} -name default\n'.format(model.config.get_config_value('ClockPeriod'))
+        filedir = os.path.dirname(os.path.abspath(__file__))
 
-            fout.write(line)
+        # project.tcl
+        f = open(f'{model.config.get_output_dir()}/project.tcl', 'w')
+        f.write('variable project_name\n')
+        f.write(f'set project_name "{model.config.get_project_name()}"\n')
+        f.write('variable backend\n')
+        f.write('set backend "vivado"\n')
+        f.write('variable part\n')
+        f.write('set part "{}"\n'.format(model.config.get_config_value('Part')))
+        f.write('variable clock_period\n')
+        f.write('set clock_period {}\n'.format(model.config.get_config_value('ClockPeriod')))
+        f.write('variable clock_uncertainty\n')
+        f.write('set clock_uncertainty {}\n'.format(model.config.get_config_value('ClockUncertainty', '12.5%')))
         f.close()
-        fout.close()
 
+        # build_prj.tcl
+        srcpath = os.path.join(filedir, '../templates/vivado/build_prj.tcl')
+        dstpath = f'{model.config.get_output_dir()}/build_prj.tcl'
+        copyfile(srcpath, dstpath)
 
-        ###################
         # vivado_synth.tcl
-        ###################
-
-        f = open(os.path.join(filedir,'../templates/vivado/vivado_synth.tcl'),'r')
-        fout = open('{}/vivado_synth.tcl'.format(model.config.get_output_dir()),'w')
-        for line in f.readlines():
-            line = line.replace('myproject', model.config.get_project_name())
-            if '-part' in line:
-                line = 'synth_design -top {} -part {}\n'.format(model.config.get_project_name(), model.config.get_config_value('XilinxPart'))
+        srcpath = os.path.join(filedir, '../templates/vivado/vivado_synth.tcl')
+        dstpath = f'{model.config.get_output_dir()}/vivado_synth.tcl'
+        copyfile(srcpath, dstpath)
 
-            fout.write(line)
-        f.close()
-        fout.close()
-
-        ###################
         # build_lib.sh
-        ###################
-
-        f = open(os.path.join(filedir,'../templates/vivado/build_lib.sh'),'r')
-        fout = open('{}/build_lib.sh'.format(model.config.get_output_dir()),'w')
+        f = open(os.path.join(filedir, '../templates/vivado/build_lib.sh'))
+        fout = open(f'{model.config.get_output_dir()}/build_lib.sh', 'w')
 
         for line in f.readlines():
             line = line.replace('myproject', model.config.get_project_name())
             line = line.replace('mystamp', model.config.get_config_value('Stamp'))
 
             fout.write(line)
         f.close()
         fout.close()
 
     def write_nnet_utils(self, model):
-        ###################
-        ## nnet_utils
-        ###################
+        """Copy the nnet_utils, AP types headers and any custom source to the project output directory
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
+        # nnet_utils
         filedir = os.path.dirname(os.path.abspath(__file__))
 
-        srcpath = os.path.join(filedir,'../templates/vivado/nnet_utils/')
-        dstpath = '{}/firmware/nnet_utils/'.format(model.config.get_output_dir())
+        srcpath = os.path.join(filedir, '../templates/vivado/nnet_utils/')
+        dstpath = f'{model.config.get_output_dir()}/firmware/nnet_utils/'
 
         if not os.path.exists(dstpath):
             os.mkdir(dstpath)
 
         headers = [os.path.basename(h) for h in glob.glob(srcpath + '*.h')]
 
         for h in headers:
             copyfile(srcpath + h, dstpath + h)
 
-        ###################
-        ## ap_types
-        ###################
-
+        # ap_types
         filedir = os.path.dirname(os.path.abspath(__file__))
 
-        srcpath = os.path.join(filedir,'../templates/vivado/ap_types/')
-        dstpath = '{}/firmware/ap_types/'.format(model.config.get_output_dir())
+        srcpath = os.path.join(filedir, '../templates/vivado/ap_types/')
+        dstpath = f'{model.config.get_output_dir()}/firmware/ap_types/'
 
         if os.path.exists(dstpath):
             rmtree(dstpath)
 
         copytree(srcpath, dstpath)
 
+        # custom source
+        filedir = os.path.dirname(os.path.abspath(__file__))
+
+        custom_source = model.config.backend.get_custom_source()
+        for dst, srcpath in custom_source.items():
+            dstpath = f'{model.config.get_output_dir()}/firmware/{dst}'
+            copyfile(srcpath, dstpath)
+
+    def write_generated_code(self, model):
+        """Write the generated code (nnet_code_gen.h)
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
+        path = f'{model.config.get_output_dir()}/firmware/nnet_utils/nnet_code_gen.h'
+        f = open(path)
+        contents = f.readlines()
+        f.close()
+        f = open(path, 'w')
+
+        for line in contents:
+            if '// hls4ml insert code' in line:
+                newline = line
+                for layer in model.get_layers():
+                    for generated_code in layer.code.values():
+                        newline += str(generated_code)
+            else:
+                newline = line
+            f.write(newline)
+        f.close()
+
     def write_yml(self, model):
-        ###################
-        # YAML config file
-        ###################
+        """Write the config to the YAML file
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
         def keras_model_representer(dumper, keras_model):
             model_path = model.config.get_output_dir() + '/keras_model.h5'
             keras_model.save(model_path)
-            return dumper.represent_scalar(u'!keras_model', model_path)
+            return dumper.represent_scalar('!keras_model', model_path)
 
         try:
             from tensorflow.keras import Model as KerasModel
+
             yaml.add_multi_representer(KerasModel, keras_model_representer)
-        except:
+        except Exception:
             pass
 
         with open(model.config.get_output_dir() + '/' + config_filename, 'w') as file:
             yaml.dump(model.config.config, file)
 
     def write_tar(self, model):
-        ###################
-        # Tarball output
-        ###################
+        """Write the generated project as a .tar.gz archive
+
+        Args:
+            model (ModelGraph): the hls4ml model.
+        """
 
         with tarfile.open(model.config.get_output_dir() + '.tar.gz', mode='w:gz') as archive:
             archive.add(model.config.get_output_dir(), recursive=True)
 
     def write_hls(self, model):
         print('Writing HLS project')
         self.write_project_dir(model)
@@ -675,10 +719,11 @@
         self.write_weights(model)
         self.write_defines(model)
         self.write_parameters(model)
         self.write_test_bench(model)
         self.write_bridge(model)
         self.write_build_script(model)
         self.write_nnet_utils(model)
+        self.write_generated_code(model)
         self.write_yml(model)
         self.write_tar(model)
         print('Done')
```

### Comparing `hls4ml-0.6.0/test/build-prj.sh` & `hls4ml-0.7.0rc1/test/build-prj.sh`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/test/cleanup.sh` & `hls4ml-0.7.0rc1/test/cleanup.sh`

 * *Files 1% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 
 #rundir=`pwd`
 
 cd "${basedir}"
 
 rm -f *.tar.gz
 
-# Delete 
+# Delete
 for dir in */ ; do
    if [ ! -f "${dir}BUILD_FAILED" ]; then
       rm -rf "${dir}"
       if [ $? -eq 0 ]; then
          echo "Removed ${dir%/}."
       else
          failed=1
```

### Comparing `hls4ml-0.6.0/test/compare-reports.sh` & `hls4ml-0.7.0rc1/test/compare-reports.sh`

 * *Files 2% similar despite different names*

```diff
@@ -75,18 +75,18 @@
    rptname="${rptname_orig[$idx_orig]}"
    idx_new="${idx_orig}"
    for j in "${!rptname_new[@]}"; do
       if [[ "${rptname_new[$j]}" = "${rptname}" ]]; then
          idx_new="${j}"
       fi
    done
-   
+
    report_orig="${reports_orig[$idx_orig]}"
    report_new="${reports_new[$idx_new]}"
-   
+
    if [ "${latency}" -eq 1 ]; then
       latency_orig=$(grep -A7 "+ Latency" <<< "${report_orig}")
       latency_new=$(grep -A7 "+ Latency" <<< "${report_new}")
       if [[ "${latency_orig}" != "${latency_new}" ]]; then
          failed=1
          echo "${rptname} has changed"
          echo ""
@@ -96,15 +96,15 @@
          right+="${latency_new}"
          column <(echo "${left}") <(echo "${right}")
          echo ""
          echo ""
          echo ""
       fi
    fi
-   
+
    if [ "${utilization}" -eq 1 ]; then
       utilization_orig=$(grep -B3 -A13 "|DSP" <<< "${report_orig}")
       utilization_new=$(grep -B3 -A13 "|DSP" <<< "${report_new}")
       if [[ "${utilization_orig}" != "${utilization_new}" ]]; then
          failed=1
          echo "${rptname} has changed"
          echo ""
@@ -117,8 +117,7 @@
          echo ""
          echo ""
       fi
    fi
 done
 
 exit ${failed}
-
```

### Comparing `hls4ml-0.6.0/test/convert-keras-models.sh` & `hls4ml-0.7.0rc1/test/convert-keras-models.sh`

 * *Files 4% similar despite different names*

```diff
@@ -68,15 +68,14 @@
          if [[ "${model_def[$i]}" == r:* ]] ; then params[5]="-r ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == s:* ]] ; then params[6]="-g ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == i:* ]] ; then params[7]="-t ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == y:* ]] ; then params[8]="-y ${model_def[$i]:2} "; fi
       done
       params[9]=${model_def[0]}
       cmd="./keras-to-hls.sh ${dir} ${params[0]}${params[1]}${params[2]}${params[3]}${params[4]}${params[5]}${params[6]}${params[7]}${params[8]}${params[9]}"
-      echo "$cmd"
       ${exec} "${cmd}"
    fi
 done
 
 #cd "${rundir}"
 
 exit ${failed}
```

### Comparing `hls4ml-0.6.0/test/convert-onnx-models.sh` & `hls4ml-0.7.0rc1/test/convert-onnx-models.sh`

 * *Files 1% similar despite different names*

```diff
@@ -63,17 +63,17 @@
          if [[ "${model_def[$i]}" == x:* ]] ; then params[0]="-x ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == c:* ]] ; then params[1]="-c ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == io:s ]] ; then params[2]="-s "; fi
          if [[ "${model_def[$i]}" == r:* ]] ; then params[3]="-r ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == i:* ]] ; then params[4]="-t ${model_def[$i]:2} "; fi
       done
       params[5]=${model_def[0]}
-      
+
       cmd="./onnx-to-hls.sh ${dir} ${params[0]}${params[1]}${params[2]}${params[3]}${params[4]}${params[5]}"
-      
+
       ${exec} "${cmd}"
    fi
 done
 
 #cd "${rundir}"
 
 exit ${failed}
```

### Comparing `hls4ml-0.6.0/test/convert-pytorch-models.sh` & `hls4ml-0.7.0rc1/test/convert-pytorch-models.sh`

 * *Files 0% similar despite different names*

```diff
@@ -63,17 +63,17 @@
          if [[ "${model_def[$i]}" == x:* ]] ; then params[0]="-x ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == c:* ]] ; then params[1]="-c ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == io:s ]] ; then params[2]="-s "; fi
          if [[ "${model_def[$i]}" == r:* ]] ; then params[3]="-r ${model_def[$i]:2} "; fi
          if [[ "${model_def[$i]}" == i:* ]] ; then params[4]="-t ${model_def[$i]:2} "; fi
       done
       params[5]=${model_def[0]}
-      
+
       cmd="./pytorch-to-hls.sh ${dir} ${params[0]}${params[1]}${params[2]}${params[3]}${params[4]}${params[5]}"
-      
+
       ${exec} "${cmd}"
    fi
 done
 
 #cd "${rundir}"
 
 exit ${failed}
```

### Comparing `hls4ml-0.6.0/test/docker/Dockerfile` & `hls4ml-0.7.0rc1/test/docker/Dockerfile`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/test/docker/README.md` & `hls4ml-0.7.0rc1/test/docker/README.md`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -64,8 +64,8 @@
 
 ## Customizing the default user
 
 Default user (named *hls4ml*) cah have its *id* and *group* changed to match a specific user on host machine with `USER_ID` and `GROUP_ID` build arguments. Useful if you want to add a shared volume. For example:
 
 ```
 docker build --network=host -t hls4ml-with-vivado --build-arg LICENSE_SERVER="1234@myserver" --build-arg USER_ID=`id -u` --build-arg GROUP_ID=`id -g` .
-```
+```
```

### Comparing `hls4ml-0.6.0/test/docker/install_config-2017.2.txt` & `hls4ml-0.7.0rc1/test/docker/install_config-2017.2.txt`

 * *Files 0% similar despite different names*

```diff
@@ -21,8 +21,7 @@
 CreateShortcutsForAllUsers=0
 
 # Choose whether shortcuts will be created on the desktop or not.
 CreateDesktopShortcuts=0
 
 # Choose whether file associations will be created or not.
 CreateFileAssociation=0
-
```

### Comparing `hls4ml-0.6.0/test/docker/install_config.txt` & `hls4ml-0.7.0rc1/test/docker/install_config.txt`

 * *Files 4% similar despite different names*

```diff
@@ -21,8 +21,7 @@
 CreateShortcutsForAllUsers=0
 
 # Choose whether shortcuts will be created on the desktop or not.
 CreateDesktopShortcuts=0
 
 # Choose whether file associations will be created or not.
 CreateFileAssociation=0
-
```

### Comparing `hls4ml-0.6.0/test/gather-reports.sh` & `hls4ml-0.7.0rc1/test/gather-reports.sh`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/test/hls4ml-onnx-test.sh` & `hls4ml-0.7.0rc1/test/hls4ml-onnx-test.sh`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 VIVADO_INSTALL_DIR=/opt/Xilinx
 VIVADO_VERSION=2017.2
 
 # If running in docker image we would first need to activate the proper conda environment
 #. activate hls4ml-py36
 
-# Convert models in onnx-models.txt 
+# Convert models in onnx-models.txt
 ./convert-onnx-models.sh -x -p 3 -f onnx-models.txt
 
 # Same for Python 2
 #. activate hls4ml-py27
 ./convert-onnx-models.sh -x -p 2 -f onnx-models.txt
 
 # Alternatively, onnx-to-hls script can be called, with the model name(s) specified, i.e.:
```

### Comparing `hls4ml-0.6.0/test/hls4ml-pytorch-test.sh` & `hls4ml-0.7.0rc1/test/hls4ml-pytorch-test.sh`

 * *Files 9% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 VIVADO_INSTALL_DIR=/opt/Xilinx
 VIVADO_VERSION=2017.2
 
 # If running in docker image we would first need to activate the proper conda environment
 #. activate hls4ml-py36
 
-# Convert models in pytorch-models.txt 
+# Convert models in pytorch-models.txt
 ./convert-pytorch-models.sh -x -p 3 -f pytorch-models.txt
 
 # Same for Python 2
 #. activate hls4ml-py27
 ./convert-pytorch-models.sh -x -p 2 -f pytorch-models.txt
 
 # Alternatively, pytorch-to-hls script can be called, with the model name(s) specified, i.e.:
```

### Comparing `hls4ml-0.6.0/test/keras-models.txt` & `hls4ml-0.7.0rc1/test/keras-models.txt`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Keras models from examples directory that will be used for testing
 #
 # Synthax:
-#    MODEL_NAME[:WEIGHTS_FILE] [x:XILINXPART] [b:BOARD] [B:BACKEND] [c:CLOCK_PERIOD] [io:s] [r:REUSE_FACTOR] [t:AP_TYPE] [s:STRATEGY] [y:CONFIG_FILE]
+#    MODEL_NAME[:WEIGHTS_FILE] [x:PART] [b:BOARD] [B:BACKEND] [c:CLOCK_PERIOD] [io:s] [r:REUSE_FACTOR] [t:AP_TYPE] [s:STRATEGY] [y:CONFIG_FILE]
 # where
 #    MODEL_NAME - Name of the file containing json model (without ".json")
 #    WEIGHTS_FILE - Name of the HDF5 file containing model weights (without ".h5")
-#    x:XILINXPART - Xilinx part number to use
+#    x:PART - FPGA part number to use
 #    b:BOARD - name of one board defined in supported_board.json file
 #    B:BACKEND - name of the backend to be used (Vivado, VivadoAccelerator)
 #    c:CLOCK_PERIOD - Clock period
 #    io:s - User streaming I/O, otherwise use parallel I/O
 #    r:REUSE_FACTOR - Reuse factor
 #    s:STRATEGY - Latency-optimized or Resource-optimized strategy
 #    t:AP_TYPE - Default precision
@@ -25,17 +25,18 @@
 #KERAS_conv1d_small
 #KERAS_conv2d_model
 #KERAS_dense_16x100x100x100x100x100x5
 KERAS_3layer_batch_norm
 KERAS_3layer_binary_smaller
 KERAS_3layer_ternary_small
 
+# Pynq backend
 KERAS_3layer b:pynq-z2 B:VivadoAccelerator x:xc7z020clg400-1 s:Resource
+garnet_1layer x:xcku115-flvb2104-2-i y:garnet_1layer_config
 
-garnet_1layer x:xcku115-flvb2104-2-i y:garnet_1layer_config 
 
 # Resource strategy
 KERAS_3layer r:2 s:Resource
 qkeras_mnist_dense r:112 s:Resource
 
 #Fails synthesis due to a problem with loop unrolling
 #jetTagger_Conv2D_Small:jetTagger_Conv2D_Small
@@ -45,8 +46,8 @@
 KERAS_3layer io:s
 KERAS_conv1d_small io:s
 KERAS_conv2d_model io:s
 jetTagger_Conv2D_Small io:s
 jetTagger_Conv2D_Small_NoBatchNorm io:s
 
 
-#KERAS_1layer x:xcku115-flvf1924-2-i
+#KERAS_1layer x:xcku115-flvf1924-2-i
```

### Comparing `hls4ml-0.6.0/test/keras-to-hls.sh` & `hls4ml-0.7.0rc1/test/keras-to-hls.sh`

 * *Files 15% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 #!/bin/bash
 
 pycmd=python
-xilinxpart="xcvu9p-flgb2104-2-e"
+part="xcvu9p-flgb2104-2-e"
 board="None"
 backend="Vivado"
 clock=5
 io=io_parallel
 rf=1
 strategy="Latency"
 type="ap_fixed<16,6>"
 yml=""
 basedir=vivado_prj
-
+precision="float"
 sanitizer="[^A-Za-z0-9._]"
 
 function print_usage {
    echo "Usage: `basename $0` [OPTION] MODEL[:H5FILE]..."
    echo ""
    echo "MODEL is the name of the model json file without extension. Optionally"
    echo "a H5 file with weights can be provided using the MODEL:H5FILE synthax."
    echo "By default, it is assumed that weights are stored in MODEL_weights.h5."
    echo "Multiple models can be specified."
    echo ""
    echo "Options are:"
-   echo "   -x XILINXPART"
-   echo "      Xilinx device part number. Defaults to 'xcvu9p-flgb2104-2-e'."
+   echo "   -x PART"
+   echo "      FPGA device part number. Defaults to 'xcvu9p-flgb2104-2-e'."
    echo "   -b BOARD"
    echo "      Board used. Defaults to 'pynq-z2'."
    echo "   -B BACKEND"
    echo "      Backend to use for the generation of the code. Defaults to 'Vivado'."
    echo "   -c CLOCK"
    echo "      Clock period to use. Defaults to 5."
    echo "   -s"
@@ -43,17 +43,17 @@
    echo "      Output directory."
    echo "   -y FILE"
    echo "      YAML config file to take HLS config from. If specified, -r, -g and -t are ignored."
    echo "   -h"
    echo "      Prints this help message."
 }
 
-while getopts ":x:b:B:c:sr:g:t:d:y:h" opt; do
+while getopts ":x:b:B:c:sr:g:t:d:y:p:h" opt; do
    case "$opt" in
-   x) xilinxpart=$OPTARG
+   x) part=$OPTARG
       ;;
    b) board=$OPTARG
       ;;
    B) backend=$OPTARG
       ;;
    c) clock=$OPTARG
       ;;
@@ -65,14 +65,16 @@
       ;;
    t) type=$OPTARG
       ;;
    d) basedir=$OPTARG
       ;;
    y) yml=$OPTARG
       ;;
+   p) precision=$OPTARG
+      ;;
    h)
       print_usage
       exit
       ;;
    :)
       echo "Option -$OPTARG requires an argument."
       exit 1
@@ -99,41 +101,48 @@
       name="${model_h5_pair[0]}"
       h5="${model_h5_pair[1]}"
    fi
 
    echo "Creating config file for model '${model}'"
    base=`echo "${h5}" | sed -e 's/\(_weights\)*$//g'`
    file="${basedir}/${base}.yml"
-   prjdir="${basedir}/${base}-backend${backend}-board${board//${sanitizer}/_}-${xilinxpart//${sanitizer}/_}-c${clock}-${io}-rf${rf}-${type//${sanitizer}/_}-${strategy}"
+   prjdir="${basedir}/${base}-backend${backend}-board${board//${sanitizer}/_}-${part//${sanitizer}/_}-c${clock}-${io}-rf${rf}-${type//${sanitizer}/_}-${strategy}"
 
    hlscfg=""
    if [ ! -z "${yml}" ]; then
       hlscfg=`sed -ne '/HLSConfig/,$p' ../example-models/config-files/${yml}`
    fi
-
    echo "KerasJson: ../example-models/keras/${name}.json" > ${file}
    echo "KerasH5:   ../example-models/keras/${h5}.h5" >> ${file}
    echo "OutputDir: ${prjdir}" >> ${file}
    echo "ProjectName: myproject" >> ${file}
-   echo "XilinxPart: ${xilinxpart}" >> ${file}
+   echo "Part: ${part}" >> ${file}
    echo "Board: ${board}" >> ${file}
    echo "Backend: ${backend}" >> ${file}
    echo "ClockPeriod: ${clock}" >> ${file}
    echo "" >> ${file}
    echo "IOType: ${io}" >> ${file}
-   
    if [ -z "${hlscfg}" ]
    then
       echo "HLSConfig:" >> ${file}
       echo "  Model:" >> ${file}
       echo "    ReuseFactor: ${rf}" >> ${file}
       echo "    Precision: ${type} " >> ${file}
       echo "    Strategy: ${strategy} " >> ${file}
    else
       echo "${hlscfg}" >> ${file}
    fi
+   # Adding VivadoAccelerator config to file
+   if [ "${backend}" = "VivadoAccelerator" ];
+   then
+     echo "AcceleratorConfig:" >> ${file}
+     echo "  Board: ${board}" >> ${file}
+     echo "  Precision:" >> ${file}
+     echo "    Input: ${precision}" >> ${file}
+     echo "    Output: ${precision}" >> ${file}
+   fi
 
    ${pycmd} ../scripts/hls4ml convert -c ${file} || exit 1
    rm ${file}
    rm -rf "${prjdir}"
    echo ""
 done
```

### Comparing `hls4ml-0.6.0/test/onnx-models.txt` & `hls4ml-0.7.0rc1/test/onnx-models.txt`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # ONNX models from examples directory that will be used for testing
 #
 # Synthax:
-#    MODEL_NAME [x:XILINXPART] [c:CLOCK_PERIOD] [io:s] [r:REUSE_FACTOR] [t:AP_TYPE] [s:STRATEGY]
+#    MODEL_NAME [x:PART] [c:CLOCK_PERIOD] [io:s] [r:REUSE_FACTOR] [t:AP_TYPE] [s:STRATEGY]
 # where
 #    MODEL_NAME - Name of the file containing the model (without ".onnx")
-#    x:XILINXPART - Xilinx part number to use
+#    x:PART - FPGA part number to use
 #    c:CLOCK_PERIOD - Clock period
 #    io:s - User streaming I/O, otherwise use parallel I/O
 #    r:REUSE_FACTOR - Reuse factor
 #    s:STRATEGY - Latency-optimized or Resource-optimized strategy
 #    t:AP_TYPE - Default precision
 #
 # Lines starting with "#" are ignored.
```

### Comparing `hls4ml-0.6.0/test/onnx-to-hls.sh` & `hls4ml-0.7.0rc1/test/pytorch-to-hls.sh`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 #!/bin/bash
 
 pycmd=python
-xilinxpart="xc7vx690tffg1927-2"
+part="xc7vx690tffg1927-2"
 clock=5
 io=io_parallel
 rf=1
 strategy="Latency"
 type="ap_fixed<16,6>"
 basedir=vivado_prj
 
 sanitizer="[^A-Za-z0-9._]"
 
 function print_usage {
    echo "Usage: `basename $0` [OPTION] MODEL..."
    echo ""
-   echo "MODEL is the name of the model onnx file without extension. Multiple"
+   echo "MODEL is the name of the model pt file without extension. Multiple"
    echo "models can be specified."
    echo ""
    echo "Options are:"
    echo "   -x DEVICE"
    echo "      Xilinx device part number. Defaults to 'xc7vx690tffg1927-2'."
    echo "   -c CLOCK"
    echo "      Clock period to use. Defaults to 5."
@@ -34,15 +34,15 @@
    echo "      Output directory."
    echo "   -h"
    echo "      Prints this help message."
 }
 
 while getopts ":x:c:sr:g:t:d:h" opt; do
    case "$opt" in
-   x) xilinxpart=$OPTARG
+   x) part=$OPTARG
       ;;
    c) clock=$OPTARG
       ;;
    s) io=io_stream
       ;;
    r) rf=$OPTARG
       ;;
@@ -75,18 +75,18 @@
 
 for model in "${models[@]}"
 do
    echo "Creating config file for model '${model}'"
    base=${model%.*}
    file="${basedir}/${base}.yml"
 
-   echo "OnnxModel: ../example-models/onnx/${model}.onnx" > ${file}
-   echo "OutputDir: ${basedir}/${base}-${xilinxpart//${sanitizer}/_}-c${clock}-${io}-rf${rf}-${type//${sanitizer}/_}-${strategy}" >> ${file}
+   echo "PytorchModel: ../example-models/pytorch/${model}.pt" > ${file}
+   echo "OutputDir: ${basedir}/${base}-${part//${sanitizer}/_}-c${clock}-${io}-rf${rf}-${type//${sanitizer}/_}-${strategy}" >> ${file}
    echo "ProjectName: myproject" >> ${file}
-   echo "XilinxPart: ${xilinxpart}" >> ${file}
+   echo "Part: ${part}" >> ${file}
    echo "ClockPeriod: ${clock}" >> ${file}
    echo "" >> ${file}
    echo "IOType: ${io}" >> ${file}
    echo "HLSConfig:" >> ${file}
    echo "  Model:" >> ${file}
    echo "    ReuseFactor: ${rf}" >> ${file}
    echo "    Precision: ${type} " >> ${file}
```

### Comparing `hls4ml-0.6.0/test/pytest/ci-template.yml` & `hls4ml-0.7.0rc1/test/pytest/ci-template.yml`

 * *Files 12% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 .pytest:
   stage: test
-  image: gitlab-registry.cern.ch/fastmachinelearning/hls4ml-testing:0.2.base
-  tags: 
+  image: gitlab-registry.cern.ch/fastmachinelearning/hls4ml-testing:0.4.base
+  tags:
     - docker
   before_script:
     - source ~/.bashrc
     - if [ $EXAMPLEMODEL == 1 ]; then git submodule init; git submodule update; fi
     - conda activate hls4ml-testing
     - pip install .[profiling]
   script:
     - cd test/pytest
     - pytest $PYTESTFILE -rA --cov-report xml --cov-report term --cov=hls4ml --junitxml=report.xml --randomly-seed=42 --randomly-dont-reorganize --randomly-dont-reset-seed
   artifacts:
     when: always
     reports:
-      junit: 
+      junit:
         - test/pytest/report.xml
-      cobertura:
-        - test/pytest/coverage.xml
+      coverage_report:
+        coverage_format: cobertura
+        path: test/pytest/coverage.xml
     paths:
-      - test/pytest/hls4mlprj*.tar.gz
+      - test/pytest/hls4mlprj*.tar.gz
```

### Comparing `hls4ml-0.6.0/test/pytest/generate_ci_yaml.py` & `hls4ml-0.7.0rc1/test/pytest/generate_ci_yaml.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,33 +1,36 @@
-import yaml
 import glob
 
+import yaml
+
 '''
 Create a Gitlab CI yml file with a separate entry for each test_* file
 in the pytests directory to parallelise the CI jobs.
 '''
 
 template = """
 pytest.{}:
   extends: .pytest
   variables:
     PYTESTFILE: {}
     EXAMPLEMODEL: {}
 """
 
+
 def uses_example_model(test_filename):
-    with open(test_filename, 'r') as f:
+    with open(test_filename) as f:
         content = f.read()
-        return 'example-models/' in content
+        return 'example-models' in content
+
 
 yml = None
 tests = glob.glob('test_*.py')
 for test in tests:
-    name = test.replace('test_','').replace('.py','')
-    new_yml = yaml.safe_load(template.format(name, 'test_{}.py'.format(name), int(uses_example_model(test))))
+    name = test.replace('test_', '').replace('.py', '')
+    new_yml = yaml.safe_load(template.format(name, f'test_{name}.py', int(uses_example_model(test))))
     if yml is None:
         yml = new_yml
     else:
         yml.update(new_yml)
 
 yamlfile = open('pytests.yml', 'w')
 yaml.safe_dump(yml, yamlfile)
```

### Comparing `hls4ml-0.6.0/test/pytest/test_globalpooling1d.py` & `hls4ml-0.7.0rc1/test/pytest/test_zeropadding.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,57 +1,73 @@
+from pathlib import Path
+
+import numpy as np
 import pytest
+from tensorflow.keras.layers import ZeroPadding1D, ZeroPadding2D
 from tensorflow.keras.models import Sequential
-from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D
-import numpy as np
+
 import hls4ml
 
+test_root_path = Path(__file__).parent
 
-in_shape = 8
+in_height = 6
+in_width = 8
 in_feat = 4
+
+pad_t = 1
+pad_b = 2
+pad_l = 3
+pad_r = 4
+
 atol = 5e-3
 
+
 @pytest.fixture(scope='module')
-def data():
-    X = np.random.rand(100, in_shape, in_feat)
+def data_1d():
+    X = np.random.rand(100, in_width, in_feat)
     return X
 
 
 @pytest.fixture(scope='module')
-def keras_model_max():
+def data_2d():
+    X = np.random.rand(100, in_height, in_width, in_feat)
+    return X
+
+
+@pytest.fixture(scope='module')
+def keras_model_1d():
     model = Sequential()
-    model.add(GlobalMaxPooling1D(input_shape=(in_shape, in_feat)))
+    model.add(ZeroPadding1D(input_shape=(in_width, in_feat), padding=(pad_l, pad_r)))
     model.compile()
     return model
 
+
 @pytest.fixture(scope='module')
-def keras_model_ave():
+def keras_model_2d():
     model = Sequential()
-    model.add(GlobalAveragePooling1D(input_shape=(in_shape, in_feat)))
+    model.add(ZeroPadding2D(input_shape=(in_height, in_width, in_feat), padding=((pad_t, pad_b), (pad_l, pad_r))))
     model.compile()
     return model
 
-  
-@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
-@pytest.mark.parametrize('model_type', ['max', 'ave'])
-def test_global_pool1d(keras_model_max, keras_model_ave, data, model_type, io_type):
-    if model_type == 'ave':
-        model = keras_model_ave
+
+@pytest.mark.parametrize('io_type', ['io_stream', 'io_parallel'])
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('model_type', ['1d', '2d'])
+def test_zeropadding(keras_model_1d, keras_model_2d, data_1d, data_2d, model_type, io_type, backend):
+    if model_type == '1d':
+        model = keras_model_1d
+        data = data_1d
     else:
-        model = keras_model_max
-    config = hls4ml.utils.config_from_keras_model(model, 
-                                                  default_precision='ap_fixed<32,1>',
-                                                  granularity='name')
-    if model_type == 'ave':
-        config['LayerName']['global_average_pooling1d']['accum_t'] = 'ap_fixed<32,6>'
-
-    hls_model = hls4ml.converters.convert_from_keras_model(model,
-                                                           hls_config=config,
-                                                           io_type=io_type,
-                                                           output_dir=f'hls4mlprj_globalplool1d_{model_type}_{io_type}',
-                                                           part='xcvu9p-flgb2104-2-i')
+        model = keras_model_2d
+        data = data_2d
+
+    config = hls4ml.utils.config_from_keras_model(model, default_precision='ap_fixed<32,1>', granularity='name')
+    odir = str(test_root_path / f'hls4mlprj_zeropadding_{model_type}_{backend}_{io_type}')
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, io_type=io_type, output_dir=odir, backend=backend
+    )
     hls_model.compile()
-    
 
     # Predict
-    y_keras = np.squeeze(model.predict(data))
-    y_hls = hls_model.predict(data)
+    y_keras = model.predict(data).flatten()
+    y_hls = hls_model.predict(data).flatten()
     np.testing.assert_allclose(y_keras, y_hls, rtol=0, atol=atol, verbose=True)
```

### Comparing `hls4ml-0.6.0/test/pytest/test_keras_api.py` & `hls4ml-0.7.0rc1/test/pytest/test_keras_api.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,45 +1,64 @@
+import math
+from pathlib import Path
+
+import numpy as np
 import pytest
-import hls4ml
 import tensorflow as tf
-import numpy as np
-from tensorflow.keras import optimizers
-from tensorflow.keras.layers import Input, Dense, Activation, Conv1D, Conv2D, \
-                                    Reshape, ELU, LeakyReLU, ThresholdedReLU, \
-                                    PReLU, BatchNormalization, Add, Subtract, \
-                                    Multiply, Average, Maximum, Minimum, Concatenate, \
-                                    MaxPooling1D, MaxPooling2D, AveragePooling1D, \
-                                    AveragePooling2D
-import math
-from tensorflow.keras import backend as K
+from tensorflow.keras.layers import (
+    ELU,
+    Activation,
+    AveragePooling1D,
+    AveragePooling2D,
+    Conv1D,
+    Conv2D,
+    Dense,
+    LeakyReLU,
+    MaxPooling1D,
+    MaxPooling2D,
+    PReLU,
+)
+
+import hls4ml
+
+test_root_path = Path(__file__).parent
 
-def test_dense():
+
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_dense(backend, io_type):
     model = tf.keras.models.Sequential()
-    model.add(Dense(2,
-              input_shape=(1,),
-              name='Dense',
-              use_bias=True,
-              kernel_initializer= tf.keras.initializers.RandomUniform(minval=1, maxval=10),
-              bias_initializer='zeros',
-              kernel_regularizer=None,
-              bias_regularizer=None,
-              activity_regularizer=None,
-              kernel_constraint=None,
-              bias_constraint=None))
+    model.add(
+        Dense(
+            2,
+            input_shape=(1,),
+            name='Dense',
+            use_bias=True,
+            kernel_initializer=tf.keras.initializers.RandomUniform(minval=1, maxval=10),
+            bias_initializer='zeros',
+            kernel_regularizer=None,
+            bias_regularizer=None,
+            activity_regularizer=None,
+            kernel_constraint=None,
+            bias_constraint=None,
+        )
+    )
     model.add(Activation(activation='elu', name='Activation'))
     model.compile(optimizer='adam', loss='mse')
 
-    X_input = np.random.rand(100,1)
+    X_input = np.random.rand(100, 1)
 
     keras_prediction = model.predict(X_input)
 
     config = hls4ml.utils.config_from_keras_model(model)
-    output_dir = 'hls4mlprj_keras_api_dense'
+    output_dir = str(test_root_path / f'hls4mlprj_keras_api_dense_{backend}_{io_type}')
 
-    hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir=output_dir)
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
 
     hls_model.compile()
 
     hls_prediction = hls_model.predict(X_input)
 
     np.testing.assert_allclose(hls_prediction, keras_prediction, rtol=1e-2, atol=0.01)
 
@@ -49,304 +68,358 @@
     assert list(hls_model.get_layers())[2].attributes['class_name'] == model.layers[1]._name
     assert list(hls_model.get_layers())[0].attributes['input_shape'] == list(model.layers[0].input_shape[1:])
     assert list(hls_model.get_layers())[1].attributes['n_in'] == model.layers[0].input_shape[1:][0]
     assert list(hls_model.get_layers())[1].attributes['n_out'] == model.layers[0].output_shape[1:][0]
     assert list(hls_model.get_layers())[2].attributes['activation'] == str(model.layers[1].activation).split()[1]
     assert list(hls_model.get_layers())[1].attributes['activation'] == str(model.layers[0].activation).split()[1]
 
+
 # TODO: add ThresholdedReLU test when it can be made to pass
 # https://github.com/fastmachinelearning/hls4ml/issues/376
-@pytest.mark.parametrize("activation_function", [Activation(activation='relu', name='Activation'),
-                                                 LeakyReLU(alpha=1.0),
-                                                 ELU(alpha=1.0),
-                                                 PReLU(alpha_initializer="zeros",),])
-                                                 #ThresholdedReLU(theta=1.0)])
-def test_activations(activation_function):
+@pytest.mark.parametrize(
+    "activation_function",
+    [
+        Activation(activation='relu', name='Activation'),
+        LeakyReLU(alpha=1.0),
+        ELU(alpha=1.0),
+        PReLU(
+            alpha_initializer="zeros",
+        ),
+        Activation(activation='sigmoid', name='Activation'),
+    ],
+)
+# ThresholdedReLU(theta=1.0)])
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_activations(activation_function, backend, io_type):
     model = tf.keras.models.Sequential()
-    model.add(Dense(64,
-              input_shape=(1,),
-              name='Dense',
-              kernel_initializer='lecun_uniform',
-              kernel_regularizer=None))
+    model.add(Dense(64, input_shape=(1,), name='Dense', kernel_initializer='lecun_uniform', kernel_regularizer=None))
     model.add(activation_function)
 
     model.compile(optimizer='adam', loss='mse')
-    X_input = np.random.rand(100,1)
+    X_input = np.random.rand(100, 1)
     keras_prediction = model.predict(X_input)
     config = hls4ml.utils.config_from_keras_model(model)
-    output_dir = 'hls4mlprj_keras_api_activations_{}'.format(activation_function.__class__.__name__)
-    hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir=output_dir)
+    output_dir = str(
+        test_root_path / f'hls4mlprj_keras_api_activations_{activation_function.__class__.__name__}_{backend}_{io_type}'
+    )
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
     hls_model.compile()
     hls_prediction = hls_model.predict(X_input)
 
     np.testing.assert_allclose(hls_prediction, keras_prediction, rtol=1e-2, atol=0.01)
 
     assert len(model.layers) + 1 == len(hls_model.get_layers())
 
     assert list(hls_model.get_layers())[2].attributes['class_name'] == activation_function.__class__.__name__
 
 
-keras_conv1d = [Conv1D]
 padds_options = ['same', 'valid']
-@pytest.mark.parametrize("conv1d", keras_conv1d)
-@pytest.mark.parametrize("padds", padds_options)
-def test_conv1d(conv1d, padds):
+
+
+@pytest.mark.parametrize('padds', padds_options)
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_conv1d(padds, backend, io_type):
     model = tf.keras.models.Sequential()
     input_shape = (10, 128, 4)
-    model.add(conv1d(filters=32,
-                     kernel_size=3,
-                     strides=1,
-                     padding=padds,
-                     activation='relu',
-                     input_shape=input_shape[1:],
-                     kernel_initializer='normal',
-                     use_bias=False,
-                     data_format='channels_last'))
+    model.add(
+        Conv1D(
+            filters=32,
+            kernel_size=3,
+            strides=1,
+            padding=padds,
+            activation='relu',
+            input_shape=input_shape[1:],
+            kernel_initializer='normal',
+            use_bias=False,
+            data_format='channels_last',
+        )
+    )
     model.add(Activation(activation='relu'))
-
     model.compile(optimizer='adam', loss='mse')
-    X_input = np.random.rand(10,128,4)
+
+    X_input = np.random.rand(10, 128, 4)
     keras_prediction = model.predict(X_input)
+
     config = hls4ml.utils.config_from_keras_model(model)
-    output_dir = 'hls4mlprj_keras_api_conv1d_{}'.format(padds)
-    hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir=output_dir)
+    output_dir = str(test_root_path / f'hls4mlprj_keras_api_conv1d_{padds}_{backend}_{io_type}')
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
     hls_model.compile()
-    hls_prediction = hls_model.predict(X_input)
+    hls_prediction = hls_model.predict(X_input).reshape(keras_prediction.shape)
 
-    assert len(model.layers) + 2 == len(hls_model.get_layers())
-    assert list(hls_model.get_layers())[1].attributes['name'] == model.layers[0]._name
+    # 5e-2 might be too high
+    np.testing.assert_allclose(hls_prediction, keras_prediction, rtol=0, atol=5e-2)
 
-    print(list(hls_model.get_layers())[1].attributes)
-    if conv1d == 'Conv1D':
-      assert list(hls_model.get_layers())[1].attributes['class_name'] == 'Conv1D'
-    assert list(hls_model.get_layers())[1].attributes['activation'] == str(model.layers[0].activation).split()[1]
-    assert list(hls_model.get_layers())[1].attributes["in_width"] == model.layers[0]._batch_input_shape[1]
-    assert list(hls_model.get_layers())[1].attributes['filt_width'] == model.layers[0].kernel_size[0]
-    assert list(hls_model.get_layers())[1].attributes['n_chan'] == model.layers[0].input_shape[2]
-    assert list(hls_model.get_layers())[1].attributes['n_filt'] == model.layers[0].filters
-    assert list(hls_model.get_layers())[1].attributes['stride_width'] == model.layers[0].strides[0]
-    assert list(hls_model.get_layers())[1].attributes['padding'] == model.layers[0].padding
-    assert list(hls_model.get_layers())[1].attributes['data_format'] == model.layers[0].data_format
-    assert list(hls_model.get_layers())[1].attributes["out_width"] == list(model.layers[0].output_shape)[1]
+    if not (backend in ['Vivado', 'Vitis'] and io_type == 'io_stream' and padds == 'same'):
+        # Vivado/Vitis inserts and additional layer for 'same' padding in io_stream
+        assert len(model.layers) + 2 == len(hls_model.get_layers())
+        assert list(hls_model.get_layers())[1].attributes['name'] == model.layers[0]._name
+        assert list(hls_model.get_layers())[1].attributes['class_name'] == 'Conv1D'
+        assert list(hls_model.get_layers())[1].attributes['activation'] == str(model.layers[0].activation).split()[1]
+        assert list(hls_model.get_layers())[1].attributes["in_width"] == model.layers[0]._batch_input_shape[1]
+        assert list(hls_model.get_layers())[1].attributes['filt_width'] == model.layers[0].kernel_size[0]
+        assert list(hls_model.get_layers())[1].attributes['n_chan'] == model.layers[0].input_shape[2]
+        assert list(hls_model.get_layers())[1].attributes['n_filt'] == model.layers[0].filters
+        assert list(hls_model.get_layers())[1].attributes['stride_width'] == model.layers[0].strides[0]
+        assert list(hls_model.get_layers())[1].attributes['padding'] == model.layers[0].padding
+        assert list(hls_model.get_layers())[1].attributes['data_format'] == model.layers[0].data_format
+        assert list(hls_model.get_layers())[1].attributes["out_width"] == list(model.layers[0].output_shape)[1]
+
+        out_width = math.ceil(float(model.layers[0]._batch_input_shape[2]) / float(model.layers[0].strides[0]))
+        pad_along_width = max(
+            (out_width - 1) * model.layers[0].strides[0]
+            + model.layers[0].kernel_size[0]
+            - model.layers[0]._batch_input_shape[2],
+            0,
+        )
+        pad_left = pad_along_width // 2
+        pad_right = pad_along_width - pad_left
 
-    out_width	= math.ceil(float(model.layers[0]._batch_input_shape[2]) / float(model.layers[0].strides[0]))
-    pad_along_width	= max((out_width - 1) * model.layers[0].strides[0] + model.layers[0].kernel_size[0] - model.layers[0]._batch_input_shape[2], 0)
-    pad_left = pad_along_width // 2
-    pad_right = pad_along_width - pad_left
+        if model.layers[0].padding == 'same':
+            assert list(hls_model.get_layers())[1].attributes['pad_left'] == pad_left
+            assert list(hls_model.get_layers())[1].attributes['pad_right'] == pad_right
+        elif model.layers[0].padding == 'valid':
+            assert list(hls_model.get_layers())[1].attributes['pad_left'] == 0
+            assert list(hls_model.get_layers())[1].attributes['pad_right'] == 0
 
-    out_valid	= math.ceil(float(model.layers[0]._batch_input_shape[1] - model.layers[0].kernel_size[0] + 1) / float(model.layers[0].strides[0]))
 
-    if model.layers[0].padding == 'same':
-        assert list(hls_model.get_layers())[1].attributes['pad_left'] == pad_left
-        assert list(hls_model.get_layers())[1].attributes['pad_right'] == pad_right
+chans_options = ['channels_last']
+padds_options = ['same', 'valid']
 
-    elif model.layers[0].padding == 'valid':
-        assert list(hls_model.get_layers())[1].attributes['pad_left'] == 0
-        assert list(hls_model.get_layers())[1].attributes['pad_right'] == 0
 
-# DONE
-keras_conv2d = [Conv2D]
-padds_options = ['same', 'valid']
-chans_options = ['channels_last']
-@pytest.mark.parametrize("conv2d", keras_conv2d)
-@pytest.mark.parametrize("chans", chans_options)
-@pytest.mark.parametrize("padds", padds_options)
-def test_conv2d(conv2d, chans, padds):
+@pytest.mark.parametrize('chans', chans_options)
+@pytest.mark.parametrize('padds', padds_options)
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_conv2d(chans, padds, backend, io_type):
     model = tf.keras.models.Sequential()
-    input_shape = (28,28,3)
-    model.add(conv2d(filters=32,
-                     kernel_size=(4,4),
-                     strides=(4,4),
-                     padding=padds,
-                     input_shape=input_shape,
-                     kernel_initializer='normal',
-                     use_bias=False,
-                     data_format=chans
-                     ))
-
+    input_shape = (28, 28, 3)
+    model.add(
+        Conv2D(
+            filters=32,
+            kernel_size=(4, 4),
+            strides=(4, 4),
+            padding=padds,
+            input_shape=input_shape,
+            kernel_initializer='normal',
+            use_bias=False,
+            data_format=chans,
+        )
+    )
     model.compile(optimizer='adam', loss='mse')
+
     X_input = np.random.rand(100, *input_shape)
     keras_prediction = model.predict(X_input)
+
     config = hls4ml.utils.config_from_keras_model(model)
-    output_dir = 'hls4mlprj_keras_api_conv2d_{}_{}'.format(chans, padds)
-    hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir=output_dir)
+    output_dir = str(test_root_path / f'hls4mlprj_keras_api_conv2d_{backend}_{chans}_{padds}_{io_type}')
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
     hls_model.compile()
     hls_prediction = hls_model.predict(X_input).reshape(keras_prediction.shape)
 
+    # A high tolerance, simply to verify correct functionality
+    np.testing.assert_allclose(hls_prediction, keras_prediction, rtol=0, atol=5e-2)
+
     assert len(model.layers) + 1 == len(hls_model.get_layers())
     assert list(hls_model.get_layers())[1].attributes['name'] == model.layers[0]._name
-
-    if conv2d == 'Conv2D':
-      assert list(hls_model.get_layers())[1].attributes['class_name'] == 'Conv2D'
+    assert list(hls_model.get_layers())[1].attributes['class_name'] == 'Conv2D'
     assert list(hls_model.get_layers())[1].attributes['activation'] == str(model.layers[0].activation).split()[1]
     assert list(hls_model.get_layers())[1].attributes['filt_width'] == model.layers[0].kernel_size[1]
     assert list(hls_model.get_layers())[1].attributes['filt_height'] == model.layers[0].kernel_size[0]
     assert list(hls_model.get_layers())[1].attributes['n_filt'] == model.layers[0].filters
     assert list(hls_model.get_layers())[1].attributes['stride_width'] == model.layers[0].strides[1]
     assert list(hls_model.get_layers())[1].attributes['stride_height'] == model.layers[0].strides[0]
     assert list(hls_model.get_layers())[1].attributes['padding'] == model.layers[0].padding
     assert list(hls_model.get_layers())[1].attributes['data_format'] == model.layers[0].data_format
 
     if model.layers[0].data_format == 'channels_first':
-      assert list(hls_model.get_layers())[1].attributes['n_chan'] == model.layers[0]._batch_input_shape[1]
-      assert list(hls_model.get_layers())[1].attributes['in_height'] == model.layers[0]._batch_input_shape[2]
-      assert list(hls_model.get_layers())[1].attributes['in_width'] == model.layers[0]._batch_input_shape[3]
-      assert list(hls_model.get_layers())[1].attributes['out_height'] == model.layers[0].output_shape[2]
-      assert list(hls_model.get_layers())[1].attributes['out_width'] == model.layers[0].output_shape[3]
+        assert list(hls_model.get_layers())[1].attributes['n_chan'] == model.layers[0]._batch_input_shape[1]
+        assert list(hls_model.get_layers())[1].attributes['in_height'] == model.layers[0]._batch_input_shape[2]
+        assert list(hls_model.get_layers())[1].attributes['in_width'] == model.layers[0]._batch_input_shape[3]
+        assert list(hls_model.get_layers())[1].attributes['out_height'] == model.layers[0].output_shape[2]
+        assert list(hls_model.get_layers())[1].attributes['out_width'] == model.layers[0].output_shape[3]
     elif model.layers[0].data_format == 'channels_last':
-      assert list(hls_model.get_layers())[1].attributes['n_chan'] == model.layers[0]._batch_input_shape[3]
-      assert list(hls_model.get_layers())[1].attributes['in_height'] == model.layers[0]._batch_input_shape[1]
-      assert list(hls_model.get_layers())[1].attributes['in_width'] == model.layers[0]._batch_input_shape[2]
-      assert list(hls_model.get_layers())[1].attributes['out_height'] == model.layers[0].output_shape[1]
-      assert list(hls_model.get_layers())[1].attributes['out_width'] == model.layers[0].output_shape[2]
-
-    if model.layers[0].padding =='same':
-      if model.layers[0].data_format == 'channels_first':
-        out_height	= model.layers[0].output_shape[2]
-        out_width	= model.layers[0].output_shape[3]
-        pad_along_height	= max((out_height - 1) * model.layers[0].strides[0] + model.layers[0].kernel_size[0] - model.layers[0]._batch_input_shape[2], 0)
-        pad_along_width	= max((out_width - 1) * model.layers[0].strides[1] + model.layers[0].kernel_size[1] - model.layers[0]._batch_input_shape[3], 0)
-
-      elif model.layers[0].data_format == 'channels_last':
-        out_height	= model.layers[0].output_shape[1]
-        out_width	= model.layers[0].output_shape[2]
-        pad_along_height	= max((out_height - 1) * model.layers[0].strides[0] + model.layers[0].kernel_size[0] - model.layers[0]._batch_input_shape[1], 0)
-        pad_along_width	= max((out_width - 1) * model.layers[0].strides[1] + model.layers[0].kernel_size[1] - model.layers[0]._batch_input_shape[2], 0)
-
-      pad_top	= pad_along_height // 2
-      pad_bottom	= pad_along_height - pad_top
-      pad_left	= pad_along_width // 2
-      pad_right	= pad_along_width - pad_left
-      assert list(hls_model.get_layers())[1].attributes['pad_top'] == pad_top
-      assert list(hls_model.get_layers())[1].attributes['pad_bottom'] == pad_bottom
-      assert list(hls_model.get_layers())[1].attributes['pad_left'] == pad_left
-      assert list(hls_model.get_layers())[1].attributes['pad_right'] == pad_right
-
-    elif model.layers[0].padding =='valid':
-      assert list(hls_model.get_layers())[1].attributes['pad_top'] == 0
-      assert list(hls_model.get_layers())[1].attributes['pad_bottom'] == 0
-      assert list(hls_model.get_layers())[1].attributes['pad_left'] == 0
-      assert list(hls_model.get_layers())[1].attributes['pad_right'] == 0
+        assert list(hls_model.get_layers())[1].attributes['n_chan'] == model.layers[0]._batch_input_shape[3]
+        assert list(hls_model.get_layers())[1].attributes['in_height'] == model.layers[0]._batch_input_shape[1]
+        assert list(hls_model.get_layers())[1].attributes['in_width'] == model.layers[0]._batch_input_shape[2]
+        assert list(hls_model.get_layers())[1].attributes['out_height'] == model.layers[0].output_shape[1]
+        assert list(hls_model.get_layers())[1].attributes['out_width'] == model.layers[0].output_shape[2]
+
+    if model.layers[0].padding == 'same':
+        if model.layers[0].data_format == 'channels_first':
+            out_height = model.layers[0].output_shape[2]
+            out_width = model.layers[0].output_shape[3]
+            pad_along_height = max(
+                (out_height - 1) * model.layers[0].strides[0]
+                + model.layers[0].kernel_size[0]
+                - model.layers[0]._batch_input_shape[2],
+                0,
+            )
+            pad_along_width = max(
+                (out_width - 1) * model.layers[0].strides[1]
+                + model.layers[0].kernel_size[1]
+                - model.layers[0]._batch_input_shape[3],
+                0,
+            )
+        elif model.layers[0].data_format == 'channels_last':
+            out_height = model.layers[0].output_shape[1]
+            out_width = model.layers[0].output_shape[2]
+            pad_along_height = max(
+                (out_height - 1) * model.layers[0].strides[0]
+                + model.layers[0].kernel_size[0]
+                - model.layers[0]._batch_input_shape[1],
+                0,
+            )
+            pad_along_width = max(
+                (out_width - 1) * model.layers[0].strides[1]
+                + model.layers[0].kernel_size[1]
+                - model.layers[0]._batch_input_shape[2],
+                0,
+            )
+        pad_top = pad_along_height // 2
+        pad_bottom = pad_along_height - pad_top
+        pad_left = pad_along_width // 2
+        pad_right = pad_along_width - pad_left
+        assert list(hls_model.get_layers())[1].attributes['pad_top'] == pad_top
+        assert list(hls_model.get_layers())[1].attributes['pad_bottom'] == pad_bottom
+        assert list(hls_model.get_layers())[1].attributes['pad_left'] == pad_left
+        assert list(hls_model.get_layers())[1].attributes['pad_right'] == pad_right
+    elif model.layers[0].padding == 'valid':
+        assert list(hls_model.get_layers())[1].attributes['pad_top'] == 0
+        assert list(hls_model.get_layers())[1].attributes['pad_bottom'] == 0
+        assert list(hls_model.get_layers())[1].attributes['pad_left'] == 0
+        assert list(hls_model.get_layers())[1].attributes['pad_right'] == 0
+
 
 pooling_layers = [MaxPooling1D, MaxPooling2D, AveragePooling1D, AveragePooling2D]
-@pytest.mark.parametrize("pooling", pooling_layers)
-@pytest.mark.parametrize("padds", padds_options)
-@pytest.mark.parametrize("chans", chans_options)
-def test_pooling(pooling, padds, chans):
+
+
+@pytest.mark.parametrize('pooling', pooling_layers)
+@pytest.mark.parametrize('padds', padds_options)
+@pytest.mark.parametrize('chans', chans_options)
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+def test_pooling(pooling, padds, chans, backend):
     assert '1D' in pooling.__name__ or '2D' in pooling.__name__
-    input_shape = (8,8,3) if '2D' in pooling.__name__ else (64,3)
-    model = tf.keras.models.Sequential()
-    if '2D' in pooling.__name__:
-        model.add(Conv2D(1, (3,3), input_shape=input_shape, 
-                         padding=padds, data_format=chans))
-    elif '1D' in pooling.__name__:
-        model.add(Conv1D(filters=32, kernel_size=3, strides=1, padding=padds,
-                      input_shape=input_shape, kernel_initializer='normal', use_bias=False,
-                      data_format=chans))  
-    
-    pool_size = (2, 2) if '2D' in pooling.__name__ else 2
-    model.add(pooling(pool_size=pool_size, strides=None, padding=padds, data_format=None))
-    model.compile(optimizer='adam', loss='mse')
 
+    input_shape = (18, 15, 3) if '2D' in pooling.__name__ else (121, 3)
     X_input = np.random.rand(100, *input_shape)
-    keras_prediction = model.predict(X_input)
-    config = hls4ml.utils.config_from_keras_model(model)
-    output_dir = 'hls4mlprj_keras_api_pooling_{}_{}_{}'.format(pooling.__name__, chans, padds)
 
-    hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir=output_dir)
+    keras_model = tf.keras.models.Sequential()
+    keras_model.add(pooling(padding=padds, input_shape=input_shape))
+    keras_model.compile()
+
+    hls_cfg = hls4ml.utils.config_from_keras_model(keras_model)
+    output_dir = str(
+        test_root_path / f'hls4mlprj_keras_api_pooling_{pooling.__name__}_channels_{chans}_padds_{padds}_backend_{backend}'
+    )
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        keras_model, hls_config=hls_cfg, output_dir=output_dir, backend=backend
+    )
     hls_model.compile()
+
+    # Verify accuracy
+    keras_prediction = keras_model.predict(X_input)
     hls_prediction = hls_model.predict(X_input).reshape(keras_prediction.shape)
+    np.testing.assert_allclose(hls_prediction, keras_prediction, rtol=0, atol=3e-2)
 
+    # Verify correct parsing of layer
     hls_pool = list(hls_model.get_layers())[-1]
-    ker_pool = model.layers[-1]
+    ker_pool = keras_model.layers[-1]
     if '2D' in pooling.__name__:
-      assert hls_pool.attributes['name'] == ker_pool._name
-      assert hls_pool.attributes['class_name'][-2] == str(2)
-      assert hls_pool.attributes['stride_height'] == ker_pool.strides[0]
-      assert hls_pool.attributes['stride_width'] == ker_pool.strides[1]
-      assert hls_pool.attributes['pool_height'] == ker_pool.pool_size[1]
-      assert hls_pool.attributes['pool_width'] == ker_pool.pool_size[0]
-      assert hls_pool.attributes['padding'] == ker_pool.padding
-
-      if hls_pool.attributes['data_format'] == 'channels_last':
-        assert hls_pool.attributes['in_height'] == ker_pool.input_shape[1]
-        assert hls_pool.attributes['in_width'] == ker_pool.input_shape[2]
-        assert hls_pool.attributes['n_filt'] == ker_pool.input_shape[3]
-      elif hls_pool.attributes['data_format'] == 'channels_first':
-        assert hls_pool.attributes['in_height'] == ker_pool.input_shape[2]
-        assert hls_pool.attributes['in_width'] == ker_pool.input_shape[3]
-        assert hls_pool.attributes['n_filt'] == ker_pool.input_shape[1]
-
-      if hls_pool.attributes['padding'] == 'same':
-        # Height
-        in_height = ker_pool.input_shape[1]
-        if ker_pool.data_format == 'channels_first':
-          in_height = ker_pool.input_shape[2]
-        out_height = int(math.ceil(float(in_height) / float(ker_pool.strides[0])))
-        assert out_height == hls_pool.attributes['out_height']
-        if in_height % ker_pool.strides[0] == 0:
-          pad_along_height = max(ker_pool.pool_size[1] - ker_pool.strides[0], 0)
-        else:
-          pad_along_height = max(ker_pool.pool_size[1] - (in_height % ker_pool.strides[0]), 0)
-        pad_top = pad_along_height // 2
-        pad_bottom = pad_along_height - pad_top
-        assert pad_bottom == hls_pool.attributes['pad_bottom']
-        assert pad_top == hls_pool.attributes['pad_top']
-
-        # Width
-        in_width = ker_pool.input_shape[2]
-        if ker_pool.data_format == 'channels_first':
-          in_height = model.layers[1].input_shape[-1]
-        out_width = int(math.ceil(float(in_width) / float(ker_pool.strides[1])))
-        assert out_width == hls_pool.attributes['out_width']
-        if in_width % ker_pool.strides[1] == 0:
-          pad_along_width = max(ker_pool.pool_size[0] - ker_pool.strides[1], 0)
-        else:
-          pad_along_width = max(ker_pool.pool_size[0] - (in_width % ker_pool.strides[1]), 0)
-        pad_left = pad_along_width // 2
-        pad_right = pad_along_width - pad_left
-        assert pad_left == hls_pool.attributes['pad_left']
-        assert pad_right == hls_pool.attributes['pad_right']
-
-      elif hls_pool.attributes['padding'] == 'valid':
-        if hls_pool.attributes['data_format'] == 'channels_first':
-          in_height = ker_pool.input_shape[2]
-          in_width = ker_pool.input_shape[3]
-        elif hls_pool.attributes['data_format'] == 'channels_last':
-          in_height = ker_pool.input_shape[1]
-          in_width = ker_pool.input_shape[2]
-
-        out_width = int(math.ceil(float(in_width - ker_pool.pool_size[0] + 1) / float(ker_pool.strides[1])))
-        out_height = int(math.ceil(float(in_height - ker_pool.pool_size[1] + 1) / float(ker_pool.strides[0])))
-
-        assert hls_pool.attributes['out_height'] == out_height
-        assert hls_pool.attributes['out_width'] == out_width
-        assert hls_pool.attributes['pad_top'] == 0
-        assert hls_pool.attributes['pad_bottom'] == 0
-        assert hls_pool.attributes['pad_left'] == 0
-        assert hls_pool.attributes['pad_right'] == 0
+        assert hls_pool.attributes['name'] == ker_pool._name
+        assert hls_pool.attributes['class_name'][-2] == str(2)
+        assert hls_pool.attributes['stride_height'] == ker_pool.strides[0]
+        assert hls_pool.attributes['stride_width'] == ker_pool.strides[1]
+        assert hls_pool.attributes['pool_height'] == ker_pool.pool_size[1]
+        assert hls_pool.attributes['pool_width'] == ker_pool.pool_size[0]
+        assert hls_pool.attributes['padding'] == ker_pool.padding
+
+        if hls_pool.attributes['data_format'] == 'channels_last':
+            assert hls_pool.attributes['in_height'] == ker_pool.input_shape[1]
+            assert hls_pool.attributes['in_width'] == ker_pool.input_shape[2]
+            assert hls_pool.attributes['n_filt'] == ker_pool.input_shape[3]
+        elif hls_pool.attributes['data_format'] == 'channels_first':
+            assert hls_pool.attributes['in_height'] == ker_pool.input_shape[2]
+            assert hls_pool.attributes['in_width'] == ker_pool.input_shape[3]
+            assert hls_pool.attributes['n_filt'] == ker_pool.input_shape[1]
+
+        if hls_pool.attributes['padding'] == 'same':
+            # Height
+            in_height = ker_pool.input_shape[1]
+            if ker_pool.data_format == 'channels_first':
+                in_height = ker_pool.input_shape[2]
+            out_height = int(math.ceil(float(in_height) / float(ker_pool.strides[0])))
+            assert out_height == hls_pool.attributes['out_height']
+            if in_height % ker_pool.strides[0] == 0:
+                pad_along_height = max(ker_pool.pool_size[1] - ker_pool.strides[0], 0)
+            else:
+                pad_along_height = max(ker_pool.pool_size[1] - (in_height % ker_pool.strides[0]), 0)
+            pad_top = pad_along_height // 2
+            pad_bottom = pad_along_height - pad_top
+            assert pad_bottom == hls_pool.attributes['pad_bottom']
+            assert pad_top == hls_pool.attributes['pad_top']
+
+            # Width
+            in_width = ker_pool.input_shape[2]
+            if ker_pool.data_format == 'channels_first':
+                in_height = keras_model.layers[1].input_shape[-1]
+            out_width = int(math.ceil(float(in_width) / float(ker_pool.strides[1])))
+            assert out_width == hls_pool.attributes['out_width']
+            if in_width % ker_pool.strides[1] == 0:
+                pad_along_width = max(ker_pool.pool_size[0] - ker_pool.strides[1], 0)
+            else:
+                pad_along_width = max(ker_pool.pool_size[0] - (in_width % ker_pool.strides[1]), 0)
+            pad_left = pad_along_width // 2
+            pad_right = pad_along_width - pad_left
+            assert pad_left == hls_pool.attributes['pad_left']
+            assert pad_right == hls_pool.attributes['pad_right']
+
+        elif hls_pool.attributes['padding'] == 'valid':
+            if hls_pool.attributes['data_format'] == 'channels_first':
+                in_height = ker_pool.input_shape[2]
+                in_width = ker_pool.input_shape[3]
+            elif hls_pool.attributes['data_format'] == 'channels_last':
+                in_height = ker_pool.input_shape[1]
+                in_width = ker_pool.input_shape[2]
+
+            out_width = int(math.ceil(float(in_width - ker_pool.pool_size[0] + 1) / float(ker_pool.strides[1])))
+            out_height = int(math.ceil(float(in_height - ker_pool.pool_size[1] + 1) / float(ker_pool.strides[0])))
+
+            assert hls_pool.attributes['out_height'] == out_height
+            assert hls_pool.attributes['out_width'] == out_width
+            assert hls_pool.attributes['pad_top'] == 0
+            assert hls_pool.attributes['pad_bottom'] == 0
+            assert hls_pool.attributes['pad_left'] == 0
+            assert hls_pool.attributes['pad_right'] == 0
 
     elif '1D' in pooling.__name__:
-      assert hls_pool.attributes['name'] == ker_pool._name
-      assert hls_pool.attributes['class_name'][-2] == str(1)
-      assert hls_pool.attributes['n_in'] == ker_pool.input_shape[1]
-      assert hls_pool.attributes['n_filt'] == ker_pool.input_shape[2]
-      assert hls_pool.attributes['pool_width'] == ker_pool.pool_size[0]
-      assert hls_pool.attributes['stride_width'] == ker_pool.strides[0]
-      assert hls_pool.attributes['padding'] == ker_pool.padding
-
-      out_same	= math.ceil(float(ker_pool.input_shape[1]) / float(ker_pool.strides[0]))
-      out_valid	= math.ceil(float(ker_pool.input_shape[1] - ker_pool.pool_size[0] + 1) / ker_pool.strides[0])
-
-      if hls_pool.attributes['padding'] == 'same':
-        assert hls_pool.attributes['n_out'] == out_same
-        if ker_pool.input_shape[1] % ker_pool.strides[0] == 0:
-          pad_along_width = max(ker_pool.pool_size[0] - ker_pool.strides[0], 0)
-        else:
-          pad_along_width = max(ker_pool.pool_size[0] - (ker_pool.input_shape[1] % ker_pool.strides[0]), 0)
-        assert hls_pool.attributes['pad_left'] == pad_along_width // 2
-        assert hls_pool.attributes['pad_right'] == pad_along_width - pad_along_width // 2
-
-      elif hls_pool.attributes['padding'] == 'valid':
-        assert hls_pool.attributes['n_out'] == out_valid
-        assert hls_pool.attributes['pad_left'] == 0
-        assert hls_pool.attributes['pad_right'] == 0
+        assert hls_pool.attributes['name'] == ker_pool._name
+        assert hls_pool.attributes['class_name'][-2] == str(1)
+        assert hls_pool.attributes['n_in'] == ker_pool.input_shape[1]
+        assert hls_pool.attributes['n_filt'] == ker_pool.input_shape[2]
+        assert hls_pool.attributes['pool_width'] == ker_pool.pool_size[0]
+        assert hls_pool.attributes['stride_width'] == ker_pool.strides[0]
+        assert hls_pool.attributes['padding'] == ker_pool.padding
+
+        out_same = math.ceil(float(ker_pool.input_shape[1]) / float(ker_pool.strides[0]))
+        out_valid = math.ceil(float(ker_pool.input_shape[1] - ker_pool.pool_size[0] + 1) / ker_pool.strides[0])
+
+        if hls_pool.attributes['padding'] == 'same':
+            assert hls_pool.attributes['n_out'] == out_same
+            if ker_pool.input_shape[1] % ker_pool.strides[0] == 0:
+                pad_along_width = max(ker_pool.pool_size[0] - ker_pool.strides[0], 0)
+            else:
+                pad_along_width = max(ker_pool.pool_size[0] - (ker_pool.input_shape[1] % ker_pool.strides[0]), 0)
+            assert hls_pool.attributes['pad_left'] == pad_along_width // 2
+            assert hls_pool.attributes['pad_right'] == pad_along_width - pad_along_width // 2
+
+        elif hls_pool.attributes['padding'] == 'valid':
+            assert hls_pool.attributes['n_out'] == out_valid
+            assert hls_pool.attributes['pad_left'] == 0
+            assert hls_pool.attributes['pad_right'] == 0
```

### Comparing `hls4ml-0.6.0/test/pytest/test_qkeras.py` & `hls4ml-0.7.0rc1/test/pytest/test_qkeras.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,224 +1,350 @@
-import pytest
-import hls4ml
+import warnings
+from pathlib import Path
+
 import numpy as np
-from tensorflow.keras.utils import to_categorical
+import pytest
+from qkeras.qlayers import QActivation, QDense
+from qkeras.quantizers import binary, quantized_bits, quantized_relu, quantized_sigmoid, quantized_tanh, ternary
+from qkeras.utils import _add_supported_quantized_objects
 from sklearn.datasets import fetch_openml
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import LabelEncoder, StandardScaler
-from tensorflow.keras.models import Sequential, model_from_json
-from tensorflow.keras.optimizers import Adam
-from tensorflow.keras.regularizers import l1
-from tensorflow.keras.layers import Activation, BatchNormalization
-from qkeras.qlayers import QDense, QActivation
-from qkeras.quantizers import quantized_bits, quantized_relu, ternary, binary
-from qkeras.utils import _add_supported_quantized_objects; co = {}; _add_supported_quantized_objects(co)
+from tensorflow.keras.layers import BatchNormalization, Input
+from tensorflow.keras.models import Model, Sequential, model_from_json
+from tensorflow.keras.utils import to_categorical
+
+import hls4ml
+
+co = {}
+_add_supported_quantized_objects(co)
+
 
-import warnings
 warnings.filterwarnings("ignore", message="numpy.dtype size changed")
 warnings.filterwarnings("ignore", message="numpy.ufunc size changed")
 
+test_root_path = Path(__file__).parent
+example_model_path = (test_root_path / '../../example-models').resolve()
+
+
 @pytest.fixture(scope='module')
 def get_jettagging_data():
-  '''
-  Download the jet tagging dataset
-  '''
-  print("Fetching data from openml")
-  data = fetch_openml('hls4ml_lhc_jets_hlf')
-  X, y = data['data'], data['target']
-  le = LabelEncoder()
-  y = le.fit_transform(y)
-  y = to_categorical(y, 5)
-  X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
-  scaler = StandardScaler()
-  X_train_val = scaler.fit_transform(X_train_val)
-  X_test = scaler.transform(X_test)
-  return X_train_val, X_test, y_train_val, y_test
+    '''
+    Download the jet tagging dataset
+    '''
+    print("Fetching data from openml")
+    data = fetch_openml('hls4ml_lhc_jets_hlf')
+    X, y = data['data'], data['target']
+    le = LabelEncoder()
+    y = le.fit_transform(y)
+    y = to_categorical(y, 5)
+    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
+    scaler = StandardScaler()
+    X_train_val = scaler.fit_transform(X_train_val)
+    X_test = scaler.transform(X_test)
+    return X_train_val, X_test, y_train_val, y_test
+
 
 @pytest.fixture(scope='module')
 def load_jettagging_model():
-  ''' 
-  Load the 3 hidden layer QKeras example model trained on the jet tagging dataset
-  '''
-  jsons = open('../../example-models/keras/qkeras_3layer.json','r').read()
-  model = model_from_json(jsons, custom_objects=co)
-  model.load_weights('../../example-models/keras/qkeras_3layer_weights.h5')
-  return model
+    '''
+    Load the 3 hidden layer QKeras example model trained on the jet tagging dataset
+    '''
+    model_path = example_model_path / 'keras/qkeras_3layer.json'
+    with model_path.open('r') as f:
+        jsons = f.read()
+    model = model_from_json(jsons, custom_objects=co)
+    model.load_weights(example_model_path / 'keras/qkeras_3layer_weights.h5')
+    return model
+
 
+# TODO - Paramaterize for Quartus (different strategies?)
 @pytest.fixture
 @pytest.mark.parametrize('strategy', ['latency', 'resource'])
 def convert(load_jettagging_model, strategy):
-  '''
-  Convert a QKeras model trained on the jet tagging dataset
-  '''
-  model = load_jettagging_model
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'
-
-  config = hls4ml.utils.config_from_keras_model(model, granularity='name')
-  config['Model']['Strategy'] = strategy
-  config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'
-  config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'
-  hls_model = hls4ml.converters.convert_from_keras_model(model,
-                                                       hls_config=config,
-                                                       output_dir='hls4mlprj_qkeras_accuracy_{}'.format(strategy),
-                                                       part='xcu250-figd2104-2L-e')
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = []                                                     
-  hls_model.compile()
-  return hls_model
+    '''
+    Convert a QKeras model trained on the jet tagging dataset
+    '''
+    model = load_jettagging_model
+
+    config = hls4ml.utils.config_from_keras_model(model, granularity='name')
+    config['Model']['Strategy'] = strategy
+    config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'
+    config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model,
+        hls_config=config,
+        output_dir=str(test_root_path / f'hls4mlprj_qkeras_accuracy_{strategy}'),
+        part='xcu250-figd2104-2L-e',
+    )
+    hls_model.compile()
+    return hls_model
+
 
 @pytest.mark.parametrize('strategy', ['latency', 'resource'])
 def test_accuracy(convert, load_jettagging_model, get_jettagging_data, strategy):
-  '''
-  Test the hls4ml-evaluated accuracy of a 3 hidden layer QKeras model trained on
-  the jet tagging dataset. QKeras model accuracy is required to be over 70%, and
-  hls4ml accuracy required to be within 1% of the QKeras model accuracy.
-  '''
-  print("Test accuracy")
-  from sklearn.metrics import accuracy_score
-
-  X_train_val, X_test, y_train_val, y_test = get_jettagging_data
-
-  hls_model = convert
-  model = load_jettagging_model
-
-  y_qkeras = model.predict(np.ascontiguousarray(X_test))
-  y_hls4ml = hls_model.predict(np.ascontiguousarray(X_test))
-
-  acc_qkeras = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))
-  acc_hls4ml = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls4ml, axis=1))
-  rel_diff = abs(acc_qkeras - acc_hls4ml) / acc_qkeras
-
-  print('Accuracy qkeras:     {}'.format(acc_qkeras))
-  print('Accuracy hls4ml:     {}'.format(acc_hls4ml))
-  print('Relative difference: {}'.format(rel_diff))
+    '''
+    Test the hls4ml-evaluated accuracy of a 3 hidden layer QKeras model trained on
+    the jet tagging dataset. QKeras model accuracy is required to be over 70%, and
+    hls4ml accuracy required to be within 1% of the QKeras model accuracy.
+    '''
+    print("Test accuracy")
+    from sklearn.metrics import accuracy_score
+
+    X_train_val, X_test, y_train_val, y_test = get_jettagging_data
+
+    hls_model = convert
+    model = load_jettagging_model
+
+    y_qkeras = model.predict(np.ascontiguousarray(X_test))
+    y_hls4ml = hls_model.predict(np.ascontiguousarray(X_test))
+
+    acc_qkeras = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))
+    acc_hls4ml = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls4ml, axis=1))
+    rel_diff = abs(acc_qkeras - acc_hls4ml) / acc_qkeras
+
+    print(f'Accuracy qkeras:     {acc_qkeras}')
+    print(f'Accuracy hls4ml:     {acc_hls4ml}')
+    print(f'Relative difference: {rel_diff}')
+
+    assert acc_qkeras > 0.7 and rel_diff < 0.01
 
-  assert acc_qkeras > 0.7 and rel_diff < 0.01
 
 def randX(batch_size, N):
-  return np.random.rand(batch_size,N)
+    return np.random.rand(batch_size, N)
+
 
 @pytest.fixture(scope='module')
 def randX_100_16():
-  return randX(100, 16)
+    return randX(100, 16)
+
 
 # TODO: include wider bitwidths when that can be made to pass
 # Note 4-bit test can still fail sometimes depending on random seed
 # https://github.com/fastmachinelearning/hls4ml/issues/381
-#@pytest.mark.parametrize('bits', [4, 6, 8])
-@pytest.mark.parametrize('bits', [4])
-def test_single_dense_activation_exact(randX_100_16, bits):
-  '''
-  Test a single Dense -> Activation layer topology for
-  bit exactness with number of bits parameter
-  '''
-  X = randX_100_16
-  model = Sequential()
-  model.add(QDense(16, input_shape=(16,), name='fc1',
-                  kernel_quantizer=quantized_bits(bits,0,alpha=1), bias_quantizer=quantized_bits(bits,0,alpha=1),
-                  kernel_initializer='lecun_uniform'))
-  model.add(QActivation(activation=quantized_relu(bits,0), name='relu1'))
-  model.compile()
-
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['relu1']
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND_CONV'
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'
-  config = hls4ml.utils.config_from_keras_model(model, granularity='name')
-  hls_model = hls4ml.converters.convert_from_keras_model(model,
-                                                       hls_config=config,
-                                                       output_dir='hls4mlprj_qkeras_single_dense_activation_exact_{}'.format(bits),
-                                                       part='xcu250-figd2104-2L-e')
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = []                                                   
-  hls_model.compile()
-
-  y_qkeras = model.predict(X)
-  y_hls4ml = hls_model.predict(X)
-  # Goal is to get it passing with all equal
-  #np.testing.assert_array_equal(y_qkeras, y_hls4ml)
-  # For now allow matching within 1 bit
-  np.testing.assert_allclose(y_qkeras.ravel(), y_hls4ml.ravel(), atol=2**-bits, rtol=1.0)
+# @pytest.mark.parametrize('bits', [4, 6, 8])
+@pytest.mark.parametrize('bits,alpha', [(4, 1), (4, 'auto_po2')])
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_single_dense_activation_exact(randX_100_16, bits, alpha, backend, io_type):
+    '''
+    Test a single Dense -> Activation layer topology for
+    bit exactness with number of bits parameter
+    '''
+    X = randX_100_16
+    model = Sequential()
+    model.add(
+        QDense(
+            16,
+            input_shape=(16,),
+            name='fc1',
+            kernel_quantizer=quantized_bits(bits, 0, alpha=alpha),
+            bias_quantizer=quantized_bits(bits, 0, alpha=1),
+            kernel_initializer='lecun_uniform',
+        )
+    )
+    model.add(QActivation(activation=quantized_relu(bits, 0), name='relu1'))
+    model.compile()
+
+    config = hls4ml.utils.config_from_keras_model(model, granularity='name')
+    output_dir = str(test_root_path / f'hls4mlprj_qkeras_single_dense_activation_exact_{bits}_{alpha}_{backend}_{io_type}')
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
+    hls_model.compile()
+
+    y_qkeras = model.predict(X)
+    y_hls4ml = hls_model.predict(X)
+    # Goal is to get it passing with all equal
+    # np.testing.assert_array_equal(y_qkeras, y_hls4ml)
+    # For now allow matching within 1 bit
+    np.testing.assert_allclose(y_qkeras.ravel(), y_hls4ml.ravel(), atol=2**-bits, rtol=1.0)
+
 
 @pytest.fixture
-def make_btnn(N, kernel_quantizer, bias_quantizer, activation_quantizer, use_batchnorm, is_xnor):
-  shape = (N,)
-  model = Sequential()
-  model.add(QDense(10, input_shape=shape, kernel_quantizer=kernel_quantizer,
-                   bias_quantizer=bias_quantizer, name='dense'))
-  if use_batchnorm:
-    model.add(BatchNormalization(name='bn'))
-  model.add(QActivation(activation=activation_quantizer))
-  model.compile()
-  return model, is_xnor
+def make_btnn(test_no, N, kernel_quantizer, bias_quantizer, activation_quantizer, use_batchnorm, is_xnor):
+    shape = (N,)
+    model = Sequential()
+    model.add(QDense(10, input_shape=shape, kernel_quantizer=kernel_quantizer, bias_quantizer=bias_quantizer, name='dense'))
+    if use_batchnorm:
+        model.add(BatchNormalization(name='bn'))
+    model.add(QActivation(activation=activation_quantizer))
+    model.compile()
+    return model, is_xnor, test_no
+
 
 @pytest.fixture(scope='module')
 def randX_100_10():
-  return randX(100, 10)
+    return randX(100, 10)
+
+
+@pytest.mark.parametrize(
+    'quantizer', [(quantized_tanh(8)), (quantized_sigmoid(5)), (quantized_sigmoid(7, use_real_sigmoid=True))]
+)
+@pytest.mark.parametrize('backend', ['Vivado', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_quantizer_special(randX_1000_1, quantizer, backend, io_type):
+    '''
+    Test a single quantizer (tanh or sigmoid) as an Activation function.
+    Checks the type inference through the conversion is correct without just
+    using the same logic.
+    '''
+    X = randX_1000_1
+    X = np.round(X * 2**10) * 2**-10  # make it an exact ap_fixed<16,6>
+    model = Sequential()
+    model.add(QActivation(input_shape=(1,), activation=quantizer, name='quantizer'))
+    model.compile()
+
+    config = hls4ml.utils.config_from_keras_model(model, granularity='name')
+    output_dir = str(
+        test_root_path / f'hls4mlprj_qkeras_quantizer_{quantizer.__class__.__name__}_{quantizer.bits}_{backend}_{io_type}'
+    )
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
+    hls_model.compile()
+
+    y_qkeras = model.predict(X)
+    y_hls4ml = hls_model.predict(X)
+    # Goal is to get it passing with all equal
+    np.testing.assert_allclose(y_qkeras, y_hls4ml, rtol=1e-2, atol=0.02)
+
+
+@pytest.mark.parametrize(
+    'test_no,N,kernel_quantizer,bias_quantizer,activation_quantizer,use_batchnorm,is_xnor',
+    [
+        (1, 10, ternary(alpha=1), quantized_bits(5, 2), 'binary_tanh', False, False),
+        (2, 10, binary(), quantized_bits(5, 2), 'binary_tanh', False, True),
+        (3, 10, ternary(alpha='auto'), quantized_bits(5, 2), binary(), True, True),
+        (4, 10, ternary(alpha='auto'), quantized_bits(5, 2), 'ternary', True, False),
+        (5, 10, ternary(alpha='auto'), quantized_bits(5, 2), ternary(threshold=0.2), True, False),
+        (6, 10, ternary(alpha='auto'), quantized_bits(5, 2), ternary(threshold=0.8), True, False),
+        (7, 10, binary(), quantized_bits(5, 2), binary(), False, True),
+    ],
+)
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_btnn(make_btnn, randX_100_10, backend, io_type):
+    model, is_xnor, test_no = make_btnn
+    X = randX_100_10
+    cfg = hls4ml.utils.config_from_keras_model(model, granularity='name', backend=backend)
+    output_dir = str(test_root_path / f'hls4mlprj_btnn_{test_no}_{backend}_{io_type}')
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, output_dir=output_dir, hls_config=cfg, backend=backend, io_type=io_type
+    )
+    hls_model.compile()
+    y_hls = hls_model.predict(X)
+    # hls4ml may return XNOR binary
+    if is_xnor:
+        y_hls = np.where(y_hls == 0, -1, 1)
+    y_ker = model.predict(X)
+    wrong = (y_hls != y_ker).ravel()
+    assert sum(wrong) / len(wrong) < 0.005
 
-@pytest.mark.parametrize('N,kernel_quantizer,bias_quantizer,activation_quantizer,use_batchnorm,is_xnor',
-                          [(10, ternary(alpha=1), quantized_bits(5,2), 'binary_tanh', False, False),
-                           (10, binary(), quantized_bits(5,2), 'binary_tanh', False, True),
-                           (10, ternary(alpha='auto'), quantized_bits(5,2), binary(), True, True),
-                           (10, ternary(alpha='auto'), quantized_bits(5,2), 'ternary', True, False),
-                           (10, ternary(alpha='auto'), quantized_bits(5,2), ternary(threshold=0.2), True, False),
-                           (10, ternary(alpha='auto'), quantized_bits(5,2), ternary(threshold=0.8), True, False),
-                           (10, binary(), quantized_bits(5,2), binary(), False, True)])
-def test_btnn(make_btnn, randX_100_10):
-  model, is_xnor = make_btnn
-  X = randX_100_10
-  cfg = hls4ml.utils.config_from_keras_model(model, granularity='name')
-  hls_model = hls4ml.converters.convert_from_keras_model(model, output_dir='btnn', hls_config=cfg)
-  hls_model.compile()
-  y_hls = hls_model.predict(X)
-  # hls4ml may return XNOR binary
-  if is_xnor:
-    y_hls = np.where(y_hls == 0, -1, 1)
-  y_ker = model.predict(X)
-  wrong = (y_hls != y_ker).ravel()
-  assert sum(wrong) / len(wrong) < 0.005
 
 @pytest.fixture(scope='module')
 def randX_1000_1():
-  return randX(1000, 1)
+    return randX(1000, 1)
+
 
 # TODO: include quantized_relu tests when they are made to pass
 # https://github.com/fastmachinelearning/hls4ml/issues/377
-@pytest.mark.parametrize('quantizer', [(quantized_bits(8,0)),
-                                       (quantized_bits(8,4)),
-                                       (quantized_bits(4,2)),
-                                       (quantized_bits(4,0)),
-                                       (quantized_bits(10,0)),
-                                       (quantized_relu(4)),
-                                       (quantized_relu(4,2)),
-                                       (quantized_relu(8)),
-                                       (quantized_relu(8,4)),
-                                       (quantized_relu(10)),
-                                       (quantized_relu(10,5))])
-def test_quantizer(randX_1000_1, quantizer):
-  '''
-  Test a single quantizer as an Activation function.
-  Checks the type inference through the conversion is correct without just
-  using the same logic.
-  '''
-  X = randX_1000_1
-  X = np.round(X * 2**10) * 2**-10 # make it an exact ap_fixed<16,6>
-  model = Sequential()
-  model.add(QActivation(input_shape=(1,), activation=quantizer, name='quantizer'))
-  model.compile()
-
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['quantizer']
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND_CONV'
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'
-  config = hls4ml.utils.config_from_keras_model(model, granularity='name')
-  output_dir = 'hls4mlprj_qkeras_quantizer_{}_{}_{}'.format(quantizer.__class__.__name__,
-                                                            quantizer.bits, quantizer.integer)
-  hls_model = hls4ml.converters.convert_from_keras_model(model,
-                                                       hls_config=config,
-                                                       output_dir=output_dir,
-                                                       part='xcu250-figd2104-2L-e')
-  hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = []                                                   
-  hls_model.compile()
-
-  y_qkeras = model.predict(X)
-  y_hls4ml = hls_model.predict(X)
-  # Goal is to get it passing with all equal
-  np.testing.assert_array_equal(y_qkeras, y_hls4ml)
+@pytest.mark.parametrize(
+    'quantizer',
+    [
+        (quantized_bits(8, 0)),
+        (quantized_bits(8, 4)),
+        (quantized_bits(4, 2)),
+        (quantized_bits(4, 0)),
+        (quantized_bits(10, 0)),
+        (quantized_relu(4)),
+        (quantized_relu(4, 2)),
+        (quantized_relu(8)),
+        (quantized_relu(8, 4)),
+        (quantized_relu(10)),
+        (quantized_relu(10, 5)),
+    ],
+)
+@pytest.mark.parametrize('backend', ['Vivado', 'Vitis', 'Quartus'])
+@pytest.mark.parametrize('io_type', ['io_parallel', 'io_stream'])
+def test_quantizer(randX_1000_1, quantizer, backend, io_type):
+    '''
+    Test a single quantizer as an Activation function.
+    Checks the type inference through the conversion is correct without just
+    using the same logic.
+    '''
+    X = randX_1000_1
+    X = np.round(X * 2**10) * 2**-10  # make it an exact ap_fixed<16,6>
+    model = Sequential()
+    model.add(QActivation(input_shape=(1,), activation=quantizer, name='quantizer'))
+    model.compile()
+
+    config = hls4ml.utils.config_from_keras_model(model, granularity='name')
+    output_dir = str(
+        test_root_path
+        / 'hls4mlprj_qkeras_quantizer_{}_{}_{}_{}_{}'.format(
+            quantizer.__class__.__name__, quantizer.bits, quantizer.integer, backend, io_type
+        )
+    )
+    hls_model = hls4ml.converters.convert_from_keras_model(
+        model, hls_config=config, output_dir=output_dir, backend=backend, io_type=io_type
+    )
+    hls_model.compile()
+
+    y_qkeras = model.predict(X)
+    y_hls4ml = hls_model.predict(X)
+    # Goal is to get it passing with all equal
+    np.testing.assert_array_equal(y_qkeras, y_hls4ml)
+
+
+@pytest.mark.parametrize(
+    'weight_quantizer,activation_quantizer,',
+    [
+        ('binary', 'binary'),
+        ('ternary', 'ternary'),
+        ('quantized_bits(4, 0, alpha=1)', 'quantized_relu(2, 0)'),
+        ('quantized_bits(4, 0, alpha=1)', 'quantized_relu(4, 0)'),
+        ('quantized_bits(4, 0, alpha=1)', 'quantized_relu(8, 0)'),
+    ],
+)
+def test_qactivation_kwarg(randX_100_10, activation_quantizer, weight_quantizer):
+    if activation_quantizer in ['binary', 'ternary']:
+        name = 'bnbt_qdense_alpha'
+    else:
+        name = f'qdense_{eval(activation_quantizer).__class__.__name__}'
+
+    inputs = Input(shape=(10,))
+
+    outputs = QDense(
+        10,
+        activation=activation_quantizer,
+        name='qdense',
+        kernel_quantizer=weight_quantizer,
+        bias_quantizer=weight_quantizer,
+        kernel_initializer='lecun_uniform',
+    )(inputs)
+    model = Model(inputs, outputs)
+
+    config = hls4ml.utils.config_from_keras_model(model, granularity='name')
+
+    out_dir = str(test_root_path / f'hls4mlprj_qactivation_kwarg_{activation_quantizer}')
+
+    hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir=out_dir)
+    hls_model.compile()
+
+    # Verify if activation in hls_model
+    assert name in [layer.name for layer in hls_model.get_layers()]
+
+    # Output tests
+    X = randX_100_10
+    X = np.round(X * 2**10) * 2**-10
+    y_qkeras = model.predict(X)
+    y_hls4ml = hls_model.predict(X)
+    if hasattr(eval(activation_quantizer), 'bits'):
+        np.testing.assert_allclose(
+            y_qkeras.ravel(), y_hls4ml.ravel(), atol=2 ** -(eval(activation_quantizer).bits - 1), rtol=1.0
+        )
+    else:
+        if activation_quantizer == 'binary':
+            y_hls4ml = np.where(y_hls4ml == 0, -1, 1)
+        wrong = (y_hls4ml != y_qkeras).ravel()
+        assert sum(wrong) / len(wrong) <= 0.005
```

### Comparing `hls4ml-0.6.0/test/pytorch-models.txt` & `hls4ml-0.7.0rc1/test/pytorch-models.txt`

 * *Files identical despite different names*

### Comparing `hls4ml-0.6.0/test/pytorch-to-hls.sh` & `hls4ml-0.7.0rc1/test/onnx-to-hls.sh`

 * *Files 11% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 #!/bin/bash
 
 pycmd=python
-xilinxpart="xc7vx690tffg1927-2"
+part="xc7vx690tffg1927-2"
 clock=5
 io=io_parallel
 rf=1
 strategy="Latency"
 type="ap_fixed<16,6>"
 basedir=vivado_prj
 
 sanitizer="[^A-Za-z0-9._]"
 
 function print_usage {
    echo "Usage: `basename $0` [OPTION] MODEL..."
    echo ""
-   echo "MODEL is the name of the model pt file without extension. Multiple"
+   echo "MODEL is the name of the model onnx file without extension. Multiple"
    echo "models can be specified."
    echo ""
    echo "Options are:"
    echo "   -x DEVICE"
    echo "      Xilinx device part number. Defaults to 'xc7vx690tffg1927-2'."
    echo "   -c CLOCK"
    echo "      Clock period to use. Defaults to 5."
@@ -34,15 +34,15 @@
    echo "      Output directory."
    echo "   -h"
    echo "      Prints this help message."
 }
 
 while getopts ":x:c:sr:g:t:d:h" opt; do
    case "$opt" in
-   x) xilinxpart=$OPTARG
+   x) part=$OPTARG
       ;;
    c) clock=$OPTARG
       ;;
    s) io=io_stream
       ;;
    r) rf=$OPTARG
       ;;
@@ -75,18 +75,18 @@
 
 for model in "${models[@]}"
 do
    echo "Creating config file for model '${model}'"
    base=${model%.*}
    file="${basedir}/${base}.yml"
 
-   echo "PytorchModel: ../example-models/pytorch/${model}.pt" > ${file}
-   echo "OutputDir: ${basedir}/${base}-${xilinxpart//${sanitizer}/_}-c${clock}-${io}-rf${rf}-${type//${sanitizer}/_}-${strategy}" >> ${file}
+   echo "OnnxModel: ../example-models/onnx/${model}.onnx" > ${file}
+   echo "OutputDir: ${basedir}/${base}-${part//${sanitizer}/_}-c${clock}-${io}-rf${rf}-${type//${sanitizer}/_}-${strategy}" >> ${file}
    echo "ProjectName: myproject" >> ${file}
-   echo "XilinxPart: ${xilinxpart}" >> ${file}
+   echo "Part: ${part}" >> ${file}
    echo "ClockPeriod: ${clock}" >> ${file}
    echo "" >> ${file}
    echo "IOType: ${io}" >> ${file}
    echo "HLSConfig:" >> ${file}
    echo "  Model:" >> ${file}
    echo "    ReuseFactor: ${rf}" >> ${file}
    echo "    Precision: ${type} " >> ${file}
```

